{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Torchvision Models\n",
    "\n",
    "\n",
    "- Pytorch model에서 finetuning을 어떻게 하는지 학습한다.\n",
    "- finetune / feature extraction을 수행\n",
    "\n",
    "<br/>\n",
    "\n",
    "두개의 transfer learning 메소드는 몇가지 과정을 따름\n",
    "\n",
    "- pretrained model 초기화\n",
    "- 새로운 데이터셋에 맞춰 출력 lyaer를 조정\n",
    "- optimization algorithm을 종우하머\n",
    "- 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  0.4.1\n",
      "Torchvision Version:  0.2.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs\n",
    "\n",
    "- pretrained model을 사용할 때, input parameter에 대해서 알아봄\n",
    "- *hymenoptera_data* dataset을 사용(**bees / ants**)\n",
    "\n",
    "`torchvision`에서 제공되는 pretrained model 리스트는 다음과 같음.  \n",
    "    \n",
    "<br/>\n",
    "\n",
    "```\n",
    "[resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "- `num_classes` : dataset의 class 개수\n",
    "- `batch_size` : 학습시의 batch size\n",
    "- `num_epochs` : 학습시의 epoch 수\n",
    "- `feature_extract` : `True`인 경우에는, feature extractor로만 사용가능; 마지막 layer만 업데이트 됨. \n",
    "    - (파라미터 trainable or non-trainable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"./data/hymenoptera_data\"\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"squeezenet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 2\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 15\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function\n",
    "\n",
    "helper function을 만들어보자.\n",
    "\n",
    "## Model Training and Validation Code\n",
    "\n",
    "- `train_model`함수는 model을 인자로 받고 training과 validation을 핸들링한다.\n",
    "- 이때 필요한 파라미터는 `model`, `dataloaders,`, `criterion`, `optimizer`, `num_epochs`, `is_inception`\n",
    "    - `is_inception` flag는 *Inception v3* 모델을 수용할 때, 사용한다.\n",
    "- 해당 helper function은 정해진 epoch안에 제일 성능이 잘나오는 모델을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Model Parameters' .requires_grad attribute\n",
    "\n",
    "- 이미 모두가 알고있듯이 `.requires_grad`가 `True`몇 finetune, `False`면 feature extractor가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and Reshape the Networks\n",
    "\n",
    "# Resnet\n",
    "\n",
    "Resnet의 FC layer구조는 아래와 같음.  \n",
    "\n",
    "```python\n",
    "(fc): Linear(in_features=512, out_features=1000, bias=True)\n",
    "```\n",
    "\n",
    "이를 다음과 같이 변경함  \n",
    "```python\n",
    "model.fc = nn.Linear(512, num_classes)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alexnet\n",
    "\n",
    "Alexnet 구조는 아래와 같음  \n",
    "```python\n",
    "(classifier): Sequential(\n",
    "    ...\n",
    "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
    " )\n",
    "```\n",
    "\n",
    "이를 다음과 같이 변경해줌\n",
    "\n",
    "```python\n",
    "model.classifier[6] = nn.Linear(4096,num_classes)\n",
    "```\n",
    "\n",
    "# Squeezenet\n",
    "\n",
    "```python\n",
    "(classifier): Sequential(\n",
    "    (0): Dropout(p=0.5)\n",
    "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
    "    (2): ReLU(inplace)\n",
    "    (3): AvgPool2d(kernel_size=13, stride=1, padding=0)\n",
    " )\n",
    "```\n",
    "\n",
    "```python\n",
    "model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "```\n",
    "\n",
    "\n",
    "# Densenet\n",
    "```python\n",
    "(classifier): Linear(in_features=1024, out_features=1000, bias=True)\n",
    "```\n",
    "\n",
    "```python\n",
    "model.classifier = nn.Linear(1024, num_classes)\n",
    "```\n",
    "\n",
    "# Inception v3\n",
    "```python\n",
    "(AuxLogits): InceptionAux(\n",
    "    ...\n",
    "    (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
    " )\n",
    " ...\n",
    "(fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
    "```\n",
    "\n",
    "```python\n",
    "model.AuxLogits.fc = nn.Linear(768, num_classes)\n",
    "model.fc = nn.Linear(2048, num_classes)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchvision/models/squeezenet.py:94: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  init.kaiming_uniform(m.weight.data)\n",
      "/usr/local/lib/python3.6/dist-packages/torchvision/models/squeezenet.py:92: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(m.weight.data, mean=0.0, std=0.01)\n",
      "Downloading: \"https://download.pytorch.org/models/squeezenet1_0-a815701f.pth\" to /home/keti-1080ti/.torch/models/squeezenet1_0-a815701f.pth\n",
      "100%|██████████| 5017600/5017600 [00:05<00:00, 972198.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (3): Fire(\n",
      "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (4): Fire(\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (5): Fire(\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (7): Fire(\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (8): Fire(\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (9): Fire(\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (10): Fire(\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (12): Fire(\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU(inplace)\n",
      "    (3): AvgPool2d(kernel_size=13, stride=1, padding=0)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Training and Validation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 0.5249 Acc: 0.7828\n",
      "val Loss: 0.5593 Acc: 0.8366\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.3089 Acc: 0.8893\n",
      "val Loss: 0.3577 Acc: 0.8889\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.2382 Acc: 0.8934\n",
      "val Loss: 0.3255 Acc: 0.9150\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.1815 Acc: 0.9303\n",
      "val Loss: 0.3479 Acc: 0.9085\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.1894 Acc: 0.9098\n",
      "val Loss: 0.3635 Acc: 0.9216\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.1244 Acc: 0.9467\n",
      "val Loss: 0.3798 Acc: 0.9281\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.1487 Acc: 0.9549\n",
      "val Loss: 0.3674 Acc: 0.9346\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.1699 Acc: 0.9016\n",
      "val Loss: 0.3662 Acc: 0.8954\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.1496 Acc: 0.9467\n",
      "val Loss: 0.3674 Acc: 0.9216\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.2392 Acc: 0.9057\n",
      "val Loss: 0.3648 Acc: 0.9281\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.1489 Acc: 0.9303\n",
      "val Loss: 0.3343 Acc: 0.9216\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.1184 Acc: 0.9426\n",
      "val Loss: 0.3438 Acc: 0.9281\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.1414 Acc: 0.9385\n",
      "val Loss: 0.3556 Acc: 0.9216\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.1391 Acc: 0.9385\n",
      "val Loss: 0.3686 Acc: 0.9216\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.1016 Acc: 0.9549\n",
      "val Loss: 0.3910 Acc: 0.9281\n",
      "\n",
      "Training complete in 0m 17s\n",
      "Best val Acc: 0.934641\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with Model Trained from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchvision/models/squeezenet.py:94: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  init.kaiming_uniform(m.weight.data)\n",
      "/usr/local/lib/python3.6/dist-packages/torchvision/models/squeezenet.py:92: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(m.weight.data, mean=0.0, std=0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.7080 Acc: 0.4467\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.6931 Acc: 0.5369\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.6931 Acc: 0.5205\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.6931 Acc: 0.5287\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.6930 Acc: 0.5041\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.6926 Acc: 0.5041\n",
      "val Loss: 0.6931 Acc: 0.4837\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.6913 Acc: 0.4959\n",
      "val Loss: 0.6919 Acc: 0.5621\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.6912 Acc: 0.4959\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.6931 Acc: 0.5328\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.6931 Acc: 0.5287\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.6931 Acc: 0.5738\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.6931 Acc: 0.5533\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.6931 Acc: 0.5492\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.6929 Acc: 0.5779\n",
      "val Loss: 0.6931 Acc: 0.4575\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.6858 Acc: 0.5492\n",
      "val Loss: 0.6815 Acc: 0.5425\n",
      "\n",
      "Training complete in 0m 19s\n",
      "Best val Acc: 0.562092\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the non-pretrained version of the model used for this run\n",
    "scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
    "scratch_model = scratch_model.to(device)\n",
    "scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
    "scratch_criterion = nn.CrossEntropyLoss()\n",
    "_,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "\n",
    "# Plot the training curves of validation accuracy vs. number\n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "ohist = []\n",
    "shist = []\n",
    "\n",
    "ohist = [h.cpu().numpy() for h in hist]\n",
    "shist = [h.cpu().numpy() for h in scratch_hist]\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts and Where to Go Next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
