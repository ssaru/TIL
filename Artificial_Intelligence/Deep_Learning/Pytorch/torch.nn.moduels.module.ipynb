{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('__call__', <function Module.__call__ at 0x7f450a041268>)\n",
      "\n",
      "('__class__', <class 'type'>)\n",
      "\n",
      "('__delattr__', <function Module.__delattr__ at 0x7f450a041488>)\n",
      "\n",
      "('__dict__', mappingproxy({'__module__': 'torch.nn.modules.module', '__doc__': 'Base class for all neural network modules.\\n\\n    Your models should also subclass this class.\\n\\n    Modules can also contain other Modules, allowing to nest them in\\n    a tree structure. You can assign the submodules as regular attributes::\\n\\n        import torch.nn as nn\\n        import torch.nn.functional as F\\n\\n        class Model(nn.Module):\\n            def __init__(self):\\n                super(Model, self).__init__()\\n                self.conv1 = nn.Conv2d(1, 20, 5)\\n                self.conv2 = nn.Conv2d(20, 20, 5)\\n\\n            def forward(self, x):\\n               x = F.relu(self.conv1(x))\\n               return F.relu(self.conv2(x))\\n\\n    Submodules assigned in this way will be registered, and will have their\\n    parameters converted too when you call `.cuda()`, etc.\\n    ', 'dump_patches': False, '_version': 1, '__init__': <function Module.__init__ at 0x7f450a0327b8>, 'forward': <function Module.forward at 0x7f450a032840>, 'register_buffer': <function Module.register_buffer at 0x7f450a0328c8>, 'register_parameter': <function Module.register_parameter at 0x7f450a032950>, 'add_module': <function Module.add_module at 0x7f450a0329d8>, '_apply': <function Module._apply at 0x7f450a032a60>, 'apply': <function Module.apply at 0x7f450a032ae8>, 'cuda': <function Module.cuda at 0x7f450a032b70>, 'cpu': <function Module.cpu at 0x7f450a032bf8>, 'type': <function Module.type at 0x7f450a032c80>, 'float': <function Module.float at 0x7f450a032d08>, 'double': <function Module.double at 0x7f450a032d90>, 'half': <function Module.half at 0x7f450a032e18>, 'to': <function Module.to at 0x7f450a032ea0>, 'register_backward_hook': <function Module.register_backward_hook at 0x7f450a032f28>, 'register_forward_pre_hook': <function Module.register_forward_pre_hook at 0x7f450a041048>, 'register_forward_hook': <function Module.register_forward_hook at 0x7f450a0410d0>, '_tracing_name': <function Module._tracing_name at 0x7f450a041158>, '_slow_forward': <function Module._slow_forward at 0x7f450a0411e0>, '__call__': <function Module.__call__ at 0x7f450a041268>, '__setstate__': <function Module.__setstate__ at 0x7f450a0412f0>, '__getattr__': <function Module.__getattr__ at 0x7f450a041378>, '__setattr__': <function Module.__setattr__ at 0x7f450a041400>, '__delattr__': <function Module.__delattr__ at 0x7f450a041488>, 'state_dict': <function Module.state_dict at 0x7f450a041510>, '_load_from_state_dict': <function Module._load_from_state_dict at 0x7f450a041598>, 'load_state_dict': <function Module.load_state_dict at 0x7f450a041620>, 'parameters': <function Module.parameters at 0x7f450a0416a8>, 'named_parameters': <function Module.named_parameters at 0x7f450a041730>, '_all_buffers': <function Module._all_buffers at 0x7f450a0417b8>, 'children': <function Module.children at 0x7f450a041840>, 'named_children': <function Module.named_children at 0x7f450a0418c8>, 'modules': <function Module.modules at 0x7f450a041950>, 'named_modules': <function Module.named_modules at 0x7f450a0419d8>, 'train': <function Module.train at 0x7f450a041a60>, 'eval': <function Module.eval at 0x7f450a041ae8>, 'zero_grad': <function Module.zero_grad at 0x7f450a041b70>, 'share_memory': <function Module.share_memory at 0x7f450a041bf8>, '_get_name': <function Module._get_name at 0x7f450a041c80>, 'extra_repr': <function Module.extra_repr at 0x7f450a041d08>, '__repr__': <function Module.__repr__ at 0x7f450a041d90>, '__dir__': <function Module.__dir__ at 0x7f450a041e18>, '__dict__': <attribute '__dict__' of 'Module' objects>, '__weakref__': <attribute '__weakref__' of 'Module' objects>}))\n",
      "\n",
      "('__dir__', <function Module.__dir__ at 0x7f450a041e18>)\n",
      "\n",
      "('__doc__', 'Base class for all neural network modules.\\n\\n    Your models should also subclass this class.\\n\\n    Modules can also contain other Modules, allowing to nest them in\\n    a tree structure. You can assign the submodules as regular attributes::\\n\\n        import torch.nn as nn\\n        import torch.nn.functional as F\\n\\n        class Model(nn.Module):\\n            def __init__(self):\\n                super(Model, self).__init__()\\n                self.conv1 = nn.Conv2d(1, 20, 5)\\n                self.conv2 = nn.Conv2d(20, 20, 5)\\n\\n            def forward(self, x):\\n               x = F.relu(self.conv1(x))\\n               return F.relu(self.conv2(x))\\n\\n    Submodules assigned in this way will be registered, and will have their\\n    parameters converted too when you call `.cuda()`, etc.\\n    ')\n",
      "\n",
      "('__eq__', <slot wrapper '__eq__' of 'object' objects>)\n",
      "\n",
      "('__format__', <method '__format__' of 'object' objects>)\n",
      "\n",
      "('__ge__', <slot wrapper '__ge__' of 'object' objects>)\n",
      "\n",
      "('__getattr__', <function Module.__getattr__ at 0x7f450a041378>)\n",
      "\n",
      "('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>)\n",
      "\n",
      "('__gt__', <slot wrapper '__gt__' of 'object' objects>)\n",
      "\n",
      "('__hash__', <slot wrapper '__hash__' of 'object' objects>)\n",
      "\n",
      "('__init__', <function Module.__init__ at 0x7f450a0327b8>)\n",
      "\n",
      "('__init_subclass__', <built-in method __init_subclass__ of type object at 0x2302ee8>)\n",
      "\n",
      "('__le__', <slot wrapper '__le__' of 'object' objects>)\n",
      "\n",
      "('__lt__', <slot wrapper '__lt__' of 'object' objects>)\n",
      "\n",
      "('__module__', 'torch.nn.modules.module')\n",
      "\n",
      "('__ne__', <slot wrapper '__ne__' of 'object' objects>)\n",
      "\n",
      "('__new__', <built-in method __new__ of type object at 0x9e3840>)\n",
      "\n",
      "('__reduce__', <method '__reduce__' of 'object' objects>)\n",
      "\n",
      "('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>)\n",
      "\n",
      "('__repr__', <function Module.__repr__ at 0x7f450a041d90>)\n",
      "\n",
      "('__setattr__', <function Module.__setattr__ at 0x7f450a041400>)\n",
      "\n",
      "('__setstate__', <function Module.__setstate__ at 0x7f450a0412f0>)\n",
      "\n",
      "('__sizeof__', <method '__sizeof__' of 'object' objects>)\n",
      "\n",
      "('__str__', <slot wrapper '__str__' of 'object' objects>)\n",
      "\n",
      "('__subclasshook__', <built-in method __subclasshook__ of type object at 0x2302ee8>)\n",
      "\n",
      "('__weakref__', <attribute '__weakref__' of 'Module' objects>)\n",
      "\n",
      "('_all_buffers', <function Module._all_buffers at 0x7f450a0417b8>)\n",
      "\n",
      "('_apply', <function Module._apply at 0x7f450a032a60>)\n",
      "\n",
      "('_get_name', <function Module._get_name at 0x7f450a041c80>)\n",
      "\n",
      "('_load_from_state_dict', <function Module._load_from_state_dict at 0x7f450a041598>)\n",
      "\n",
      "('_slow_forward', <function Module._slow_forward at 0x7f450a0411e0>)\n",
      "\n",
      "('_tracing_name', <function Module._tracing_name at 0x7f450a041158>)\n",
      "\n",
      "('_version', 1)\n",
      "\n",
      "('add_module', <function Module.add_module at 0x7f450a0329d8>)\n",
      "\n",
      "('apply', <function Module.apply at 0x7f450a032ae8>)\n",
      "\n",
      "('children', <function Module.children at 0x7f450a041840>)\n",
      "\n",
      "('cpu', <function Module.cpu at 0x7f450a032bf8>)\n",
      "\n",
      "('cuda', <function Module.cuda at 0x7f450a032b70>)\n",
      "\n",
      "('double', <function Module.double at 0x7f450a032d90>)\n",
      "\n",
      "('dump_patches', False)\n",
      "\n",
      "('eval', <function Module.eval at 0x7f450a041ae8>)\n",
      "\n",
      "('extra_repr', <function Module.extra_repr at 0x7f450a041d08>)\n",
      "\n",
      "('float', <function Module.float at 0x7f450a032d08>)\n",
      "\n",
      "('forward', <function Module.forward at 0x7f450a032840>)\n",
      "\n",
      "('half', <function Module.half at 0x7f450a032e18>)\n",
      "\n",
      "('load_state_dict', <function Module.load_state_dict at 0x7f450a041620>)\n",
      "\n",
      "('modules', <function Module.modules at 0x7f450a041950>)\n",
      "\n",
      "('named_children', <function Module.named_children at 0x7f450a0418c8>)\n",
      "\n",
      "('named_modules', <function Module.named_modules at 0x7f450a0419d8>)\n",
      "\n",
      "('named_parameters', <function Module.named_parameters at 0x7f450a041730>)\n",
      "\n",
      "('parameters', <function Module.parameters at 0x7f450a0416a8>)\n",
      "\n",
      "('register_backward_hook', <function Module.register_backward_hook at 0x7f450a032f28>)\n",
      "\n",
      "('register_buffer', <function Module.register_buffer at 0x7f450a0328c8>)\n",
      "\n",
      "('register_forward_hook', <function Module.register_forward_hook at 0x7f450a0410d0>)\n",
      "\n",
      "('register_forward_pre_hook', <function Module.register_forward_pre_hook at 0x7f450a041048>)\n",
      "\n",
      "('register_parameter', <function Module.register_parameter at 0x7f450a032950>)\n",
      "\n",
      "('share_memory', <function Module.share_memory at 0x7f450a041bf8>)\n",
      "\n",
      "('state_dict', <function Module.state_dict at 0x7f450a041510>)\n",
      "\n",
      "('to', <function Module.to at 0x7f450a032ea0>)\n",
      "\n",
      "('train', <function Module.train at 0x7f450a041a60>)\n",
      "\n",
      "('type', <function Module.type at 0x7f450a032c80>)\n",
      "\n",
      "('zero_grad', <function Module.zero_grad at 0x7f450a041b70>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for element in inspect.getmembers(torch.nn.modules.Module):\n",
    "    print(\"{}\\n\".format(element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch `Module` class\n",
    "\n",
    "![torch nn modules module](https://user-images.githubusercontent.com/13328380/51440701-33105700-1d0d-11e9-9611-6bd5c517433c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "1. `:meth:`는 `method`를 의미함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _addindent(s_, numSpaces):\n",
    "    s = s_.split('\\n')\n",
    "    # don't do anything for single-line stuff\n",
    "    if len(s) == 1:\n",
    "        return s_\n",
    "    first = s.pop(0)\n",
    "    s = [(numSpaces * ' ') + line for line in s]\n",
    "    s = '\\n'.join(s)\n",
    "    s = first + '\\n' + s\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__repr__`에서 사용한다. 나중에 보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init signature: torch.nn.modules.Module()\n",
    "Docstring:     \n",
    "## Base class for all neural network modules.\n",
    "\n",
    "Your models should also subclass this class.\n",
    "\n",
    "Modules can also contain other Modules, allowing to nest them in\n",
    "a tree structure. You can assign the submodules as regular attributes::\n",
    "```python\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Model, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "        def forward(self, x):\n",
    "           x = F.relu(self.conv1(x))\n",
    "           return F.relu(self.conv2(x))\n",
    "```\n",
    "\n",
    "Submodules assigned in this way will be registered, and will have their\n",
    "parameters converted too when you call :meth:`to`, etc.\n",
    "    \n",
    "File:           ~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py  \n",
    "Type:           type  \n",
    "Subclasses:     Linear, Bilinear, _ConvNd, Threshold, RReLU, Hardtanh, Sigmoid, Tanh, ELU, CELU, ...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field\n",
    "# what is mean?\n",
    "dump_patches = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field\n",
    "_version = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows better BC support for :meth:`load_state_dict`. In\n",
    "    :meth:`state_dict`, the version number will be saved as in the attribute\n",
    "    `_metadata` of the returned state dict, and thus pickled. `_metadata` is a\n",
    "    dictionary with keys that follow the naming convention of state dict. See\n",
    "    ``_load_from_state_dict`` on how to use this information in loading.\n",
    "\n",
    "    If new parameters/buffers are added/removed from a module, this number shall\n",
    "    be bumped, and the module's `_load_from_state_dict` method can compare the\n",
    "    version number and do appropriate changes if the state dict is from before\n",
    "    the change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_state_dict` method에 대한 BC 지원이 향상된다. `state_dict` method에서, 버전 번호는 return된 state dict의 `_metadata` 내부의 속성으로써 저장될 것이고, pickled된다.\n",
    "\n",
    "`_metadata`는 state dict의 naming convention을 따르는 key가 있는 dictionary다. 해당 정보를 어떻게 사용하는지 보고싶으면`_load_from_state_dict`을 봐라.\n",
    "\n",
    "만약 모듈에서 새로운 파라미터/버퍼가 추가되거나 제거된다면 해당 숫자는 충돌을 일으켜야한다.\n",
    "모듈의 `_load_from_state_dict` method는 버전 번호를 비교할 수 있고, 만약에 state dict이 이전 버전에서 온 경우, 이를 적절하게 변경할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "        self._backend = thnn_backend\n",
    "        self._parameters = OrderedDict()\n",
    "        self._buffers = OrderedDict()\n",
    "        self._backward_hooks = OrderedDict()\n",
    "        self._forward_hooks = OrderedDict()\n",
    "        self._forward_pre_hooks = OrderedDict()\n",
    "        self._state_dict_hooks = OrderedDict()\n",
    "        self._load_state_dict_pre_hooks = OrderedDict()\n",
    "        self._modules = OrderedDict()\n",
    "        self.training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " def forward(self, *input):\n",
    "        r\"\"\"Defines the computation performed at every call.\n",
    "\n",
    "        Should be overridden by all subclasses.\n",
    "\n",
    "        .. note::\n",
    "            Although the recipe for forward pass needs to be defined within\n",
    "            this function, one should call the :class:`Module` instance afterwards\n",
    "            instead of this since the former takes care of running the\n",
    "            registered hooks while the latter silently ignores them.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signature: torch.nn.modules.Module.forward(self, *input)\n",
    "Docstring:\n",
    "Defines the computation performed at every call.\n",
    "\n",
    "Should be overridden by all subclasses.\n",
    "\n",
    ".. note::\n",
    "    Although the recipe for forward pass needs to be defined within\n",
    "    this function, one should call the :class:`Module` instance afterwards\n",
    "    instead of this since the former takes care of running the\n",
    "    registered hooks while the latter silently ignores them.  \n",
    "    \n",
    "File:      ~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py  \n",
    "Type:      function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 subclass들에 의해서 override되어야 한다.\n",
    "\n",
    "_**무슨 말일까?....**_\n",
    ">forwad pass를 위한 recipe는 `forward` method와 같이 정의되어야하지만  latter는 등록된 hook을 조용히 무시하는 반면에 former는 등록된 hook을 실행하기 때문에 Module Class instance는 나중에 호출할 필요가 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def register_buffer(self, name, tensor):\n",
    "        r\"\"\"Adds a persistent buffer to the module.\n",
    "\n",
    "        This is typically used to register a buffer that should not to be\n",
    "        considered a model parameter. For example, BatchNorm's ``running_mean``\n",
    "        is not a parameter, but is part of the persistent state.\n",
    "\n",
    "        Buffers can be accessed as attributes using given names.\n",
    "\n",
    "        Args:\n",
    "            name (string): name of the buffer. The buffer can be accessed\n",
    "                from this module using the given name\n",
    "            tensor (Tensor): buffer to be registered.\n",
    "\n",
    "        Example::\n",
    "\n",
    "            >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "\n",
    "        \"\"\"\n",
    "        if not isinstance(name, torch._six.string_classes):\n",
    "            raise TypeError(\"buffer name should be a string. \"\n",
    "                            \"Got {}\".format(torch.typename(name)))\n",
    "        elif '.' in name:\n",
    "            raise KeyError(\"buffer name can't contain \\\".\\\"\")\n",
    "        elif name == '':\n",
    "            raise KeyError(\"buffer name can't be empty string \\\"\\\"\")\n",
    "        elif hasattr(self, name) and name not in self._buffers:\n",
    "            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
    "        elif tensor is not None and not isinstance(tensor, torch.Tensor):\n",
    "            raise TypeError(\"cannot assign '{}' object to buffer '{}' \"\n",
    "                            \"(torch Tensor or None required)\"\n",
    "                            .format(torch.typename(tensor), name))\n",
    "        else:\n",
    "            self._buffers[name] = tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signature: torch.nn.modules.Module.register_buffer(self, name, tensor)\n",
    "Docstring:\n",
    "Adds a persistent buffer to the module.\n",
    "\n",
    "This is typically used to register a buffer that should not to be\n",
    "considered a model parameter. For example, BatchNorm's ``running_mean``\n",
    "is not a parameter, but is part of the persistent state.\n",
    "\n",
    "Buffers can be accessed as attributes using given names.\n",
    "\n",
    "Args:\n",
    "    name (string): name of the buffer. The buffer can be accessed\n",
    "        from this module using the given name\n",
    "    tensor (Tensor): buffer to be registered.\n",
    "\n",
    "Example::\n",
    "\n",
    "    >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "    \n",
    "File:      ~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py  \n",
    "Type:      function  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 함수는 특별하게 model parameter로 고려되면 안되는 파라미터(buffer)를 등록하는데 사용된다.\n",
    "> 예를들어서 BatchNorm의 `running_mean`은 prameter가 아니지만 지속성있는 상태(persistent state)의 일부다.\n",
    "\n",
    "Buffer들은 주어진 이름을 사용해서 attribute로써 접근될 수 있다.\n",
    "\n",
    "Args: name(string): buffer의 이름이다. Buffer는 지정할 이름과, Tensor를 이용해서 등록할 수 있다.\n",
    "Example ::\n",
    "\n",
    "```python\n",
    "self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = torch.nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = torch.nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = torch.nn.Linear(500, 10)\n",
    "        \n",
    "         # Initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity=\"leaky_relu\")\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict()\n",
      "OrderedDict([('running_mean', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))])\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "print(model._buffers)\n",
    "model.register_buffer('running_mean', torch.zeros(10))\n",
    "print(model._buffers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_parameter(self, name, param):\n",
    "        r\"\"\"Adds a parameter to the module.\n",
    "\n",
    "        The parameter can be accessed as an attribute using given name.\n",
    "\n",
    "        Args:\n",
    "            name (string): name of the parameter. The parameter can be accessed\n",
    "                from this module using the given name\n",
    "            parameter (Parameter): parameter to be added to the module.\n",
    "        \"\"\"\n",
    "        if '_parameters' not in self.__dict__:\n",
    "            raise AttributeError(\n",
    "                \"cannot assign parameter before Module.__init__() call\")\n",
    "\n",
    "        elif not isinstance(name, torch._six.string_classes):\n",
    "            raise TypeError(\"parameter name should be a string. \"\n",
    "                            \"Got {}\".format(torch.typename(name)))\n",
    "        elif '.' in name:\n",
    "            raise KeyError(\"parameter name can't contain \\\".\\\"\")\n",
    "        elif name == '':\n",
    "            raise KeyError(\"parameter name can't be empty string \\\"\\\"\")\n",
    "        elif hasattr(self, name) and name not in self._parameters:\n",
    "            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
    "\n",
    "        if param is None:\n",
    "            self._parameters[name] = None\n",
    "        elif not isinstance(param, Parameter):\n",
    "            raise TypeError(\"cannot assign '{}' object to parameter '{}' \"\n",
    "                            \"(torch.nn.Parameter or None required)\"\n",
    "                            .format(torch.typename(param), name))\n",
    "        elif param.grad_fn:\n",
    "            raise ValueError(\n",
    "                \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \"\n",
    "                \"parameters must be created explicitly. To express '{0}' \"\n",
    "                \"as a function of another Tensor, compute the value in \"\n",
    "                \"the forward() method.\".format(name))\n",
    "        else:\n",
    "            self._parameters[name] = param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signature: torch.nn.modules.Module.register_parameter(self, name, param)\n",
    "Docstring:\n",
    "Adds a parameter to the module.\n",
    "\n",
    "The parameter can be accessed as an attribute using given name.\n",
    "\n",
    "Args:\n",
    "    name (string): name of the parameter. The parameter can be accessed\n",
    "        from this module using the given name\n",
    "    parameter (Parameter): parameter to be added to the module.\n",
    "    \n",
    "File:      /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py  \n",
    "Type:      function  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모듈에 parameter를 붙인다.  \n",
    "prameter는 arribute로써 주어진 name을 통해서 접근될 수 있다.  \n",
    "\n",
    "Args:  \n",
    "    `name` (string) : 파라미터의 이름  \n",
    "    `prameter` (Parameter) : 파라미터는 module에 추가될 수 있다.  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=Model()\n",
    "modules = model.named_modules()\n",
    "\n",
    "print(model._parameters)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : weight, param : Parameter containing:\n",
      "tensor([[[[ 0.1256, -0.0537, -0.0008,  0.1376, -0.0072],\n",
      "          [-0.0000, -0.0756,  0.0890, -0.0943,  0.0257],\n",
      "          [-0.1106,  0.0491, -0.1002,  0.0427, -0.0406],\n",
      "          [-0.1177, -0.0019, -0.0125, -0.0154, -0.0017],\n",
      "          [ 0.1378, -0.0054,  0.0884,  0.0192,  0.1156]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0041,  0.0232, -0.0037, -0.0688, -0.0224],\n",
      "          [-0.0317, -0.0706,  0.0554,  0.0567,  0.0071],\n",
      "          [-0.0256, -0.0148,  0.0683, -0.0069,  0.0576],\n",
      "          [-0.0174, -0.0327,  0.0223,  0.0101, -0.1699],\n",
      "          [-0.0929, -0.0328,  0.0014, -0.0125,  0.0525]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0826, -0.0791,  0.0714,  0.0114, -0.0213],\n",
      "          [ 0.1119, -0.0747,  0.1620, -0.0542,  0.0526],\n",
      "          [-0.0772, -0.0358, -0.0468, -0.0163, -0.0189],\n",
      "          [ 0.0005, -0.1133,  0.0447,  0.0323,  0.1017],\n",
      "          [ 0.0520, -0.0589,  0.1569,  0.0322,  0.0377]]],\n",
      "\n",
      "\n",
      "        [[[-0.0178, -0.1064, -0.0632, -0.0355, -0.0400],\n",
      "          [ 0.0376, -0.0404, -0.0196,  0.0480,  0.0045],\n",
      "          [-0.0863,  0.0086,  0.0591, -0.0549,  0.0368],\n",
      "          [-0.0931, -0.0666,  0.0120, -0.0738,  0.0238],\n",
      "          [-0.0550, -0.0419, -0.0366,  0.0485, -0.0116]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0149, -0.0029,  0.0408, -0.0298,  0.0164],\n",
      "          [-0.0653,  0.1118, -0.0976, -0.0212, -0.0496],\n",
      "          [ 0.0933,  0.0743,  0.0466,  0.0261,  0.0197],\n",
      "          [ 0.0885, -0.0003,  0.0393,  0.0669, -0.0691],\n",
      "          [-0.0471,  0.0119,  0.1075, -0.0225,  0.0296]]],\n",
      "\n",
      "\n",
      "        [[[-0.0354,  0.1486,  0.0621, -0.0292, -0.0192],\n",
      "          [-0.0150,  0.0133, -0.0480,  0.0329, -0.0824],\n",
      "          [ 0.0503, -0.0702, -0.0315,  0.0209, -0.0183],\n",
      "          [-0.0068, -0.0890,  0.0276,  0.0559, -0.0885],\n",
      "          [ 0.0752,  0.0533, -0.0636, -0.0638,  0.0143]]],\n",
      "\n",
      "\n",
      "        [[[-0.0444, -0.0853,  0.1814, -0.0664,  0.1087],\n",
      "          [-0.0032, -0.0109, -0.1032, -0.0875, -0.0828],\n",
      "          [-0.0706,  0.0574, -0.0850, -0.0282, -0.0358],\n",
      "          [ 0.0989, -0.0723, -0.0818,  0.0247, -0.0199],\n",
      "          [ 0.1062,  0.0273, -0.0294, -0.0560,  0.0158]]],\n",
      "\n",
      "\n",
      "        [[[-0.0599,  0.1448, -0.0180, -0.0411,  0.0525],\n",
      "          [ 0.0005, -0.0032,  0.0258, -0.0001,  0.1335],\n",
      "          [-0.0185,  0.0707, -0.0060, -0.0000, -0.0431],\n",
      "          [ 0.0446, -0.0211, -0.0892, -0.0148,  0.0285],\n",
      "          [ 0.0478, -0.0137,  0.0134,  0.0450,  0.0575]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0459, -0.0338, -0.0929, -0.0773,  0.0303],\n",
      "          [-0.0289,  0.0066,  0.0245,  0.0308, -0.0078],\n",
      "          [-0.0613, -0.0085, -0.1193, -0.0452, -0.0688],\n",
      "          [-0.0042, -0.1438, -0.1033,  0.0278, -0.0569],\n",
      "          [-0.0761,  0.0741, -0.0218, -0.0640, -0.0493]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0654,  0.0208,  0.0603, -0.0741,  0.0560],\n",
      "          [ 0.0064,  0.1190, -0.0454, -0.0376,  0.0250],\n",
      "          [-0.0391, -0.0653, -0.0147, -0.0294, -0.0522],\n",
      "          [-0.0822, -0.0480, -0.0381, -0.0218,  0.0947],\n",
      "          [ 0.0000, -0.0006,  0.0867, -0.0003,  0.0948]]],\n",
      "\n",
      "\n",
      "        [[[-0.0643, -0.0222, -0.1759, -0.0424, -0.0298],\n",
      "          [-0.0294,  0.0153, -0.0745,  0.0684,  0.0049],\n",
      "          [-0.0780,  0.0023,  0.0121,  0.0747, -0.0334],\n",
      "          [ 0.0215, -0.0508, -0.0521, -0.0436,  0.0012],\n",
      "          [-0.0178,  0.0843, -0.0529, -0.0156, -0.0384]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0388, -0.1492, -0.1971, -0.0205,  0.0958],\n",
      "          [ 0.0389, -0.0970, -0.0199, -0.0021,  0.0228],\n",
      "          [-0.0870, -0.0616,  0.0473,  0.0321,  0.0662],\n",
      "          [ 0.0085, -0.0163, -0.0113, -0.0239, -0.0216],\n",
      "          [-0.0541,  0.0386, -0.0333, -0.0751, -0.0083]]],\n",
      "\n",
      "\n",
      "        [[[-0.0275,  0.0369, -0.1610, -0.0649, -0.1135],\n",
      "          [ 0.0110, -0.0284,  0.0561,  0.0143, -0.1097],\n",
      "          [-0.0792, -0.0772, -0.0228,  0.0657,  0.1354],\n",
      "          [-0.0202, -0.0359,  0.0030,  0.0126, -0.0447],\n",
      "          [ 0.0873,  0.0513,  0.0630, -0.0674,  0.0488]]],\n",
      "\n",
      "\n",
      "        [[[-0.0403,  0.0245, -0.0529,  0.0020, -0.0462],\n",
      "          [ 0.1117,  0.0742,  0.0299,  0.0846, -0.0912],\n",
      "          [ 0.1352, -0.0842,  0.0734,  0.0166, -0.1324],\n",
      "          [ 0.0180,  0.0349,  0.0623, -0.0129, -0.1129],\n",
      "          [-0.0269, -0.0660, -0.0103,  0.1514, -0.0020]]],\n",
      "\n",
      "\n",
      "        [[[-0.0938,  0.0263, -0.0412, -0.1515, -0.0085],\n",
      "          [ 0.0694, -0.1193, -0.0344, -0.0504, -0.1001],\n",
      "          [ 0.1276, -0.0271,  0.0371,  0.0234, -0.0945],\n",
      "          [-0.0241, -0.0289,  0.0231, -0.1003, -0.0057],\n",
      "          [ 0.0333,  0.0401, -0.0421,  0.0298, -0.0484]]],\n",
      "\n",
      "\n",
      "        [[[-0.0966, -0.0584, -0.0690, -0.0748,  0.0240],\n",
      "          [-0.0614, -0.0001, -0.0314,  0.1531, -0.0372],\n",
      "          [ 0.1148,  0.0369, -0.0528, -0.0605, -0.0640],\n",
      "          [ 0.0219,  0.0192, -0.0370, -0.0150,  0.0838],\n",
      "          [ 0.1060,  0.0552,  0.0346, -0.0129,  0.0107]]],\n",
      "\n",
      "\n",
      "        [[[-0.0585,  0.0225, -0.0175,  0.1201, -0.0583],\n",
      "          [ 0.0405, -0.0708, -0.0234,  0.1347,  0.0436],\n",
      "          [-0.1087,  0.0083, -0.0691, -0.0898, -0.0362],\n",
      "          [ 0.0635, -0.0522,  0.0860, -0.0259,  0.1311],\n",
      "          [ 0.0434, -0.0265, -0.0413, -0.0187, -0.1387]]],\n",
      "\n",
      "\n",
      "        [[[-0.0019, -0.0127, -0.0183,  0.0228,  0.0114],\n",
      "          [-0.0185, -0.0503, -0.0934,  0.0367,  0.0124],\n",
      "          [-0.0859,  0.0650,  0.0200, -0.0352,  0.0011],\n",
      "          [-0.0105, -0.0219,  0.0383,  0.0302,  0.0697],\n",
      "          [ 0.0196, -0.0573, -0.0404,  0.0490,  0.0726]]],\n",
      "\n",
      "\n",
      "        [[[-0.0120, -0.0771, -0.0415, -0.0984,  0.0722],\n",
      "          [ 0.0220, -0.0792, -0.0338, -0.0907, -0.1293],\n",
      "          [-0.0586, -0.0410,  0.0550, -0.0009,  0.0450],\n",
      "          [ 0.0204, -0.0176,  0.0036,  0.0400, -0.0329],\n",
      "          [ 0.0285,  0.0556, -0.1273,  0.0170,  0.0046]]],\n",
      "\n",
      "\n",
      "        [[[-0.0382,  0.0865,  0.0304,  0.0561,  0.0626],\n",
      "          [ 0.0038,  0.0540, -0.0964, -0.1662,  0.1111],\n",
      "          [-0.0368, -0.0226,  0.0221,  0.0865, -0.0767],\n",
      "          [ 0.0533, -0.0799,  0.0689,  0.0715, -0.0031],\n",
      "          [-0.0206,  0.0916, -0.0501, -0.0446, -0.0427]]]],\n",
      "       requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : bias, param : Parameter containing:\n",
      "tensor([-0.1673,  0.1089,  0.0201, -0.1256,  0.0507,  0.0614,  0.1886, -0.1630,\n",
      "        -0.1828,  0.0027, -0.1125,  0.1045,  0.0243, -0.1909, -0.0251, -0.0465,\n",
      "         0.0234,  0.0568, -0.1818,  0.0438], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, module in modules:        \n",
    "    if name == \"conv1\":\n",
    "        for name, param in module._parameters.items():\n",
    "            print(type(param))\n",
    "            print(\"name : {}, param : {}\".format(name, param))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : Parameter containing:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], requires_grad=True)\n",
      "\n",
      "inserted parameter in model : OrderedDict([('test', Parameter containing:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], requires_grad=True))])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = torch.nn.Parameter(data=torch.Tensor(data=[[1, 2, 3], [4, 5, 6]]), requires_grad=True)\n",
    "print(\"test : {}\".format(test))\n",
    "print()\n",
    "\n",
    "\n",
    "model.register_parameter(\"test\", test)\n",
    "print(\"inserted parameter in model : {}\".format(model._parameters))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`string`으로 구성된 name과 `Parameter` class로 구성된 parameter값이 `torch.nn.modules.Module class`의 필드 `_parateters`에 `OrderedDict()` 형태로 저장되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_module(self, name, module):\n",
    "        r\"\"\"Adds a child module to the current module.\n",
    "\n",
    "        The module can be accessed as an attribute using the given name.\n",
    "\n",
    "        Args:\n",
    "            name (string): name of the child module. The child module can be\n",
    "                accessed from this module using the given name\n",
    "            parameter (Module): child module to be added to the module.\n",
    "        \"\"\"\n",
    "        if not isinstance(module, Module) and module is not None:\n",
    "            raise TypeError(\"{} is not a Module subclass\".format(\n",
    "                torch.typename(module)))\n",
    "        elif not isinstance(name, torch._six.string_classes):\n",
    "            raise TypeError(\"module name should be a string. Got {}\".format(\n",
    "                torch.typename(name)))\n",
    "        elif hasattr(self, name) and name not in self._modules:\n",
    "            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
    "        elif '.' in name:\n",
    "            raise KeyError(\"module name can't contain \\\".\\\"\")\n",
    "        elif name == '':\n",
    "            raise KeyError(\"module name can't be empty string \\\"\\\"\")\n",
    "        self._modules[name] = module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signature: torch.nn.modules.Module.add_module(self, name, module)  \n",
    "Docstring:  \n",
    "Adds a child module to the current module.  \n",
    "  \n",
    "The module can be accessed as an attribute using the given name.  \n",
    "    \n",
    "Args:  \n",
    "    `name` (string): name of the child module. The child module can be  \n",
    "        accessed from this module using the given name  \n",
    "    `parameter` (Module): child module to be added to the module.  \n",
    "        \n",
    "File:      /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py  \n",
    "Type:      function  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 모듈에 child 모듈을 추가한다.  \n",
    "\n",
    "해당 모듈은 name이라는 attribute로 접근될 수 있다.  \n",
    "\n",
    "`name`(string) : 이름\n",
    "`parameter` (Module) : 추가되는 child module이다.\n",
    "    \n",
    "함수 자체에     \n",
    "- `subclass` 가능한 모듈 확인\n",
    "- `name`이 string인제 확인\n",
    "- `name`이 중복되지 않는지 확인\n",
    "- `name`에 `.`이 들어가는지 확인\n",
    "- `name`이 공백문자`\"\"`인지 확인  \n",
    "    \n",
    "과 같은 assert를 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1', Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))), ('conv2', Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))), ('fc1', Linear(in_features=800, out_features=500, bias=True)), ('fc2', Linear(in_features=500, out_features=10, bias=True))])\n",
      "\n",
      "OrderedDict([('conv1', Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))), ('conv2', Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))), ('fc1', Linear(in_features=800, out_features=500, bias=True)), ('fc2', Linear(in_features=500, out_features=10, bias=True)), ('conv3', Conv2d(50, 50, kernel_size=(5, 5), stride=(1, 1)))])\n"
     ]
    }
   ],
   "source": [
    "model=Model()\n",
    "\n",
    "print(model._modules)\n",
    "print()\n",
    "model.add_module(\"conv3\", torch.nn.Conv2d(50, 50, kernel_size=(5, 5), stride=(1,1)))\n",
    "print(model._modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 인자는 내 모듈과 sub module에 대한 관계를 표현한다.  \n",
    "\n",
    "그 이후부터는 `name`:`Module` 형태의 key-value 형태로  \n",
    "`_modules`라는 필드에 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.modules.Module.add_module?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
