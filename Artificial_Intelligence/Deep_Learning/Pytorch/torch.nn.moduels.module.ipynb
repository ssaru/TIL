{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = torch.nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = torch.nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = torch.nn.Linear(500, 10)            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('__call__', <function Module.__call__ at 0x7f0608805268>)\n",
      "\n",
      "('__class__', <class 'type'>)\n",
      "\n",
      "('__delattr__', <function Module.__delattr__ at 0x7f0608805488>)\n",
      "\n",
      "('__dict__', mappingproxy({'__module__': 'torch.nn.modules.module', '__doc__': 'Base class for all neural network modules.\\n\\n    Your models should also subclass this class.\\n\\n    Modules can also contain other Modules, allowing to nest them in\\n    a tree structure. You can assign the submodules as regular attributes::\\n\\n        import torch.nn as nn\\n        import torch.nn.functional as F\\n\\n        class Model(nn.Module):\\n            def __init__(self):\\n                super(Model, self).__init__()\\n                self.conv1 = nn.Conv2d(1, 20, 5)\\n                self.conv2 = nn.Conv2d(20, 20, 5)\\n\\n            def forward(self, x):\\n               x = F.relu(self.conv1(x))\\n               return F.relu(self.conv2(x))\\n\\n    Submodules assigned in this way will be registered, and will have their\\n    parameters converted too when you call `.cuda()`, etc.\\n    ', 'dump_patches': False, '_version': 1, '__init__': <function Module.__init__ at 0x7f06087f67b8>, 'forward': <function Module.forward at 0x7f06087f6840>, 'register_buffer': <function Module.register_buffer at 0x7f06087f68c8>, 'register_parameter': <function Module.register_parameter at 0x7f06087f6950>, 'add_module': <function Module.add_module at 0x7f06087f69d8>, '_apply': <function Module._apply at 0x7f06087f6a60>, 'apply': <function Module.apply at 0x7f06087f6ae8>, 'cuda': <function Module.cuda at 0x7f06087f6b70>, 'cpu': <function Module.cpu at 0x7f06087f6bf8>, 'type': <function Module.type at 0x7f06087f6c80>, 'float': <function Module.float at 0x7f06087f6d08>, 'double': <function Module.double at 0x7f06087f6d90>, 'half': <function Module.half at 0x7f06087f6e18>, 'to': <function Module.to at 0x7f06087f6ea0>, 'register_backward_hook': <function Module.register_backward_hook at 0x7f06087f6f28>, 'register_forward_pre_hook': <function Module.register_forward_pre_hook at 0x7f0608805048>, 'register_forward_hook': <function Module.register_forward_hook at 0x7f06088050d0>, '_tracing_name': <function Module._tracing_name at 0x7f0608805158>, '_slow_forward': <function Module._slow_forward at 0x7f06088051e0>, '__call__': <function Module.__call__ at 0x7f0608805268>, '__setstate__': <function Module.__setstate__ at 0x7f06088052f0>, '__getattr__': <function Module.__getattr__ at 0x7f0608805378>, '__setattr__': <function Module.__setattr__ at 0x7f0608805400>, '__delattr__': <function Module.__delattr__ at 0x7f0608805488>, 'state_dict': <function Module.state_dict at 0x7f0608805510>, '_load_from_state_dict': <function Module._load_from_state_dict at 0x7f0608805598>, 'load_state_dict': <function Module.load_state_dict at 0x7f0608805620>, 'parameters': <function Module.parameters at 0x7f06088056a8>, 'named_parameters': <function Module.named_parameters at 0x7f0608805730>, '_all_buffers': <function Module._all_buffers at 0x7f06088057b8>, 'children': <function Module.children at 0x7f0608805840>, 'named_children': <function Module.named_children at 0x7f06088058c8>, 'modules': <function Module.modules at 0x7f0608805950>, 'named_modules': <function Module.named_modules at 0x7f06088059d8>, 'train': <function Module.train at 0x7f0608805a60>, 'eval': <function Module.eval at 0x7f0608805ae8>, 'zero_grad': <function Module.zero_grad at 0x7f0608805b70>, 'share_memory': <function Module.share_memory at 0x7f0608805bf8>, '_get_name': <function Module._get_name at 0x7f0608805c80>, 'extra_repr': <function Module.extra_repr at 0x7f0608805d08>, '__repr__': <function Module.__repr__ at 0x7f0608805d90>, '__dir__': <function Module.__dir__ at 0x7f0608805e18>, '__dict__': <attribute '__dict__' of 'Module' objects>, '__weakref__': <attribute '__weakref__' of 'Module' objects>}))\n",
      "\n",
      "('__dir__', <function Module.__dir__ at 0x7f0608805e18>)\n",
      "\n",
      "('__doc__', 'Base class for all neural network modules.\\n\\n    Your models should also subclass this class.\\n\\n    Modules can also contain other Modules, allowing to nest them in\\n    a tree structure. You can assign the submodules as regular attributes::\\n\\n        import torch.nn as nn\\n        import torch.nn.functional as F\\n\\n        class Model(nn.Module):\\n            def __init__(self):\\n                super(Model, self).__init__()\\n                self.conv1 = nn.Conv2d(1, 20, 5)\\n                self.conv2 = nn.Conv2d(20, 20, 5)\\n\\n            def forward(self, x):\\n               x = F.relu(self.conv1(x))\\n               return F.relu(self.conv2(x))\\n\\n    Submodules assigned in this way will be registered, and will have their\\n    parameters converted too when you call `.cuda()`, etc.\\n    ')\n",
      "\n",
      "('__eq__', <slot wrapper '__eq__' of 'object' objects>)\n",
      "\n",
      "('__format__', <method '__format__' of 'object' objects>)\n",
      "\n",
      "('__ge__', <slot wrapper '__ge__' of 'object' objects>)\n",
      "\n",
      "('__getattr__', <function Module.__getattr__ at 0x7f0608805378>)\n",
      "\n",
      "('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>)\n",
      "\n",
      "('__gt__', <slot wrapper '__gt__' of 'object' objects>)\n",
      "\n",
      "('__hash__', <slot wrapper '__hash__' of 'object' objects>)\n",
      "\n",
      "('__init__', <function Module.__init__ at 0x7f06087f67b8>)\n",
      "\n",
      "('__init_subclass__', <built-in method __init_subclass__ of type object at 0x31ec178>)\n",
      "\n",
      "('__le__', <slot wrapper '__le__' of 'object' objects>)\n",
      "\n",
      "('__lt__', <slot wrapper '__lt__' of 'object' objects>)\n",
      "\n",
      "('__module__', 'torch.nn.modules.module')\n",
      "\n",
      "('__ne__', <slot wrapper '__ne__' of 'object' objects>)\n",
      "\n",
      "('__new__', <built-in method __new__ of type object at 0x9e3840>)\n",
      "\n",
      "('__reduce__', <method '__reduce__' of 'object' objects>)\n",
      "\n",
      "('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>)\n",
      "\n",
      "('__repr__', <function Module.__repr__ at 0x7f0608805d90>)\n",
      "\n",
      "('__setattr__', <function Module.__setattr__ at 0x7f0608805400>)\n",
      "\n",
      "('__setstate__', <function Module.__setstate__ at 0x7f06088052f0>)\n",
      "\n",
      "('__sizeof__', <method '__sizeof__' of 'object' objects>)\n",
      "\n",
      "('__str__', <slot wrapper '__str__' of 'object' objects>)\n",
      "\n",
      "('__subclasshook__', <built-in method __subclasshook__ of type object at 0x31ec178>)\n",
      "\n",
      "('__weakref__', <attribute '__weakref__' of 'Module' objects>)\n",
      "\n",
      "('_all_buffers', <function Module._all_buffers at 0x7f06088057b8>)\n",
      "\n",
      "('_apply', <function Module._apply at 0x7f06087f6a60>)\n",
      "\n",
      "('_get_name', <function Module._get_name at 0x7f0608805c80>)\n",
      "\n",
      "('_load_from_state_dict', <function Module._load_from_state_dict at 0x7f0608805598>)\n",
      "\n",
      "('_slow_forward', <function Module._slow_forward at 0x7f06088051e0>)\n",
      "\n",
      "('_tracing_name', <function Module._tracing_name at 0x7f0608805158>)\n",
      "\n",
      "('_version', 1)\n",
      "\n",
      "('add_module', <function Module.add_module at 0x7f06087f69d8>)\n",
      "\n",
      "('apply', <function Module.apply at 0x7f06087f6ae8>)\n",
      "\n",
      "('children', <function Module.children at 0x7f0608805840>)\n",
      "\n",
      "('cpu', <function Module.cpu at 0x7f06087f6bf8>)\n",
      "\n",
      "('cuda', <function Module.cuda at 0x7f06087f6b70>)\n",
      "\n",
      "('double', <function Module.double at 0x7f06087f6d90>)\n",
      "\n",
      "('dump_patches', False)\n",
      "\n",
      "('eval', <function Module.eval at 0x7f0608805ae8>)\n",
      "\n",
      "('extra_repr', <function Module.extra_repr at 0x7f0608805d08>)\n",
      "\n",
      "('float', <function Module.float at 0x7f06087f6d08>)\n",
      "\n",
      "('forward', <function Module.forward at 0x7f06087f6840>)\n",
      "\n",
      "('half', <function Module.half at 0x7f06087f6e18>)\n",
      "\n",
      "('load_state_dict', <function Module.load_state_dict at 0x7f0608805620>)\n",
      "\n",
      "('modules', <function Module.modules at 0x7f0608805950>)\n",
      "\n",
      "('named_children', <function Module.named_children at 0x7f06088058c8>)\n",
      "\n",
      "('named_modules', <function Module.named_modules at 0x7f06088059d8>)\n",
      "\n",
      "('named_parameters', <function Module.named_parameters at 0x7f0608805730>)\n",
      "\n",
      "('parameters', <function Module.parameters at 0x7f06088056a8>)\n",
      "\n",
      "('register_backward_hook', <function Module.register_backward_hook at 0x7f06087f6f28>)\n",
      "\n",
      "('register_buffer', <function Module.register_buffer at 0x7f06087f68c8>)\n",
      "\n",
      "('register_forward_hook', <function Module.register_forward_hook at 0x7f06088050d0>)\n",
      "\n",
      "('register_forward_pre_hook', <function Module.register_forward_pre_hook at 0x7f0608805048>)\n",
      "\n",
      "('register_parameter', <function Module.register_parameter at 0x7f06087f6950>)\n",
      "\n",
      "('share_memory', <function Module.share_memory at 0x7f0608805bf8>)\n",
      "\n",
      "('state_dict', <function Module.state_dict at 0x7f0608805510>)\n",
      "\n",
      "('to', <function Module.to at 0x7f06087f6ea0>)\n",
      "\n",
      "('train', <function Module.train at 0x7f0608805a60>)\n",
      "\n",
      "('type', <function Module.type at 0x7f06087f6c80>)\n",
      "\n",
      "('zero_grad', <function Module.zero_grad at 0x7f0608805b70>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for element in inspect.getmembers(torch.nn.modules.Module):\n",
    "    print(\"{}\\n\".format(element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch `Module` class\n",
    "\n",
    "![torch nn modules module](https://user-images.githubusercontent.com/13328380/51440701-33105700-1d0d-11e9-9611-6bd5c517433c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "1. `:meth:`는 `method`를 의미함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _addindent(s_, numSpaces):\n",
    "    s = s_.split('\\n')\n",
    "    # don't do anything for single-line stuff\n",
    "    if len(s) == 1:\n",
    "        return s_\n",
    "    first = s.pop(0)\n",
    "    s = [(numSpaces * ' ') + line for line in s]\n",
    "    s = '\\n'.join(s)\n",
    "    s = first + '\\n' + s\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__repr__`에서 사용한다. 나중에 보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init signature: torch.nn.modules.Module()\n",
    "Docstring:     \n",
    "## Base class for all neural network modules.\n",
    "\n",
    "Your models should also subclass this class.\n",
    "\n",
    "Modules can also contain other Modules, allowing to nest them in\n",
    "a tree structure. You can assign the submodules as regular attributes::\n",
    "```python\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Model, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "        def forward(self, x):\n",
    "           x = F.relu(self.conv1(x))\n",
    "           return F.relu(self.conv2(x))\n",
    "```\n",
    "\n",
    "Submodules assigned in this way will be registered, and will have their\n",
    "parameters converted too when you call :meth:`to`, etc.\n",
    "    \n",
    "File:           ~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py  \n",
    "Type:           type  \n",
    "Subclasses:     Linear, Bilinear, _ConvNd, Threshold, RReLU, Hardtanh, Sigmoid, Tanh, ELU, CELU, ...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field\n",
    "# what is mean?\n",
    "dump_patches = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field\n",
    "_version = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows better BC support for :meth:`load_state_dict`. In\n",
    "    :meth:`state_dict`, the version number will be saved as in the attribute\n",
    "    `_metadata` of the returned state dict, and thus pickled. `_metadata` is a\n",
    "    dictionary with keys that follow the naming convention of state dict. See\n",
    "    ``_load_from_state_dict`` on how to use this information in loading.\n",
    "\n",
    "    If new parameters/buffers are added/removed from a module, this number shall\n",
    "    be bumped, and the module's `_load_from_state_dict` method can compare the\n",
    "    version number and do appropriate changes if the state dict is from before\n",
    "    the change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_state_dict` method에 대한 BC 지원이 향상된다. `state_dict` method에서, 버전 번호는 return된 state dict의 `_metadata` 내부의 속성으로써 저장될 것이고, pickled된다.\n",
    "\n",
    "`_metadata`는 state dict의 naming convention을 따르는 key가 있는 dictionary다. 해당 정보를 어떻게 사용하는지 보고싶으면`_load_from_state_dict`을 봐라.\n",
    "\n",
    "만약 모듈에서 새로운 파라미터/버퍼가 추가되거나 제거된다면 해당 숫자는 충돌을 일으켜야한다.\n",
    "모듈의 `_load_from_state_dict` method는 버전 번호를 비교할 수 있고, 만약에 state dict이 이전 버전에서 온 경우, 이를 적절하게 변경할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "        self._backend = thnn_backend\n",
    "        self._parameters = OrderedDict()\n",
    "        self._buffers = OrderedDict()\n",
    "        self._backward_hooks = OrderedDict()\n",
    "        self._forward_hooks = OrderedDict()\n",
    "        self._forward_pre_hooks = OrderedDict()\n",
    "        self._state_dict_hooks = OrderedDict()\n",
    "        self._load_state_dict_pre_hooks = OrderedDict()\n",
    "        self._modules = OrderedDict()\n",
    "        self.training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " def forward(self, *input):\n",
    "        r\"\"\"Defines the computation performed at every call.\n",
    "\n",
    "        Should be overridden by all subclasses.\n",
    "\n",
    "        .. note::\n",
    "            Although the recipe for forward pass needs to be defined within\n",
    "            this function, one should call the :class:`Module` instance afterwards\n",
    "            instead of this since the former takes care of running the\n",
    "            registered hooks while the latter silently ignores them.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signature: torch.nn.modules.Module.forward(self, *input)\n",
    "Docstring:\n",
    "Defines the computation performed at every call.\n",
    "\n",
    "Should be overridden by all subclasses.\n",
    "\n",
    ".. note::\n",
    "    Although the recipe for forward pass needs to be defined within\n",
    "    this function, one should call the :class:`Module` instance afterwards\n",
    "    instead of this since the former takes care of running the\n",
    "    registered hooks while the latter silently ignores them.  \n",
    "    \n",
    "File:      ~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py  \n",
    "Type:      function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 subclass들에 의해서 override되어야 한다.\n",
    "\n",
    "_**무슨 말일까?....**_\n",
    ">forwad pass를 위한 recipe는 `forward` method와 같이 정의되어야하지만  latter는 등록된 hook을 조용히 무시하는 반면에 former는 등록된 hook을 실행하기 때문에 Module Class instance는 나중에 호출할 필요가 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# register_buffer method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def register_buffer(self, name, tensor):\n",
    "        r\"\"\"Adds a persistent buffer to the module.\n",
    "\n",
    "        This is typically used to register a buffer that should not to be\n",
    "        considered a model parameter. For example, BatchNorm's ``running_mean``\n",
    "        is not a parameter, but is part of the persistent state.\n",
    "\n",
    "        Buffers can be accessed as attributes using given names.\n",
    "\n",
    "        Args:\n",
    "            name (string): name of the buffer. The buffer can be accessed\n",
    "                from this module using the given name\n",
    "            tensor (Tensor): buffer to be registered.\n",
    "\n",
    "        Example::\n",
    "\n",
    "            >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "\n",
    "        \"\"\"\n",
    "        if not isinstance(name, torch._six.string_classes):\n",
    "            raise TypeError(\"buffer name should be a string. \"\n",
    "                            \"Got {}\".format(torch.typename(name)))\n",
    "        elif '.' in name:\n",
    "            raise KeyError(\"buffer name can't contain \\\".\\\"\")\n",
    "        elif name == '':\n",
    "            raise KeyError(\"buffer name can't be empty string \\\"\\\"\")\n",
    "        elif hasattr(self, name) and name not in self._buffers:\n",
    "            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
    "        elif tensor is not None and not isinstance(tensor, torch.Tensor):\n",
    "            raise TypeError(\"cannot assign '{}' object to buffer '{}' \"\n",
    "                            \"(torch Tensor or None required)\"\n",
    "                            .format(torch.typename(tensor), name))\n",
    "        else:\n",
    "            self._buffers[name] = tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signature: torch.nn.modules.Module.register_buffer(self, name, tensor)\n",
    "Docstring:\n",
    "Adds a persistent buffer to the module.\n",
    "\n",
    "This is typically used to register a buffer that should not to be\n",
    "considered a model parameter. For example, BatchNorm's ``running_mean``\n",
    "is not a parameter, but is part of the persistent state.\n",
    "\n",
    "Buffers can be accessed as attributes using given names.\n",
    "\n",
    "Args:\n",
    "    name (string): name of the buffer. The buffer can be accessed\n",
    "        from this module using the given name\n",
    "    tensor (Tensor): buffer to be registered.\n",
    "\n",
    "Example::\n",
    "\n",
    "    >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "    \n",
    "File:      ~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py  \n",
    "Type:      function  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 함수는 특별하게 model parameter로 고려되면 안되는 파라미터(buffer)를 등록하는데 사용된다.\n",
    "> 예를들어서 BatchNorm의 `running_mean`은 prameter가 아니지만 지속성있는 상태(persistent state)의 일부다.\n",
    "\n",
    "Buffer들은 주어진 이름을 사용해서 attribute로써 접근될 수 있다.\n",
    "\n",
    "Args: name(string): buffer의 이름이다. Buffer는 지정할 이름과, Tensor를 이용해서 등록할 수 있다.\n",
    "Example ::\n",
    "\n",
    "```python\n",
    "self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict()\n",
      "OrderedDict([('running_mean', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))])\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "print(model._buffers)\n",
    "model.register_buffer('running_mean', torch.zeros(10))\n",
    "print(model._buffers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# register_parameter method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_parameter(self, name, param):\n",
    "        r\"\"\"Adds a parameter to the module.\n",
    "\n",
    "        The parameter can be accessed as an attribute using given name.\n",
    "\n",
    "        Args:\n",
    "            name (string): name of the parameter. The parameter can be accessed\n",
    "                from this module using the given name\n",
    "            parameter (Parameter): parameter to be added to the module.\n",
    "        \"\"\"\n",
    "        if '_parameters' not in self.__dict__:\n",
    "            raise AttributeError(\n",
    "                \"cannot assign parameter before Module.__init__() call\")\n",
    "\n",
    "        elif not isinstance(name, torch._six.string_classes):\n",
    "            raise TypeError(\"parameter name should be a string. \"\n",
    "                            \"Got {}\".format(torch.typename(name)))\n",
    "        elif '.' in name:\n",
    "            raise KeyError(\"parameter name can't contain \\\".\\\"\")\n",
    "        elif name == '':\n",
    "            raise KeyError(\"parameter name can't be empty string \\\"\\\"\")\n",
    "        elif hasattr(self, name) and name not in self._parameters:\n",
    "            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
    "\n",
    "        if param is None:\n",
    "            self._parameters[name] = None\n",
    "        elif not isinstance(param, Parameter):\n",
    "            raise TypeError(\"cannot assign '{}' object to parameter '{}' \"\n",
    "                            \"(torch.nn.Parameter or None required)\"\n",
    "                            .format(torch.typename(param), name))\n",
    "        elif param.grad_fn:\n",
    "            raise ValueError(\n",
    "                \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \"\n",
    "                \"parameters must be created explicitly. To express '{0}' \"\n",
    "                \"as a function of another Tensor, compute the value in \"\n",
    "                \"the forward() method.\".format(name))\n",
    "        else:\n",
    "            self._parameters[name] = param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signature: torch.nn.modules.Module.register_parameter(self, name, param)\n",
    "Docstring:\n",
    "Adds a parameter to the module.\n",
    "\n",
    "The parameter can be accessed as an attribute using given name.\n",
    "\n",
    "Args:\n",
    "    name (string): name of the parameter. The parameter can be accessed\n",
    "        from this module using the given name\n",
    "    parameter (Parameter): parameter to be added to the module.\n",
    "    \n",
    "File:      /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py  \n",
    "Type:      function  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모듈에 parameter를 붙인다.  \n",
    "prameter는 arribute로써 주어진 name을 통해서 접근될 수 있다.  \n",
    "\n",
    "Args:  \n",
    "    `name` (string) : 파라미터의 이름  \n",
    "    `prameter` (Parameter) : 파라미터는 module에 추가될 수 있다.  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=Model()\n",
    "modules = model.named_modules()\n",
    "\n",
    "print(model._parameters)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : weight, param : Parameter containing:\n",
      "tensor([[[[-0.0907, -0.0802,  0.0011,  0.0006, -0.1596],\n",
      "          [ 0.0190,  0.0878,  0.0035,  0.1024, -0.0985],\n",
      "          [ 0.0068, -0.0525, -0.0273,  0.0430,  0.0790],\n",
      "          [-0.0674, -0.0540,  0.0403,  0.0758,  0.0374],\n",
      "          [ 0.0119,  0.0617,  0.1093, -0.0600, -0.0122]]],\n",
      "\n",
      "\n",
      "        [[[-0.0442, -0.0499, -0.0537,  0.0008, -0.0538],\n",
      "          [-0.0807,  0.0216,  0.0038,  0.0651,  0.0333],\n",
      "          [-0.1334,  0.0040,  0.0141, -0.1223, -0.0221],\n",
      "          [ 0.1278, -0.0557,  0.0303, -0.0271, -0.0429],\n",
      "          [-0.0381,  0.1114, -0.1150,  0.0146, -0.0388]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0156,  0.0320,  0.0116,  0.1366, -0.0394],\n",
      "          [ 0.0497,  0.1084,  0.0228, -0.0436, -0.1416],\n",
      "          [ 0.0698,  0.0313,  0.0017, -0.0645, -0.0234],\n",
      "          [-0.0484,  0.0193, -0.0515, -0.0593, -0.0158],\n",
      "          [ 0.0463,  0.1613, -0.0504,  0.0242, -0.0445]]],\n",
      "\n",
      "\n",
      "        [[[-0.0458, -0.0600,  0.0449,  0.0098,  0.0069],\n",
      "          [ 0.0337,  0.0804, -0.0092, -0.1067, -0.0246],\n",
      "          [ 0.1044, -0.0717,  0.0088,  0.0454, -0.0273],\n",
      "          [ 0.0076,  0.0550, -0.1795,  0.0175,  0.0090],\n",
      "          [-0.1275,  0.0004, -0.0885,  0.0386,  0.0106]]],\n",
      "\n",
      "\n",
      "        [[[-0.0092, -0.0174, -0.0194, -0.0232,  0.0034],\n",
      "          [ 0.0215, -0.1041,  0.0059,  0.0323, -0.0889],\n",
      "          [ 0.1243,  0.0986,  0.0812, -0.0506, -0.0032],\n",
      "          [-0.0141, -0.0375, -0.0466, -0.0036,  0.0206],\n",
      "          [-0.0518,  0.0086,  0.0815, -0.0697, -0.0563]]],\n",
      "\n",
      "\n",
      "        [[[-0.0331, -0.0663,  0.0476,  0.0333,  0.0832],\n",
      "          [ 0.0427,  0.0163,  0.0531,  0.0575,  0.0104],\n",
      "          [ 0.0246, -0.0491, -0.0183,  0.0156,  0.0518],\n",
      "          [-0.0515,  0.0539,  0.0255, -0.0656, -0.0454],\n",
      "          [-0.0435,  0.0292, -0.0302,  0.0650,  0.1404]]],\n",
      "\n",
      "\n",
      "        [[[-0.0452,  0.0382,  0.0553,  0.1191,  0.1735],\n",
      "          [ 0.0522, -0.0235, -0.0266,  0.1185,  0.0572],\n",
      "          [-0.1053, -0.0312, -0.0359, -0.0174,  0.0299],\n",
      "          [-0.0997,  0.0913, -0.0542, -0.0027, -0.0654],\n",
      "          [ 0.0190,  0.0651,  0.0216,  0.0091, -0.0051]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0495,  0.0518,  0.0592,  0.0859,  0.0553],\n",
      "          [-0.1052, -0.0555, -0.0095,  0.0311,  0.0307],\n",
      "          [ 0.0029, -0.0307,  0.0851, -0.0320, -0.0169],\n",
      "          [ 0.0667, -0.0196,  0.0127, -0.1128, -0.0676],\n",
      "          [-0.1041, -0.0699, -0.0876, -0.0634, -0.0550]]],\n",
      "\n",
      "\n",
      "        [[[-0.0261,  0.0116,  0.1093,  0.0278, -0.0672],\n",
      "          [-0.0936,  0.1538,  0.0150,  0.1290,  0.1250],\n",
      "          [-0.0667,  0.0961,  0.0147, -0.0068,  0.0627],\n",
      "          [-0.0302, -0.0380,  0.0843,  0.0200,  0.0141],\n",
      "          [-0.0252,  0.0339, -0.0927,  0.0680,  0.0806]]],\n",
      "\n",
      "\n",
      "        [[[-0.0444,  0.0209,  0.0687, -0.0230, -0.0107],\n",
      "          [ 0.1046,  0.0816, -0.0155, -0.0227,  0.0622],\n",
      "          [-0.0905,  0.0336,  0.0139,  0.0450, -0.0398],\n",
      "          [-0.0140, -0.0089,  0.0061, -0.0050, -0.1046],\n",
      "          [ 0.1850,  0.0132, -0.0472,  0.1052,  0.0585]]],\n",
      "\n",
      "\n",
      "        [[[-0.0168, -0.0450,  0.0732, -0.0191,  0.0450],\n",
      "          [ 0.0327,  0.0113,  0.0380, -0.0262, -0.0170],\n",
      "          [-0.0519,  0.2087, -0.0437,  0.0638, -0.1500],\n",
      "          [-0.0140,  0.0603,  0.0612, -0.0472, -0.0856],\n",
      "          [-0.0304, -0.0292, -0.0230, -0.1106,  0.1392]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0098,  0.0397, -0.0563, -0.0286, -0.0296],\n",
      "          [-0.0424, -0.0455, -0.0748,  0.0351, -0.1100],\n",
      "          [-0.0934,  0.0690,  0.0084,  0.0042,  0.0622],\n",
      "          [-0.0351, -0.0166,  0.0568,  0.0775,  0.0166],\n",
      "          [-0.0232, -0.0571, -0.0039,  0.0331,  0.0156]]],\n",
      "\n",
      "\n",
      "        [[[-0.0408,  0.0841,  0.1409,  0.1240, -0.1439],\n",
      "          [-0.0098, -0.0034, -0.0133,  0.0995, -0.0950],\n",
      "          [-0.0086, -0.0429, -0.0558,  0.0198,  0.0145],\n",
      "          [ 0.0193, -0.0196,  0.0215, -0.0321,  0.0149],\n",
      "          [-0.0075, -0.0139, -0.0045,  0.0224,  0.0278]]],\n",
      "\n",
      "\n",
      "        [[[-0.0331, -0.0251, -0.0782,  0.1143, -0.0900],\n",
      "          [-0.0238, -0.1043, -0.1269,  0.1968,  0.0054],\n",
      "          [-0.0322, -0.0250,  0.0313,  0.0337, -0.0018],\n",
      "          [-0.0266, -0.0809,  0.1049, -0.0029,  0.0550],\n",
      "          [ 0.0610, -0.0044, -0.0006,  0.0391,  0.0724]]],\n",
      "\n",
      "\n",
      "        [[[-0.0995, -0.0587, -0.0543,  0.0389,  0.1401],\n",
      "          [ 0.0119, -0.0014, -0.0137,  0.0785,  0.1017],\n",
      "          [-0.0040,  0.0093,  0.0010, -0.0623, -0.0400],\n",
      "          [ 0.0718, -0.0249, -0.0309, -0.0700, -0.0741],\n",
      "          [-0.0651,  0.0503, -0.0378, -0.0252, -0.0584]]],\n",
      "\n",
      "\n",
      "        [[[-0.0119,  0.0368,  0.0415, -0.0807,  0.0400],\n",
      "          [ 0.0674, -0.0843,  0.0810,  0.0012,  0.0343],\n",
      "          [-0.0656,  0.1767, -0.0307,  0.0451, -0.0151],\n",
      "          [-0.0945,  0.1043,  0.0151,  0.0438,  0.0158],\n",
      "          [ 0.0303,  0.0717, -0.0482, -0.0482, -0.0631]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0296, -0.0701, -0.0203,  0.0912, -0.0559],\n",
      "          [-0.0956, -0.0131, -0.0336, -0.0807,  0.0321],\n",
      "          [ 0.0214,  0.0245,  0.0084,  0.1667, -0.0024],\n",
      "          [-0.0027,  0.1174, -0.0747,  0.0367, -0.0033],\n",
      "          [ 0.0212, -0.1321, -0.0413,  0.0738, -0.0907]]],\n",
      "\n",
      "\n",
      "        [[[-0.1028,  0.0214,  0.0277, -0.0194, -0.0346],\n",
      "          [ 0.0138, -0.0468, -0.0418, -0.0673, -0.0941],\n",
      "          [ 0.0108,  0.0565, -0.1585,  0.0239,  0.0289],\n",
      "          [-0.0140,  0.0410,  0.0295, -0.0207, -0.0531],\n",
      "          [ 0.1635,  0.0375,  0.0304,  0.0713,  0.0295]]],\n",
      "\n",
      "\n",
      "        [[[-0.0762,  0.0207,  0.0541, -0.0613,  0.0144],\n",
      "          [ 0.0419,  0.0497,  0.0424,  0.0298,  0.0244],\n",
      "          [ 0.0576, -0.0049,  0.0185, -0.0593, -0.0332],\n",
      "          [ 0.0418, -0.0272, -0.0480, -0.0386,  0.0755],\n",
      "          [ 0.0837, -0.0219, -0.0045, -0.0849, -0.0661]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1153, -0.0516,  0.0467, -0.0278,  0.1342],\n",
      "          [ 0.0707, -0.1382,  0.0526, -0.0826, -0.0275],\n",
      "          [ 0.0742, -0.0122,  0.0300, -0.0829, -0.1176],\n",
      "          [ 0.0410,  0.0662,  0.0962, -0.0765,  0.1034],\n",
      "          [ 0.0470, -0.0127, -0.0015,  0.0621,  0.1452]]]],\n",
      "       requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : bias, param : Parameter containing:\n",
      "tensor([-0.0396, -0.0896,  0.0660, -0.0696,  0.0443,  0.0630, -0.1508,  0.1821,\n",
      "         0.0492, -0.0134,  0.0612,  0.1146,  0.1700,  0.0769, -0.0333,  0.1478,\n",
      "         0.0013, -0.0599, -0.1023,  0.0178], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, module in modules:        \n",
    "    if name == \"conv1\":\n",
    "        for name, param in module._parameters.items():\n",
    "            print(type(param))\n",
    "            print(\"name : {}, param : {}\".format(name, param))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : Parameter containing:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], requires_grad=True)\n",
      "\n",
      "inserted parameter in model : OrderedDict([('test', Parameter containing:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], requires_grad=True))])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = torch.nn.Parameter(data=torch.Tensor(data=[[1, 2, 3], [4, 5, 6]]), requires_grad=True)\n",
    "print(\"test : {}\".format(test))\n",
    "print()\n",
    "\n",
    "\n",
    "model.register_parameter(\"test\", test)\n",
    "print(\"inserted parameter in model : {}\".format(model._parameters))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`string`으로 구성된 name과 `Parameter` class로 구성된 parameter값이 `torch.nn.modules.Module class`의 필드 `_parateters`에 `OrderedDict()` 형태로 저장되어있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add_module Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_module(self, name, module):\n",
    "        r\"\"\"Adds a child module to the current module.\n",
    "\n",
    "        The module can be accessed as an attribute using the given name.\n",
    "\n",
    "        Args:\n",
    "            name (string): name of the child module. The child module can be\n",
    "                accessed from this module using the given name\n",
    "            parameter (Module): child module to be added to the module.\n",
    "        \"\"\"\n",
    "        if not isinstance(module, Module) and module is not None:\n",
    "            raise TypeError(\"{} is not a Module subclass\".format(\n",
    "                torch.typename(module)))\n",
    "        elif not isinstance(name, torch._six.string_classes):\n",
    "            raise TypeError(\"module name should be a string. Got {}\".format(\n",
    "                torch.typename(name)))\n",
    "        elif hasattr(self, name) and name not in self._modules:\n",
    "            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
    "        elif '.' in name:\n",
    "            raise KeyError(\"module name can't contain \\\".\\\"\")\n",
    "        elif name == '':\n",
    "            raise KeyError(\"module name can't be empty string \\\"\\\"\")\n",
    "        self._modules[name] = module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signature: torch.nn.modules.Module.add_module(self, name, module)  \n",
    "Docstring:  \n",
    "Adds a child module to the current module.  \n",
    "  \n",
    "The module can be accessed as an attribute using the given name.  \n",
    "    \n",
    "Args:  \n",
    "    `name` (string): name of the child module. The child module can be  \n",
    "        accessed from this module using the given name  \n",
    "    `parameter` (Module): child module to be added to the module.  \n",
    "        \n",
    "File:      /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py  \n",
    "Type:      function  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 모듈에 child 모듈을 추가한다.  \n",
    "\n",
    "해당 모듈은 name이라는 attribute로 접근될 수 있다.  \n",
    "\n",
    "`name`(string) : 이름\n",
    "`parameter` (Module) : 추가되는 child module이다.\n",
    "    \n",
    "함수 자체에     \n",
    "- `subclass` 가능한 모듈 확인\n",
    "- `name`이 string인제 확인\n",
    "- `name`이 중복되지 않는지 확인\n",
    "- `name`에 `.`이 들어가는지 확인\n",
    "- `name`이 공백문자`\"\"`인지 확인  \n",
    "    \n",
    "과 같은 assert를 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1', Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))), ('conv2', Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))), ('fc1', Linear(in_features=800, out_features=500, bias=True)), ('fc2', Linear(in_features=500, out_features=10, bias=True))])\n",
      "\n",
      "OrderedDict([('conv1', Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))), ('conv2', Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))), ('fc1', Linear(in_features=800, out_features=500, bias=True)), ('fc2', Linear(in_features=500, out_features=10, bias=True)), ('conv3', Conv2d(50, 50, kernel_size=(5, 5), stride=(1, 1)))])\n"
     ]
    }
   ],
   "source": [
    "model=Model()\n",
    "\n",
    "print(model._modules)\n",
    "print()\n",
    "model.add_module(\"conv3\", torch.nn.Conv2d(50, 50, kernel_size=(5, 5), stride=(1,1)))\n",
    "print(model._modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 인자는 내 모듈과 sub module에 대한 관계를 표현한다.  \n",
    "\n",
    "그 이후부터는 `name`:`Module` 형태의 key-value 형태로  \n",
    "`_modules`라는 필드에 저장된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(self, fn):\n",
    "        r\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
    "        as well as self. Typical use includes initializing the parameters of a model\n",
    "        (see also :ref:`torch-nn-init`).\n",
    "\n",
    "        Args:\n",
    "            fn (:class:`Module` -> None): function to be applied to each submodule\n",
    "\n",
    "        Returns:\n",
    "            Module: self\n",
    "\n",
    "        Example::\n",
    "\n",
    "            >>> def init_weights(m):\n",
    "                    print(m)\n",
    "                    if type(m) == nn.Linear:\n",
    "                        m.weight.data.fill_(1.0)\n",
    "                        print(m.weight)\n",
    "\n",
    "            >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
    "            >>> net.apply(init_weights)\n",
    "            Linear(in_features=2, out_features=2, bias=True)\n",
    "            Parameter containing:\n",
    "            tensor([[ 1.,  1.],\n",
    "                    [ 1.,  1.]])\n",
    "            Linear(in_features=2, out_features=2, bias=True)\n",
    "            Parameter containing:\n",
    "            tensor([[ 1.,  1.],\n",
    "                    [ 1.,  1.]])\n",
    "            Sequential(\n",
    "              (0): Linear(in_features=2, out_features=2, bias=True)\n",
    "              (1): Linear(in_features=2, out_features=2, bias=True)\n",
    "            )\n",
    "            Sequential(\n",
    "              (0): Linear(in_features=2, out_features=2, bias=True)\n",
    "              (1): Linear(in_features=2, out_features=2, bias=True)\n",
    "            )\n",
    "        \"\"\"\n",
    "        for module in self.children():\n",
    "            module.apply(fn)\n",
    "        fn(self)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signature: torch.nn.modules.Module.apply(self, fn)  \n",
    "Docstring:  \n",
    "Applies ``fn`` recursively to every submodule (as returned by ``.children()``)  \n",
    "as well as self. Typical use includes initializing the parameters of a model  \n",
    "(see also :ref:`torch-nn-init`).\n",
    "\n",
    "Args:\n",
    "    fn (:class:`Module` -> None): function to be applied to each submodule\n",
    "\n",
    "Returns:\n",
    "    Module: self\n",
    "\n",
    "Example::\n",
    "```\n",
    "    >>> def init_weights(m):\n",
    "            print(m)\n",
    "            if type(m) == nn.Linear:\n",
    "                m.weight.data.fill_(1.0)\n",
    "                print(m.weight)\n",
    "\n",
    "    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
    "    >>> net.apply(init_weights)\n",
    "    Linear(in_features=2, out_features=2, bias=True)\n",
    "    Parameter containing:\n",
    "    tensor([[ 1.,  1.],\n",
    "            [ 1.,  1.]])\n",
    "    Linear(in_features=2, out_features=2, bias=True)\n",
    "    Parameter containing:\n",
    "    tensor([[ 1.,  1.],\n",
    "            [ 1.,  1.]])\n",
    "    Sequential(\n",
    "      (0): Linear(in_features=2, out_features=2, bias=True)\n",
    "      (1): Linear(in_features=2, out_features=2, bias=True)\n",
    "    )\n",
    "    Sequential(\n",
    "      (0): Linear(in_features=2, out_features=2, bias=True)\n",
    "      (1): Linear(in_features=2, out_features=2, bias=True)\n",
    "    )\n",
    "```\n",
    "File:      /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py  \n",
    "Type:      function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Child 모듈까지 순회를 돌면서, 특정 함수를 적용하는 함수.  \n",
    "보통 parameter `initialization`을 할 때, 주로 사용하며  \n",
    "`torch-nn-init`에 적용되어있으니 참고하면 좋음\n",
    "\n",
    "`self.children()`이라는 함수 자체가 Generator로 동작하며, 자식 모듈의 `_modules`필드의 `module`만 추출해서  \n",
    "전달해주는 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict()\n",
      "\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : weight, param : Parameter containing:\n",
      "tensor([[[[ 0.0274, -0.0712, -0.1975,  0.1097, -0.1743],\n",
      "          [-0.1763,  0.0062,  0.0185, -0.0779,  0.0483],\n",
      "          [-0.1178,  0.0908,  0.1600, -0.0142,  0.1374],\n",
      "          [ 0.0447,  0.0619, -0.1012,  0.0240, -0.0933],\n",
      "          [ 0.0134,  0.0155,  0.0852,  0.0144,  0.1008]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1576,  0.0331,  0.1390,  0.0411,  0.0879],\n",
      "          [ 0.1758, -0.0723, -0.0274, -0.1336, -0.0195],\n",
      "          [ 0.0430,  0.1374, -0.1217, -0.1562,  0.0974],\n",
      "          [-0.0421,  0.0980,  0.0194,  0.0316,  0.1093],\n",
      "          [ 0.1806, -0.1007, -0.0586,  0.1650, -0.1538]]],\n",
      "\n",
      "\n",
      "        [[[-0.1982,  0.0379, -0.1875, -0.1482, -0.0263],\n",
      "          [ 0.0410,  0.1408,  0.1751, -0.1555, -0.1155],\n",
      "          [ 0.1006,  0.1464,  0.0467,  0.1421, -0.0444],\n",
      "          [ 0.1885, -0.0079,  0.0328, -0.1616, -0.1955],\n",
      "          [-0.1518,  0.0864,  0.1817, -0.0944,  0.0476]]],\n",
      "\n",
      "\n",
      "        [[[-0.1277,  0.0752,  0.1966, -0.1862, -0.1221],\n",
      "          [ 0.1757,  0.1069,  0.0627, -0.1832, -0.1510],\n",
      "          [-0.1465, -0.1639,  0.0291, -0.0666, -0.0413],\n",
      "          [-0.1776,  0.1629,  0.1136, -0.1882, -0.0665],\n",
      "          [ 0.1191,  0.0953,  0.1819,  0.1379,  0.0835]]],\n",
      "\n",
      "\n",
      "        [[[-0.1586, -0.1652,  0.0291, -0.1222,  0.0152],\n",
      "          [ 0.0967, -0.0465, -0.1982,  0.1480,  0.1797],\n",
      "          [-0.0463,  0.0700, -0.0356, -0.0287,  0.0605],\n",
      "          [-0.1971,  0.0135,  0.0908, -0.0130,  0.0184],\n",
      "          [-0.1299,  0.0766, -0.1365,  0.0017,  0.0800]]],\n",
      "\n",
      "\n",
      "        [[[-0.0811,  0.1639,  0.0926,  0.0068, -0.1390],\n",
      "          [ 0.1820, -0.1872,  0.1340, -0.1655, -0.0109],\n",
      "          [ 0.1698,  0.0714, -0.1331,  0.0653, -0.0893],\n",
      "          [-0.1196,  0.1563, -0.1837,  0.1744,  0.0264],\n",
      "          [-0.1016,  0.0329, -0.0424,  0.0503,  0.0234]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0984, -0.0748, -0.1155, -0.0533, -0.1844],\n",
      "          [ 0.1922, -0.1838,  0.0408,  0.1059, -0.0263],\n",
      "          [-0.0225,  0.0062,  0.1396,  0.1612,  0.0675],\n",
      "          [-0.1026, -0.1773, -0.0906,  0.1661,  0.0258],\n",
      "          [-0.1122,  0.1628, -0.0376,  0.0230, -0.0608]]],\n",
      "\n",
      "\n",
      "        [[[-0.1889,  0.1128,  0.1541,  0.1472, -0.0071],\n",
      "          [-0.0244,  0.0804,  0.1768, -0.0975, -0.0291],\n",
      "          [ 0.1440, -0.0826, -0.1902, -0.1938, -0.0294],\n",
      "          [ 0.0731, -0.0160, -0.0191, -0.0284,  0.0959],\n",
      "          [-0.0060, -0.0920, -0.0312,  0.0077,  0.1391]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1986,  0.0722, -0.0381,  0.0032, -0.0255],\n",
      "          [ 0.0939,  0.1103, -0.0503,  0.0371,  0.0322],\n",
      "          [-0.0016,  0.1414, -0.1962,  0.0969, -0.1141],\n",
      "          [-0.0079,  0.1595, -0.1240, -0.0194, -0.0579],\n",
      "          [-0.1329, -0.0874,  0.1253,  0.0998, -0.1577]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0407, -0.1646,  0.0866, -0.1002, -0.0576],\n",
      "          [ 0.1425, -0.0144,  0.0051, -0.0571,  0.0660],\n",
      "          [ 0.1819,  0.1401,  0.0083, -0.1092,  0.0171],\n",
      "          [ 0.1752, -0.1527, -0.0292, -0.0176, -0.0523],\n",
      "          [ 0.0667, -0.1029, -0.0034,  0.1941, -0.0948]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1066, -0.1587,  0.1464,  0.0191,  0.1595],\n",
      "          [ 0.1738,  0.1074,  0.0665,  0.1932,  0.1326],\n",
      "          [ 0.0547,  0.0270,  0.1638, -0.0733, -0.1154],\n",
      "          [ 0.0330, -0.0049, -0.0323,  0.1497, -0.1116],\n",
      "          [ 0.0521, -0.1733, -0.0895, -0.1528,  0.1009]]],\n",
      "\n",
      "\n",
      "        [[[-0.1201, -0.0388,  0.0799,  0.0153, -0.0486],\n",
      "          [ 0.0482,  0.1256,  0.1628, -0.1714,  0.1964],\n",
      "          [-0.1491, -0.0629, -0.1486,  0.1370, -0.0065],\n",
      "          [ 0.0149, -0.0739, -0.1851, -0.1071,  0.0395],\n",
      "          [ 0.1231, -0.1019,  0.0096, -0.1927,  0.0051]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1219, -0.1830,  0.1707, -0.1665,  0.1643],\n",
      "          [-0.1284, -0.0541, -0.1536, -0.0725,  0.0993],\n",
      "          [ 0.0444, -0.0843, -0.0417,  0.0098,  0.0411],\n",
      "          [ 0.1178, -0.1160, -0.1317, -0.1820, -0.0103],\n",
      "          [ 0.1352,  0.0512,  0.0832, -0.1921,  0.0638]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1535, -0.1546, -0.0368,  0.0251, -0.1503],\n",
      "          [-0.0474, -0.1890,  0.0570,  0.0108,  0.0118],\n",
      "          [ 0.1472,  0.0788,  0.0545, -0.0763, -0.1302],\n",
      "          [-0.0657, -0.0518, -0.0741, -0.0797,  0.1563],\n",
      "          [ 0.1538,  0.1702,  0.1189,  0.0132, -0.0282]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1968, -0.1775, -0.1936,  0.0834,  0.1277],\n",
      "          [-0.0789,  0.1688, -0.1696, -0.1540, -0.1345],\n",
      "          [-0.0578,  0.0622,  0.0475,  0.1575, -0.0933],\n",
      "          [-0.0356,  0.0179,  0.0255,  0.1588,  0.0393],\n",
      "          [-0.1860,  0.1312,  0.1766,  0.0901, -0.0866]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0130,  0.0435,  0.0631, -0.0539, -0.1765],\n",
      "          [-0.0010,  0.1815, -0.1903,  0.0207,  0.0485],\n",
      "          [ 0.1658,  0.0352,  0.1339,  0.1609, -0.0385],\n",
      "          [ 0.1267,  0.1500, -0.0979,  0.1472, -0.0675],\n",
      "          [ 0.0645,  0.0855,  0.1472, -0.1834,  0.1812]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1681,  0.1605,  0.0366, -0.0014,  0.1779],\n",
      "          [-0.1257,  0.0629,  0.1315, -0.0390, -0.1211],\n",
      "          [ 0.0399, -0.0738,  0.0674,  0.1465,  0.1954],\n",
      "          [ 0.1469,  0.1529, -0.1426, -0.0503, -0.0425],\n",
      "          [-0.0202, -0.0996,  0.1067, -0.0690,  0.0092]]],\n",
      "\n",
      "\n",
      "        [[[-0.1893,  0.0376, -0.0532, -0.1225,  0.1241],\n",
      "          [ 0.0004, -0.0085, -0.1095,  0.1956, -0.0246],\n",
      "          [-0.1424,  0.1842, -0.0263,  0.1156, -0.0544],\n",
      "          [-0.0760,  0.0088,  0.1167, -0.0454, -0.0591],\n",
      "          [-0.1396, -0.1464,  0.1346,  0.1082,  0.1597]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1115, -0.1692,  0.1531, -0.1057,  0.1641],\n",
      "          [ 0.0327,  0.0349, -0.0775,  0.0223, -0.0992],\n",
      "          [ 0.1014,  0.1220,  0.0109,  0.0703, -0.0140],\n",
      "          [-0.1598,  0.0503, -0.1113,  0.0966,  0.0620],\n",
      "          [ 0.1547,  0.0633,  0.1997,  0.0046, -0.1452]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1939, -0.0582, -0.1868, -0.1250,  0.1422],\n",
      "          [ 0.1220,  0.0645,  0.0183,  0.0498, -0.0470],\n",
      "          [ 0.1335,  0.1641,  0.0292,  0.1943,  0.1591],\n",
      "          [ 0.0315,  0.1755, -0.0688,  0.0879, -0.0203],\n",
      "          [ 0.0723,  0.0512,  0.1088,  0.0210, -0.1265]]]],\n",
      "       requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : bias, param : Parameter containing:\n",
      "tensor([ 0.0768,  0.1511, -0.1331, -0.0407,  0.0556,  0.1216, -0.0503, -0.1041,\n",
      "        -0.1968, -0.1800, -0.1625,  0.1626,  0.1165,  0.1550, -0.0134,  0.0447,\n",
      "        -0.0724, -0.0027, -0.1943, -0.1794], requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : weight, param : Parameter containing:\n",
      "tensor([[[[-0.0141,  0.0180,  0.0350, -0.0342,  0.0104],\n",
      "          [ 0.0356, -0.0217, -0.0416, -0.0395, -0.0420],\n",
      "          [ 0.0241,  0.0010, -0.0025,  0.0250,  0.0106],\n",
      "          [-0.0091,  0.0097,  0.0150, -0.0276,  0.0061],\n",
      "          [-0.0310, -0.0137, -0.0319, -0.0175,  0.0144]],\n",
      "\n",
      "         [[-0.0326,  0.0330,  0.0198,  0.0380,  0.0417],\n",
      "          [-0.0443,  0.0156,  0.0217,  0.0239, -0.0032],\n",
      "          [ 0.0326, -0.0036, -0.0151, -0.0364, -0.0366],\n",
      "          [ 0.0354, -0.0236, -0.0087, -0.0386,  0.0222],\n",
      "          [-0.0382, -0.0324, -0.0161, -0.0131, -0.0087]],\n",
      "\n",
      "         [[ 0.0388, -0.0406, -0.0156,  0.0033,  0.0322],\n",
      "          [ 0.0283, -0.0114, -0.0243,  0.0057, -0.0321],\n",
      "          [ 0.0273, -0.0205,  0.0194, -0.0063, -0.0060],\n",
      "          [-0.0414, -0.0235,  0.0145, -0.0263, -0.0398],\n",
      "          [-0.0252, -0.0279,  0.0194, -0.0170, -0.0350]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0413, -0.0148, -0.0413,  0.0291, -0.0363],\n",
      "          [ 0.0198, -0.0141,  0.0065, -0.0127,  0.0276],\n",
      "          [ 0.0081,  0.0166,  0.0149, -0.0100,  0.0359],\n",
      "          [-0.0070,  0.0250,  0.0155, -0.0367, -0.0230],\n",
      "          [-0.0198, -0.0359, -0.0288,  0.0142, -0.0094]],\n",
      "\n",
      "         [[-0.0062, -0.0207, -0.0120,  0.0445,  0.0034],\n",
      "          [-0.0028,  0.0250, -0.0152, -0.0080,  0.0400],\n",
      "          [-0.0138,  0.0287, -0.0012,  0.0241,  0.0211],\n",
      "          [-0.0445,  0.0083, -0.0227, -0.0316,  0.0073],\n",
      "          [ 0.0060, -0.0057, -0.0069,  0.0112,  0.0293]],\n",
      "\n",
      "         [[ 0.0170, -0.0291, -0.0049,  0.0005, -0.0361],\n",
      "          [-0.0443, -0.0176,  0.0156, -0.0185, -0.0345],\n",
      "          [ 0.0346,  0.0099,  0.0087, -0.0283,  0.0348],\n",
      "          [-0.0185, -0.0410,  0.0119,  0.0416,  0.0441],\n",
      "          [ 0.0165, -0.0074, -0.0377,  0.0111, -0.0443]]],\n",
      "\n",
      "\n",
      "        [[[-0.0372, -0.0172,  0.0025, -0.0051,  0.0089],\n",
      "          [ 0.0326,  0.0244, -0.0226, -0.0006,  0.0396],\n",
      "          [ 0.0326,  0.0249,  0.0389, -0.0150, -0.0159],\n",
      "          [ 0.0018,  0.0160, -0.0052,  0.0441, -0.0247],\n",
      "          [-0.0171, -0.0129, -0.0438, -0.0101, -0.0135]],\n",
      "\n",
      "         [[ 0.0200,  0.0020,  0.0245,  0.0347, -0.0409],\n",
      "          [-0.0081,  0.0125,  0.0215,  0.0221,  0.0283],\n",
      "          [-0.0346, -0.0053, -0.0333,  0.0423,  0.0399],\n",
      "          [-0.0291, -0.0409,  0.0028, -0.0096,  0.0093],\n",
      "          [ 0.0434,  0.0296,  0.0367,  0.0055, -0.0135]],\n",
      "\n",
      "         [[ 0.0154,  0.0220, -0.0154, -0.0182, -0.0131],\n",
      "          [ 0.0281, -0.0164,  0.0255, -0.0115, -0.0262],\n",
      "          [-0.0220, -0.0432, -0.0006, -0.0185, -0.0015],\n",
      "          [ 0.0193, -0.0204,  0.0355,  0.0002,  0.0235],\n",
      "          [ 0.0282, -0.0119,  0.0366, -0.0363, -0.0181]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0423,  0.0377,  0.0159,  0.0057,  0.0098],\n",
      "          [ 0.0220, -0.0270,  0.0378, -0.0189,  0.0281],\n",
      "          [ 0.0211, -0.0353, -0.0126,  0.0162, -0.0160],\n",
      "          [ 0.0381,  0.0212,  0.0011,  0.0178, -0.0228],\n",
      "          [ 0.0265,  0.0298,  0.0072, -0.0072, -0.0332]],\n",
      "\n",
      "         [[-0.0137,  0.0065,  0.0285, -0.0428,  0.0364],\n",
      "          [ 0.0295, -0.0079,  0.0357,  0.0365, -0.0053],\n",
      "          [ 0.0002,  0.0275, -0.0370,  0.0049,  0.0358],\n",
      "          [ 0.0115, -0.0376,  0.0371,  0.0163, -0.0017],\n",
      "          [ 0.0303,  0.0120, -0.0404, -0.0428, -0.0287]],\n",
      "\n",
      "         [[ 0.0407, -0.0397, -0.0363, -0.0102,  0.0227],\n",
      "          [ 0.0344,  0.0194,  0.0032, -0.0274,  0.0108],\n",
      "          [ 0.0021, -0.0109, -0.0195,  0.0326,  0.0010],\n",
      "          [-0.0432, -0.0116, -0.0314,  0.0056, -0.0379],\n",
      "          [ 0.0384,  0.0112,  0.0071,  0.0313, -0.0095]]],\n",
      "\n",
      "\n",
      "        [[[-0.0300,  0.0069,  0.0024, -0.0188,  0.0172],\n",
      "          [-0.0123,  0.0057,  0.0165,  0.0132, -0.0272],\n",
      "          [-0.0151, -0.0356,  0.0316, -0.0055, -0.0438],\n",
      "          [-0.0306,  0.0059,  0.0062, -0.0083,  0.0366],\n",
      "          [-0.0182, -0.0222, -0.0210,  0.0284,  0.0125]],\n",
      "\n",
      "         [[-0.0106,  0.0165,  0.0004, -0.0027, -0.0245],\n",
      "          [ 0.0367, -0.0363, -0.0055,  0.0434, -0.0009],\n",
      "          [-0.0205,  0.0245,  0.0005, -0.0275,  0.0110],\n",
      "          [-0.0427,  0.0022, -0.0435, -0.0315, -0.0296],\n",
      "          [-0.0248, -0.0070,  0.0007, -0.0180, -0.0396]],\n",
      "\n",
      "         [[-0.0208,  0.0085, -0.0213, -0.0081,  0.0376],\n",
      "          [-0.0374,  0.0082,  0.0089, -0.0418,  0.0326],\n",
      "          [-0.0132, -0.0060, -0.0331,  0.0336, -0.0253],\n",
      "          [ 0.0005,  0.0213, -0.0300,  0.0063, -0.0265],\n",
      "          [ 0.0365, -0.0282,  0.0009,  0.0126, -0.0403]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0165, -0.0418, -0.0103, -0.0138, -0.0232],\n",
      "          [ 0.0216, -0.0398, -0.0237, -0.0268, -0.0172],\n",
      "          [ 0.0139, -0.0125, -0.0236,  0.0128, -0.0374],\n",
      "          [-0.0136,  0.0410, -0.0370,  0.0316,  0.0253],\n",
      "          [-0.0360,  0.0374,  0.0149,  0.0151,  0.0345]],\n",
      "\n",
      "         [[-0.0220,  0.0345, -0.0056, -0.0408,  0.0060],\n",
      "          [-0.0004, -0.0372,  0.0055,  0.0420,  0.0132],\n",
      "          [-0.0089,  0.0136,  0.0362, -0.0040,  0.0027],\n",
      "          [ 0.0120, -0.0258, -0.0185, -0.0187, -0.0028],\n",
      "          [-0.0368,  0.0211,  0.0207,  0.0373,  0.0368]],\n",
      "\n",
      "         [[ 0.0445,  0.0074,  0.0062,  0.0408,  0.0121],\n",
      "          [-0.0198, -0.0180, -0.0026, -0.0122, -0.0260],\n",
      "          [ 0.0274, -0.0036,  0.0251,  0.0337,  0.0213],\n",
      "          [ 0.0334,  0.0036, -0.0328,  0.0383, -0.0268],\n",
      "          [-0.0100,  0.0068,  0.0090,  0.0227,  0.0040]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0427, -0.0063, -0.0114,  0.0139, -0.0151],\n",
      "          [ 0.0326,  0.0112,  0.0163, -0.0408, -0.0341],\n",
      "          [ 0.0192, -0.0381, -0.0075, -0.0157, -0.0137],\n",
      "          [-0.0155, -0.0276, -0.0197, -0.0005,  0.0038],\n",
      "          [ 0.0105, -0.0058,  0.0034,  0.0031,  0.0310]],\n",
      "\n",
      "         [[ 0.0050, -0.0377, -0.0098,  0.0163,  0.0296],\n",
      "          [-0.0053, -0.0406, -0.0231,  0.0329, -0.0390],\n",
      "          [-0.0185, -0.0275, -0.0008,  0.0342, -0.0102],\n",
      "          [-0.0039,  0.0367,  0.0235, -0.0246, -0.0028],\n",
      "          [-0.0303,  0.0243, -0.0269, -0.0323, -0.0272]],\n",
      "\n",
      "         [[-0.0144,  0.0363,  0.0071, -0.0150, -0.0125],\n",
      "          [ 0.0108,  0.0066,  0.0392,  0.0182,  0.0351],\n",
      "          [-0.0230,  0.0400,  0.0402,  0.0424,  0.0170],\n",
      "          [-0.0316,  0.0135, -0.0115,  0.0296, -0.0110],\n",
      "          [ 0.0188, -0.0393, -0.0284, -0.0382, -0.0184]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0089, -0.0331,  0.0327,  0.0132, -0.0080],\n",
      "          [-0.0123, -0.0246, -0.0433, -0.0207, -0.0191],\n",
      "          [ 0.0328,  0.0281, -0.0080,  0.0111, -0.0177],\n",
      "          [ 0.0115,  0.0359,  0.0009,  0.0232,  0.0189],\n",
      "          [ 0.0008,  0.0115,  0.0337,  0.0049, -0.0441]],\n",
      "\n",
      "         [[ 0.0327,  0.0411,  0.0264, -0.0286, -0.0394],\n",
      "          [ 0.0223, -0.0422, -0.0297, -0.0152, -0.0089],\n",
      "          [ 0.0205, -0.0291,  0.0278, -0.0261, -0.0163],\n",
      "          [-0.0220,  0.0111,  0.0155,  0.0275,  0.0273],\n",
      "          [ 0.0111, -0.0014, -0.0007, -0.0051, -0.0126]],\n",
      "\n",
      "         [[-0.0075, -0.0393,  0.0253,  0.0211,  0.0005],\n",
      "          [ 0.0387,  0.0415, -0.0342, -0.0195, -0.0182],\n",
      "          [-0.0153,  0.0237,  0.0146, -0.0105, -0.0179],\n",
      "          [-0.0095,  0.0268, -0.0177, -0.0263,  0.0265],\n",
      "          [ 0.0095,  0.0228,  0.0279,  0.0191,  0.0124]]],\n",
      "\n",
      "\n",
      "        [[[-0.0233, -0.0335, -0.0234, -0.0071, -0.0336],\n",
      "          [ 0.0187, -0.0354, -0.0204,  0.0230, -0.0312],\n",
      "          [ 0.0309, -0.0190,  0.0104, -0.0145, -0.0219],\n",
      "          [-0.0068,  0.0222,  0.0220,  0.0337, -0.0184],\n",
      "          [ 0.0047, -0.0447,  0.0172,  0.0354, -0.0066]],\n",
      "\n",
      "         [[-0.0193,  0.0135,  0.0034, -0.0250, -0.0245],\n",
      "          [ 0.0436,  0.0282,  0.0375, -0.0332,  0.0394],\n",
      "          [ 0.0060,  0.0411, -0.0403, -0.0374,  0.0170],\n",
      "          [ 0.0227, -0.0297, -0.0082, -0.0094,  0.0232],\n",
      "          [ 0.0025,  0.0080, -0.0037, -0.0229,  0.0274]],\n",
      "\n",
      "         [[ 0.0310,  0.0312, -0.0134, -0.0213, -0.0425],\n",
      "          [ 0.0407, -0.0040, -0.0081, -0.0444,  0.0356],\n",
      "          [-0.0145, -0.0175, -0.0129,  0.0099, -0.0109],\n",
      "          [-0.0264,  0.0368, -0.0233, -0.0381, -0.0391],\n",
      "          [-0.0277, -0.0296,  0.0198, -0.0228, -0.0022]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0173, -0.0201,  0.0296,  0.0270,  0.0162],\n",
      "          [-0.0063,  0.0185, -0.0178,  0.0170,  0.0272],\n",
      "          [ 0.0101, -0.0178, -0.0015, -0.0131, -0.0023],\n",
      "          [ 0.0263, -0.0018, -0.0046, -0.0089, -0.0017],\n",
      "          [-0.0146,  0.0225, -0.0420,  0.0387, -0.0197]],\n",
      "\n",
      "         [[ 0.0424,  0.0301, -0.0155, -0.0267, -0.0420],\n",
      "          [ 0.0280, -0.0312,  0.0234,  0.0279, -0.0318],\n",
      "          [-0.0277, -0.0246,  0.0417, -0.0087, -0.0092],\n",
      "          [ 0.0416,  0.0316,  0.0235, -0.0324, -0.0085],\n",
      "          [-0.0013,  0.0380, -0.0275,  0.0419, -0.0214]],\n",
      "\n",
      "         [[-0.0035, -0.0409,  0.0398,  0.0272,  0.0124],\n",
      "          [-0.0202, -0.0059, -0.0121, -0.0053, -0.0381],\n",
      "          [-0.0270,  0.0104, -0.0041,  0.0155, -0.0139],\n",
      "          [ 0.0433,  0.0228,  0.0225, -0.0280,  0.0265],\n",
      "          [ 0.0378,  0.0183,  0.0118,  0.0223, -0.0301]]],\n",
      "\n",
      "\n",
      "        [[[-0.0362, -0.0418, -0.0163,  0.0268, -0.0094],\n",
      "          [ 0.0247,  0.0036,  0.0172, -0.0026, -0.0389],\n",
      "          [ 0.0120,  0.0358, -0.0344,  0.0106,  0.0257],\n",
      "          [-0.0427, -0.0346, -0.0287, -0.0315, -0.0058],\n",
      "          [-0.0294,  0.0169, -0.0004, -0.0276,  0.0044]],\n",
      "\n",
      "         [[ 0.0416,  0.0322,  0.0389, -0.0063, -0.0187],\n",
      "          [ 0.0435,  0.0060, -0.0441,  0.0218, -0.0097],\n",
      "          [ 0.0102,  0.0392, -0.0032, -0.0255, -0.0137],\n",
      "          [-0.0053, -0.0371, -0.0322,  0.0220,  0.0060],\n",
      "          [ 0.0016,  0.0303, -0.0400,  0.0121,  0.0090]],\n",
      "\n",
      "         [[-0.0362, -0.0199,  0.0349,  0.0121,  0.0175],\n",
      "          [-0.0412,  0.0114,  0.0421, -0.0136, -0.0099],\n",
      "          [-0.0420,  0.0147,  0.0328,  0.0408,  0.0346],\n",
      "          [ 0.0258,  0.0034,  0.0244, -0.0293, -0.0350],\n",
      "          [ 0.0243,  0.0334, -0.0181,  0.0429, -0.0358]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0359,  0.0215, -0.0079,  0.0362,  0.0193],\n",
      "          [-0.0142,  0.0445, -0.0169, -0.0111, -0.0374],\n",
      "          [ 0.0395,  0.0170, -0.0151,  0.0283,  0.0429],\n",
      "          [ 0.0249, -0.0134, -0.0029, -0.0257, -0.0147],\n",
      "          [ 0.0302, -0.0434,  0.0112, -0.0137, -0.0021]],\n",
      "\n",
      "         [[-0.0144,  0.0395, -0.0404,  0.0338, -0.0009],\n",
      "          [-0.0196,  0.0096,  0.0259,  0.0179,  0.0426],\n",
      "          [ 0.0030, -0.0322, -0.0280, -0.0256,  0.0313],\n",
      "          [ 0.0326,  0.0169, -0.0365,  0.0445,  0.0234],\n",
      "          [-0.0245,  0.0354, -0.0446,  0.0394,  0.0097]],\n",
      "\n",
      "         [[ 0.0123, -0.0101,  0.0147, -0.0070, -0.0269],\n",
      "          [-0.0310, -0.0297, -0.0007,  0.0385,  0.0095],\n",
      "          [ 0.0158,  0.0229,  0.0312,  0.0220, -0.0329],\n",
      "          [-0.0137, -0.0397,  0.0200, -0.0308,  0.0406],\n",
      "          [-0.0338, -0.0420,  0.0282,  0.0058,  0.0193]]]],\n",
      "       requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : bias, param : Parameter containing:\n",
      "tensor([-0.0260,  0.0049,  0.0053,  0.0355,  0.0140, -0.0141,  0.0321,  0.0253,\n",
      "        -0.0033,  0.0090, -0.0039,  0.0071,  0.0331,  0.0429, -0.0196, -0.0373,\n",
      "        -0.0197,  0.0076,  0.0013,  0.0129, -0.0272,  0.0236,  0.0037,  0.0207,\n",
      "        -0.0370, -0.0210,  0.0419, -0.0184,  0.0193,  0.0147,  0.0048, -0.0011,\n",
      "         0.0008,  0.0107,  0.0295, -0.0370, -0.0140, -0.0181, -0.0233,  0.0155,\n",
      "         0.0196,  0.0381, -0.0251,  0.0426, -0.0310, -0.0126, -0.0235, -0.0362,\n",
      "         0.0385,  0.0252], requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : weight, param : Parameter containing:\n",
      "tensor([[-0.0321,  0.0129, -0.0132,  ...,  0.0063,  0.0343,  0.0279],\n",
      "        [ 0.0292,  0.0057, -0.0115,  ..., -0.0086, -0.0162,  0.0242],\n",
      "        [-0.0197,  0.0035, -0.0039,  ..., -0.0290,  0.0057, -0.0245],\n",
      "        ...,\n",
      "        [-0.0108,  0.0262,  0.0227,  ..., -0.0314, -0.0339, -0.0163],\n",
      "        [ 0.0309, -0.0142, -0.0339,  ..., -0.0260,  0.0160,  0.0328],\n",
      "        [ 0.0083, -0.0331, -0.0319,  ...,  0.0217,  0.0158, -0.0195]],\n",
      "       requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : bias, param : Parameter containing:\n",
      "tensor([ 0.0271, -0.0045,  0.0094, -0.0015,  0.0086,  0.0133,  0.0122,  0.0326,\n",
      "        -0.0107,  0.0061,  0.0140, -0.0061,  0.0243, -0.0066,  0.0262,  0.0225,\n",
      "         0.0331, -0.0353, -0.0312,  0.0002, -0.0161,  0.0039,  0.0222,  0.0022,\n",
      "         0.0022,  0.0118, -0.0165,  0.0234,  0.0146,  0.0207, -0.0040, -0.0249,\n",
      "        -0.0190,  0.0258, -0.0111, -0.0297, -0.0074,  0.0276, -0.0198,  0.0047,\n",
      "        -0.0261, -0.0127, -0.0021, -0.0094,  0.0067, -0.0153, -0.0075,  0.0025,\n",
      "        -0.0184,  0.0159,  0.0237, -0.0198, -0.0134,  0.0005,  0.0064,  0.0067,\n",
      "         0.0175, -0.0118, -0.0103,  0.0031,  0.0168, -0.0005,  0.0057,  0.0254,\n",
      "         0.0168, -0.0037, -0.0057, -0.0127,  0.0203,  0.0091, -0.0193, -0.0342,\n",
      "        -0.0122, -0.0085, -0.0279,  0.0053,  0.0321,  0.0335, -0.0050,  0.0153,\n",
      "         0.0080, -0.0197,  0.0041, -0.0094, -0.0010,  0.0135, -0.0090,  0.0028,\n",
      "        -0.0049,  0.0331, -0.0046, -0.0190, -0.0222, -0.0087, -0.0155,  0.0282,\n",
      "         0.0014,  0.0145,  0.0102,  0.0079,  0.0067,  0.0099,  0.0016, -0.0045,\n",
      "        -0.0332, -0.0263, -0.0116,  0.0253, -0.0021, -0.0307, -0.0065, -0.0268,\n",
      "        -0.0102, -0.0090,  0.0304,  0.0238,  0.0060, -0.0168,  0.0198,  0.0189,\n",
      "        -0.0021, -0.0129, -0.0193, -0.0275,  0.0045, -0.0262, -0.0048,  0.0215,\n",
      "         0.0154, -0.0003, -0.0091,  0.0117, -0.0030, -0.0295, -0.0297,  0.0109,\n",
      "         0.0108, -0.0329, -0.0302, -0.0092,  0.0043, -0.0056,  0.0224, -0.0238,\n",
      "        -0.0331, -0.0271, -0.0093,  0.0104,  0.0310,  0.0036, -0.0329,  0.0352,\n",
      "        -0.0179, -0.0138,  0.0237,  0.0225, -0.0173, -0.0139, -0.0154, -0.0322,\n",
      "        -0.0185, -0.0115, -0.0215,  0.0225, -0.0253,  0.0327,  0.0142,  0.0256,\n",
      "        -0.0312, -0.0246,  0.0179, -0.0057,  0.0348,  0.0190,  0.0239,  0.0276,\n",
      "        -0.0320, -0.0116, -0.0109, -0.0089, -0.0046, -0.0313,  0.0324, -0.0185,\n",
      "         0.0102,  0.0109, -0.0326,  0.0339, -0.0077, -0.0237, -0.0071, -0.0341,\n",
      "        -0.0149,  0.0284, -0.0102,  0.0326,  0.0144, -0.0289,  0.0041,  0.0014,\n",
      "        -0.0221, -0.0076, -0.0007,  0.0160,  0.0055,  0.0104, -0.0264, -0.0289,\n",
      "         0.0287, -0.0087,  0.0168,  0.0204,  0.0259,  0.0192,  0.0229, -0.0345,\n",
      "         0.0032,  0.0313,  0.0140, -0.0182, -0.0313, -0.0247,  0.0211, -0.0301,\n",
      "        -0.0157, -0.0261, -0.0048,  0.0190,  0.0002, -0.0079,  0.0202, -0.0235,\n",
      "         0.0126,  0.0019,  0.0072,  0.0284,  0.0122,  0.0026, -0.0346,  0.0034,\n",
      "        -0.0064,  0.0294, -0.0218, -0.0291,  0.0161,  0.0006,  0.0026,  0.0080,\n",
      "        -0.0072,  0.0148,  0.0256, -0.0031,  0.0022, -0.0150, -0.0240, -0.0103,\n",
      "        -0.0192, -0.0124,  0.0168,  0.0103, -0.0159,  0.0332, -0.0089, -0.0012,\n",
      "        -0.0345, -0.0071,  0.0195, -0.0205,  0.0120, -0.0210,  0.0013,  0.0038,\n",
      "        -0.0115,  0.0152, -0.0266, -0.0041, -0.0117,  0.0321, -0.0114, -0.0300,\n",
      "         0.0105, -0.0254, -0.0353,  0.0134, -0.0061, -0.0186,  0.0001,  0.0111,\n",
      "         0.0317, -0.0155,  0.0073, -0.0100,  0.0285,  0.0076,  0.0042,  0.0220,\n",
      "        -0.0023, -0.0082, -0.0124, -0.0197,  0.0074,  0.0117,  0.0052, -0.0003,\n",
      "         0.0158,  0.0077,  0.0227, -0.0143,  0.0274,  0.0194, -0.0241, -0.0144,\n",
      "         0.0166,  0.0213, -0.0353,  0.0063, -0.0091, -0.0049,  0.0292, -0.0173,\n",
      "        -0.0212, -0.0183,  0.0228, -0.0185,  0.0089,  0.0274,  0.0062, -0.0304,\n",
      "         0.0283, -0.0105, -0.0107, -0.0212,  0.0072, -0.0214,  0.0226, -0.0325,\n",
      "         0.0064, -0.0209, -0.0053,  0.0256,  0.0006,  0.0060, -0.0348,  0.0195,\n",
      "         0.0250,  0.0252, -0.0210, -0.0179, -0.0173, -0.0066,  0.0081, -0.0273,\n",
      "        -0.0012, -0.0066,  0.0063,  0.0281,  0.0291,  0.0273, -0.0063,  0.0020,\n",
      "        -0.0037, -0.0067, -0.0333,  0.0143, -0.0036,  0.0146,  0.0343,  0.0093,\n",
      "        -0.0164, -0.0301,  0.0019, -0.0307,  0.0109,  0.0261,  0.0062, -0.0020,\n",
      "         0.0147, -0.0087,  0.0281,  0.0205,  0.0320,  0.0048, -0.0268, -0.0222,\n",
      "        -0.0019, -0.0345,  0.0302, -0.0284, -0.0011, -0.0306,  0.0290, -0.0228,\n",
      "        -0.0341, -0.0338,  0.0219, -0.0352,  0.0162,  0.0134, -0.0110,  0.0011,\n",
      "        -0.0152, -0.0225, -0.0172, -0.0301,  0.0122, -0.0342,  0.0340,  0.0058,\n",
      "         0.0079,  0.0325, -0.0310,  0.0300, -0.0318,  0.0072, -0.0101, -0.0298,\n",
      "         0.0171,  0.0319,  0.0078,  0.0341,  0.0161, -0.0336,  0.0185,  0.0353,\n",
      "        -0.0177,  0.0249, -0.0333, -0.0247, -0.0168, -0.0332, -0.0154,  0.0021,\n",
      "         0.0189,  0.0027,  0.0332, -0.0005,  0.0302,  0.0230, -0.0191, -0.0078,\n",
      "         0.0016, -0.0008, -0.0297,  0.0288,  0.0205, -0.0195, -0.0002, -0.0162,\n",
      "         0.0017, -0.0284,  0.0130,  0.0192, -0.0323,  0.0139, -0.0030,  0.0341,\n",
      "         0.0108, -0.0064, -0.0181, -0.0257,  0.0194, -0.0237, -0.0227, -0.0140,\n",
      "        -0.0177, -0.0260,  0.0138,  0.0329,  0.0209,  0.0177, -0.0353, -0.0147,\n",
      "         0.0233,  0.0019, -0.0321,  0.0317, -0.0223, -0.0007, -0.0040,  0.0081,\n",
      "         0.0338, -0.0032, -0.0172,  0.0148, -0.0099, -0.0110, -0.0164,  0.0213,\n",
      "        -0.0301, -0.0271, -0.0088,  0.0218, -0.0272, -0.0169,  0.0063, -0.0094,\n",
      "        -0.0143, -0.0260, -0.0003,  0.0004], requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : weight, param : Parameter containing:\n",
      "tensor([[-0.0217,  0.0383,  0.0437,  ...,  0.0269, -0.0168,  0.0100],\n",
      "        [-0.0346, -0.0074,  0.0227,  ...,  0.0209, -0.0186,  0.0399],\n",
      "        [-0.0032,  0.0258,  0.0238,  ..., -0.0113, -0.0056,  0.0002],\n",
      "        ...,\n",
      "        [ 0.0210, -0.0255,  0.0149,  ...,  0.0071,  0.0346,  0.0175],\n",
      "        [-0.0322, -0.0380,  0.0316,  ...,  0.0322,  0.0358,  0.0420],\n",
      "        [ 0.0296, -0.0291, -0.0409,  ...,  0.0279,  0.0104,  0.0008]],\n",
      "       requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : bias, param : Parameter containing:\n",
      "tensor([-0.0076,  0.0212,  0.0009,  0.0436,  0.0015, -0.0206, -0.0361, -0.0119,\n",
      "         0.0302, -0.0198], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model=Model()\n",
    "modules = model.named_modules()\n",
    "\n",
    "print(model._parameters)\n",
    "print()\n",
    "\n",
    "for name, module in modules:            \n",
    "    for name, param in module._parameters.items():\n",
    "        print(type(param))\n",
    "        print(\"name : {}, param : {}\".format(name, param))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted parameter in model : OrderedDict([('weight', Parameter containing:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], requires_grad=True))])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = torch.nn.Parameter(data=torch.Tensor(data=[[1, 2, 3], [4, 5, 6]]), requires_grad=True)\n",
    "model.register_parameter(\"weight\", test)\n",
    "print(\"inserted parameter in model : {}\".format(model._parameters))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_wegiths(m):\n",
    "    print(m)\n",
    "    m.weight.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "Linear(in_features=800, out_features=500, bias=True)\n",
      "Linear(in_features=500, out_features=10, bias=True)\n",
      "Model(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n",
      "OrderedDict([('weight', Parameter containing:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]], requires_grad=True))])\n",
      "\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : weight, param : Parameter containing:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]], requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : weight, param : Parameter containing:\n",
      "tensor([[[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]]], requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : bias, param : Parameter containing:\n",
      "tensor([-0.0818, -0.0632,  0.1379, -0.0658, -0.0417,  0.1713,  0.0182,  0.1507,\n",
      "        -0.1484, -0.1418,  0.0199,  0.1642,  0.0938,  0.1311,  0.0365, -0.1290,\n",
      "        -0.1858, -0.0129,  0.1231, -0.1568], requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : weight, param : Parameter containing:\n",
      "tensor([[[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]]], requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : bias, param : Parameter containing:\n",
      "tensor([-0.0058, -0.0356, -0.0040,  0.0167,  0.0215, -0.0244,  0.0219, -0.0272,\n",
      "        -0.0030, -0.0167, -0.0047, -0.0447, -0.0174, -0.0297,  0.0318,  0.0270,\n",
      "         0.0347, -0.0368, -0.0314, -0.0208,  0.0423,  0.0132, -0.0197,  0.0313,\n",
      "         0.0372,  0.0277, -0.0323,  0.0357,  0.0296, -0.0187, -0.0338, -0.0171,\n",
      "        -0.0442,  0.0369,  0.0263, -0.0387,  0.0182,  0.0033, -0.0288, -0.0304,\n",
      "         0.0206,  0.0179,  0.0384, -0.0197, -0.0005, -0.0025,  0.0147,  0.0213,\n",
      "         0.0141, -0.0082], requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : weight, param : Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : bias, param : Parameter containing:\n",
      "tensor([ 0.0325, -0.0179, -0.0050, -0.0264, -0.0269,  0.0095,  0.0312,  0.0207,\n",
      "        -0.0125, -0.0240,  0.0004,  0.0011,  0.0182,  0.0348, -0.0044, -0.0106,\n",
      "         0.0329, -0.0048,  0.0274, -0.0316,  0.0038,  0.0275,  0.0141, -0.0079,\n",
      "         0.0241,  0.0113, -0.0349,  0.0055,  0.0026,  0.0342, -0.0154,  0.0140,\n",
      "        -0.0299,  0.0038, -0.0085,  0.0195, -0.0215, -0.0293,  0.0268,  0.0333,\n",
      "         0.0082, -0.0336,  0.0301,  0.0265,  0.0116, -0.0157, -0.0167,  0.0072,\n",
      "        -0.0246, -0.0092, -0.0144, -0.0131,  0.0248,  0.0069,  0.0168, -0.0332,\n",
      "        -0.0178,  0.0164, -0.0016,  0.0055, -0.0160,  0.0077, -0.0045,  0.0084,\n",
      "         0.0062,  0.0063,  0.0325,  0.0197, -0.0238, -0.0104, -0.0099,  0.0152,\n",
      "        -0.0297,  0.0008, -0.0016, -0.0273,  0.0068, -0.0214, -0.0154,  0.0011,\n",
      "        -0.0060,  0.0235, -0.0267,  0.0197,  0.0198, -0.0091,  0.0334, -0.0294,\n",
      "        -0.0018, -0.0177,  0.0112, -0.0026,  0.0283,  0.0287, -0.0329, -0.0231,\n",
      "         0.0035,  0.0313, -0.0321, -0.0349, -0.0025, -0.0129,  0.0192, -0.0078,\n",
      "        -0.0014, -0.0125,  0.0042,  0.0225, -0.0149, -0.0311, -0.0075, -0.0343,\n",
      "         0.0007,  0.0292,  0.0180, -0.0134, -0.0343, -0.0111,  0.0143, -0.0101,\n",
      "        -0.0125,  0.0039, -0.0192,  0.0019, -0.0332, -0.0223,  0.0153, -0.0268,\n",
      "        -0.0190, -0.0260,  0.0172, -0.0270, -0.0275,  0.0084,  0.0230,  0.0325,\n",
      "        -0.0215,  0.0335, -0.0053,  0.0142,  0.0151,  0.0036,  0.0171, -0.0259,\n",
      "        -0.0270, -0.0284, -0.0048,  0.0161,  0.0044,  0.0172,  0.0008,  0.0168,\n",
      "         0.0335, -0.0137,  0.0172,  0.0013,  0.0332, -0.0254, -0.0166,  0.0048,\n",
      "         0.0337, -0.0184,  0.0061, -0.0204, -0.0279, -0.0106,  0.0219, -0.0049,\n",
      "         0.0350,  0.0080,  0.0282, -0.0261, -0.0335, -0.0337,  0.0288, -0.0300,\n",
      "        -0.0223, -0.0207, -0.0337,  0.0341,  0.0254, -0.0254, -0.0094, -0.0199,\n",
      "         0.0309,  0.0166, -0.0300, -0.0154, -0.0103, -0.0336,  0.0109, -0.0216,\n",
      "        -0.0119, -0.0217,  0.0263, -0.0027,  0.0101, -0.0124, -0.0169, -0.0058,\n",
      "        -0.0300, -0.0334, -0.0295, -0.0239, -0.0103,  0.0069,  0.0133,  0.0101,\n",
      "         0.0048,  0.0316, -0.0071, -0.0091, -0.0076, -0.0282, -0.0168,  0.0053,\n",
      "         0.0201,  0.0288, -0.0163, -0.0077,  0.0053,  0.0080,  0.0213,  0.0181,\n",
      "        -0.0190,  0.0058, -0.0288,  0.0343,  0.0044,  0.0088, -0.0161,  0.0278,\n",
      "        -0.0269, -0.0113,  0.0207,  0.0347,  0.0176, -0.0081, -0.0316, -0.0164,\n",
      "         0.0140, -0.0122,  0.0195, -0.0208, -0.0220, -0.0291, -0.0090, -0.0162,\n",
      "         0.0011, -0.0225, -0.0288, -0.0156, -0.0352, -0.0152, -0.0141,  0.0183,\n",
      "        -0.0271, -0.0164,  0.0106,  0.0252,  0.0317,  0.0156, -0.0279,  0.0334,\n",
      "         0.0218, -0.0314, -0.0315, -0.0247, -0.0265, -0.0110,  0.0023,  0.0187,\n",
      "        -0.0002, -0.0086, -0.0317,  0.0015, -0.0049, -0.0122,  0.0132, -0.0145,\n",
      "         0.0013,  0.0104, -0.0107,  0.0025,  0.0057,  0.0012, -0.0008, -0.0039,\n",
      "         0.0281,  0.0325,  0.0266, -0.0189, -0.0262, -0.0346,  0.0293, -0.0139,\n",
      "         0.0233,  0.0074,  0.0066,  0.0050,  0.0290, -0.0007,  0.0197, -0.0060,\n",
      "        -0.0022, -0.0084,  0.0039, -0.0197, -0.0042, -0.0255,  0.0136,  0.0097,\n",
      "         0.0074, -0.0338,  0.0285,  0.0120,  0.0107, -0.0259,  0.0174, -0.0230,\n",
      "         0.0167, -0.0015, -0.0258,  0.0284, -0.0257,  0.0183, -0.0251,  0.0090,\n",
      "        -0.0223, -0.0066, -0.0152, -0.0261, -0.0320, -0.0013,  0.0306, -0.0230,\n",
      "        -0.0345, -0.0319, -0.0049, -0.0348,  0.0103,  0.0193,  0.0352,  0.0238,\n",
      "        -0.0077,  0.0263,  0.0196, -0.0130, -0.0236, -0.0075,  0.0102, -0.0208,\n",
      "         0.0081,  0.0274,  0.0021,  0.0223, -0.0091, -0.0005, -0.0190, -0.0312,\n",
      "        -0.0032,  0.0303, -0.0234, -0.0134,  0.0189, -0.0106, -0.0165, -0.0256,\n",
      "        -0.0328,  0.0026, -0.0023, -0.0043, -0.0183, -0.0179,  0.0139,  0.0188,\n",
      "        -0.0090, -0.0314,  0.0316,  0.0204, -0.0327,  0.0060, -0.0025,  0.0208,\n",
      "        -0.0303, -0.0300,  0.0032,  0.0208,  0.0293,  0.0192,  0.0070, -0.0280,\n",
      "        -0.0282,  0.0352,  0.0228,  0.0026,  0.0077, -0.0038, -0.0185, -0.0154,\n",
      "         0.0279, -0.0210, -0.0258, -0.0134, -0.0199, -0.0109, -0.0169, -0.0212,\n",
      "         0.0163, -0.0252,  0.0335, -0.0132, -0.0062,  0.0324,  0.0246, -0.0098,\n",
      "         0.0234,  0.0352, -0.0051,  0.0116,  0.0274, -0.0252, -0.0104,  0.0329,\n",
      "        -0.0189, -0.0188, -0.0260, -0.0321,  0.0322,  0.0050,  0.0058, -0.0240,\n",
      "         0.0156, -0.0176,  0.0225, -0.0053, -0.0350, -0.0202,  0.0214,  0.0064,\n",
      "        -0.0029, -0.0147, -0.0012, -0.0080, -0.0055,  0.0197,  0.0281, -0.0234,\n",
      "         0.0299, -0.0277,  0.0309, -0.0190,  0.0224, -0.0087,  0.0055,  0.0064,\n",
      "        -0.0085, -0.0332,  0.0252,  0.0107,  0.0254,  0.0173,  0.0063,  0.0242,\n",
      "         0.0194, -0.0306,  0.0080, -0.0305,  0.0289, -0.0257, -0.0286, -0.0306,\n",
      "         0.0253, -0.0217,  0.0137,  0.0159,  0.0120, -0.0097, -0.0311, -0.0221,\n",
      "         0.0104, -0.0309,  0.0158, -0.0201,  0.0120,  0.0058, -0.0082,  0.0187,\n",
      "        -0.0113, -0.0214,  0.0286, -0.0208,  0.0282, -0.0025, -0.0047,  0.0164,\n",
      "        -0.0134,  0.0023,  0.0081,  0.0021], requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : weight, param : Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "name : bias, param : Parameter containing:\n",
      "tensor([-0.0059,  0.0014,  0.0124, -0.0325, -0.0294, -0.0032, -0.0088,  0.0354,\n",
      "         0.0294,  0.0098], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model.apply(assign_wegiths)\n",
    "modules = model.named_modules()\n",
    "print(model._parameters)\n",
    "print()\n",
    "\n",
    "for name, module in modules:            \n",
    "    for name, param in module._parameters.items():\n",
    "        print(type(param))\n",
    "        print(\"name : {}, param : {}\".format(name, param))          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply 자체는 module에서 직접적으로 접근할 수 있는 필드를 조작해주는 함수를 작성하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0411, -0.1729,  0.1706, -0.0673,  0.0461],\n",
      "          [-0.0476, -0.0046,  0.0093,  0.1229,  0.0585],\n",
      "          [-0.1174, -0.0133,  0.1780, -0.1656, -0.0202],\n",
      "          [ 0.0122,  0.0513,  0.0029,  0.0255, -0.1029],\n",
      "          [-0.0906,  0.0370, -0.0006, -0.1794, -0.0729]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0072, -0.0406, -0.1690, -0.1485, -0.1301],\n",
      "          [-0.1819, -0.0775,  0.0449,  0.1010, -0.1782],\n",
      "          [-0.0521,  0.0931,  0.0296,  0.1918,  0.1151],\n",
      "          [-0.1578, -0.0983,  0.0035,  0.1465, -0.0625],\n",
      "          [ 0.1458, -0.0007, -0.0002,  0.0013, -0.1074]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0110, -0.1626, -0.0492, -0.0662,  0.1429],\n",
      "          [-0.0963, -0.1025,  0.1668, -0.0074, -0.0139],\n",
      "          [ 0.0108, -0.1534, -0.0606,  0.0542,  0.0116],\n",
      "          [ 0.1076, -0.0449,  0.0081,  0.1506, -0.0259],\n",
      "          [ 0.0206, -0.1881, -0.1942,  0.1247,  0.1459]]],\n",
      "\n",
      "\n",
      "        [[[-0.1728, -0.1562,  0.1251, -0.0961,  0.1123],\n",
      "          [-0.1489,  0.0946,  0.0344,  0.1181,  0.1117],\n",
      "          [-0.1341,  0.1344,  0.1149, -0.1092,  0.0089],\n",
      "          [ 0.1290, -0.0512,  0.0530,  0.1813,  0.0641],\n",
      "          [ 0.0802, -0.0717, -0.0508, -0.0682,  0.0559]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0631,  0.1543, -0.1726, -0.0492,  0.1192],\n",
      "          [-0.1761,  0.1032,  0.0389, -0.0370,  0.1072],\n",
      "          [-0.1470,  0.1388,  0.1911,  0.0785, -0.1127],\n",
      "          [ 0.0552, -0.1653, -0.1328,  0.0012,  0.0937],\n",
      "          [ 0.1152, -0.0988, -0.0538, -0.1257,  0.0370]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1673, -0.1511, -0.1734, -0.1163, -0.1598],\n",
      "          [ 0.0267, -0.1173, -0.1417, -0.1562,  0.0355],\n",
      "          [ 0.0611, -0.1812, -0.0273, -0.1828, -0.0325],\n",
      "          [-0.0105,  0.1409, -0.0523, -0.1648,  0.0212],\n",
      "          [ 0.1260,  0.0271, -0.0008,  0.1807,  0.0970]]],\n",
      "\n",
      "\n",
      "        [[[-0.1391, -0.1479, -0.0394,  0.1937, -0.1441],\n",
      "          [ 0.0076, -0.1196, -0.0801,  0.0890, -0.0681],\n",
      "          [-0.0946, -0.0881,  0.1000,  0.1630, -0.1107],\n",
      "          [ 0.1693,  0.0399,  0.1325, -0.0216, -0.0557],\n",
      "          [ 0.1544,  0.0362,  0.0127,  0.1800,  0.1564]]],\n",
      "\n",
      "\n",
      "        [[[-0.1259, -0.1438, -0.1066,  0.0550, -0.1183],\n",
      "          [-0.0088, -0.1278, -0.1717,  0.1944, -0.0738],\n",
      "          [-0.0302, -0.0086,  0.0195, -0.1137, -0.0572],\n",
      "          [-0.0401, -0.1978, -0.0481, -0.0754,  0.1909],\n",
      "          [-0.1060, -0.1082,  0.0132,  0.0339,  0.0066]]],\n",
      "\n",
      "\n",
      "        [[[-0.1457,  0.0714, -0.1245, -0.1769, -0.0876],\n",
      "          [ 0.0928,  0.0133,  0.1361,  0.1278, -0.0478],\n",
      "          [-0.0170, -0.1756, -0.1367,  0.0382,  0.0042],\n",
      "          [-0.0261,  0.1730, -0.1687, -0.1878,  0.1347],\n",
      "          [ 0.1475, -0.0297,  0.0090,  0.0977,  0.1015]]],\n",
      "\n",
      "\n",
      "        [[[-0.0469, -0.0687, -0.1201, -0.1608, -0.0880],\n",
      "          [-0.1539, -0.0952,  0.1719,  0.0481, -0.0877],\n",
      "          [-0.1013, -0.1839, -0.1329,  0.1661,  0.0305],\n",
      "          [-0.0403,  0.0522, -0.1862, -0.0767, -0.0926],\n",
      "          [-0.1876, -0.0494,  0.1938, -0.0122,  0.1186]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0309, -0.1171, -0.1970,  0.1636, -0.0348],\n",
      "          [-0.1154,  0.0238, -0.0661,  0.0956,  0.0877],\n",
      "          [-0.1855,  0.1546, -0.1902,  0.1218,  0.0052],\n",
      "          [ 0.1212,  0.0732, -0.1235,  0.0600, -0.1086],\n",
      "          [-0.1425,  0.0155, -0.1237, -0.0169, -0.1596]]],\n",
      "\n",
      "\n",
      "        [[[-0.0148,  0.0731,  0.1983,  0.1861,  0.0908],\n",
      "          [-0.1919, -0.1878, -0.1759, -0.1932,  0.1432],\n",
      "          [-0.0895,  0.1379,  0.0535,  0.1223, -0.1038],\n",
      "          [-0.0071, -0.0605,  0.0704, -0.0764,  0.1534],\n",
      "          [-0.0057, -0.0407,  0.1762, -0.1588,  0.0205]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0463, -0.1469,  0.1357, -0.0798,  0.1085],\n",
      "          [ 0.1776,  0.0615, -0.0087, -0.0382, -0.0338],\n",
      "          [ 0.1671, -0.0537, -0.0476,  0.0593,  0.1972],\n",
      "          [ 0.0008,  0.0303,  0.1997,  0.1174,  0.0405],\n",
      "          [ 0.1349, -0.1586,  0.0963, -0.0671,  0.0986]]],\n",
      "\n",
      "\n",
      "        [[[-0.0775,  0.0897,  0.1130, -0.0205,  0.1049],\n",
      "          [ 0.0349, -0.1591, -0.1533, -0.0985,  0.0763],\n",
      "          [ 0.0371,  0.1035, -0.1648,  0.1322, -0.0877],\n",
      "          [-0.0396, -0.1472, -0.1024,  0.0872,  0.0544],\n",
      "          [-0.0880,  0.1173,  0.0534, -0.1180, -0.1970]]],\n",
      "\n",
      "\n",
      "        [[[-0.0327, -0.1124,  0.0680,  0.0347, -0.1784],\n",
      "          [ 0.0158, -0.1093, -0.0450, -0.0844,  0.1401],\n",
      "          [-0.1344,  0.0193, -0.0174, -0.1548, -0.0821],\n",
      "          [-0.0492,  0.0457, -0.1648, -0.0754, -0.0283],\n",
      "          [ 0.1642,  0.0025,  0.0595, -0.0723, -0.0362]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1887, -0.1807,  0.0631,  0.1579, -0.1998],\n",
      "          [ 0.1247,  0.0085, -0.1143, -0.0764, -0.0248],\n",
      "          [ 0.0037, -0.1540,  0.1115,  0.0723, -0.1283],\n",
      "          [-0.0055, -0.0633,  0.0751,  0.1949, -0.0172],\n",
      "          [-0.0119, -0.1949, -0.1115,  0.1470, -0.1935]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0203, -0.0467,  0.1153, -0.1061,  0.0073],\n",
      "          [-0.1374, -0.0073,  0.1795, -0.1322, -0.0914],\n",
      "          [-0.1730,  0.1987,  0.0858, -0.0744,  0.1329],\n",
      "          [-0.0944,  0.0914,  0.1228,  0.0359,  0.0885],\n",
      "          [-0.0622,  0.1342,  0.1236,  0.0518, -0.0235]]],\n",
      "\n",
      "\n",
      "        [[[-0.0892, -0.1438,  0.0550, -0.0412,  0.0192],\n",
      "          [ 0.1342, -0.1504, -0.1275,  0.1487,  0.0803],\n",
      "          [ 0.0955, -0.1382,  0.0018, -0.1986,  0.0698],\n",
      "          [ 0.0091,  0.0382,  0.1101,  0.1563,  0.1332],\n",
      "          [ 0.1027, -0.1081, -0.1330, -0.0828,  0.1848]]],\n",
      "\n",
      "\n",
      "        [[[-0.0285, -0.1030, -0.0168,  0.0591, -0.1169],\n",
      "          [ 0.0269,  0.1854, -0.1529,  0.1265, -0.1033],\n",
      "          [ 0.1553,  0.0806,  0.0485,  0.1585, -0.1982],\n",
      "          [ 0.1352,  0.1895, -0.1937, -0.0188, -0.1632],\n",
      "          [-0.1691, -0.1913,  0.1965,  0.0430, -0.0821]]],\n",
      "\n",
      "\n",
      "        [[[-0.1168, -0.0039,  0.0152,  0.0151, -0.0858],\n",
      "          [-0.0565,  0.0502, -0.1008, -0.0699, -0.1407],\n",
      "          [-0.0252,  0.0732,  0.1659,  0.0579, -0.0174],\n",
      "          [-0.1423,  0.0146, -0.1682,  0.0807, -0.1951],\n",
      "          [-0.1877, -0.1012,  0.0392,  0.0386,  0.0546]]]],\n",
      "       requires_grad=True)\n",
      "Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0025,  0.0063, -0.0115,  0.0333, -0.0147],\n",
      "          [ 0.0087,  0.0090, -0.0212, -0.0122, -0.0418],\n",
      "          [-0.0365,  0.0076, -0.0158,  0.0245, -0.0221],\n",
      "          [ 0.0084,  0.0122,  0.0125, -0.0282,  0.0022],\n",
      "          [ 0.0109, -0.0229,  0.0072, -0.0326, -0.0316]],\n",
      "\n",
      "         [[ 0.0223, -0.0027,  0.0239, -0.0043, -0.0204],\n",
      "          [-0.0143,  0.0270, -0.0057, -0.0257,  0.0378],\n",
      "          [ 0.0445,  0.0345,  0.0053, -0.0001,  0.0300],\n",
      "          [-0.0146,  0.0304,  0.0319, -0.0267, -0.0293],\n",
      "          [ 0.0245,  0.0018,  0.0004, -0.0176, -0.0098]],\n",
      "\n",
      "         [[ 0.0050,  0.0367,  0.0120,  0.0208,  0.0252],\n",
      "          [-0.0354,  0.0282,  0.0430, -0.0424,  0.0202],\n",
      "          [-0.0290,  0.0416, -0.0406, -0.0302, -0.0182],\n",
      "          [ 0.0246,  0.0019, -0.0302,  0.0264, -0.0433],\n",
      "          [ 0.0284, -0.0411, -0.0130, -0.0401, -0.0115]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0155, -0.0427,  0.0286,  0.0129, -0.0312],\n",
      "          [ 0.0237,  0.0236, -0.0345, -0.0437, -0.0136],\n",
      "          [-0.0116,  0.0002,  0.0246,  0.0230,  0.0329],\n",
      "          [-0.0070,  0.0410,  0.0430,  0.0184,  0.0368],\n",
      "          [-0.0395,  0.0022,  0.0447,  0.0029, -0.0403]],\n",
      "\n",
      "         [[-0.0060,  0.0373,  0.0384, -0.0284,  0.0292],\n",
      "          [ 0.0158, -0.0215, -0.0025,  0.0317,  0.0025],\n",
      "          [ 0.0233,  0.0151,  0.0118, -0.0401,  0.0138],\n",
      "          [-0.0431,  0.0421, -0.0305, -0.0178,  0.0191],\n",
      "          [-0.0026,  0.0162, -0.0293, -0.0256, -0.0372]],\n",
      "\n",
      "         [[ 0.0058, -0.0009, -0.0266,  0.0076,  0.0326],\n",
      "          [ 0.0364, -0.0018,  0.0128,  0.0331, -0.0280],\n",
      "          [-0.0116,  0.0113,  0.0155, -0.0236,  0.0010],\n",
      "          [ 0.0146, -0.0412,  0.0053, -0.0175,  0.0396],\n",
      "          [-0.0039, -0.0309, -0.0447,  0.0235, -0.0180]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0314,  0.0154, -0.0385,  0.0036,  0.0130],\n",
      "          [ 0.0361, -0.0088,  0.0244, -0.0265, -0.0244],\n",
      "          [-0.0111, -0.0229, -0.0129,  0.0046, -0.0099],\n",
      "          [ 0.0377, -0.0135,  0.0078,  0.0437, -0.0200],\n",
      "          [ 0.0160, -0.0055,  0.0352,  0.0076,  0.0423]],\n",
      "\n",
      "         [[ 0.0236,  0.0228,  0.0015,  0.0091,  0.0164],\n",
      "          [-0.0063,  0.0414, -0.0155,  0.0138,  0.0247],\n",
      "          [ 0.0090,  0.0437, -0.0250,  0.0366,  0.0102],\n",
      "          [ 0.0023,  0.0066,  0.0106,  0.0260,  0.0016],\n",
      "          [ 0.0109, -0.0031,  0.0208,  0.0040, -0.0111]],\n",
      "\n",
      "         [[-0.0136,  0.0296,  0.0412, -0.0110,  0.0005],\n",
      "          [-0.0271,  0.0243, -0.0353, -0.0128,  0.0056],\n",
      "          [-0.0132, -0.0392, -0.0042, -0.0167, -0.0338],\n",
      "          [ 0.0130, -0.0332, -0.0392, -0.0107,  0.0297],\n",
      "          [ 0.0191, -0.0364,  0.0367,  0.0446,  0.0183]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0210, -0.0191,  0.0159,  0.0086,  0.0410],\n",
      "          [ 0.0058,  0.0288, -0.0182,  0.0290,  0.0151],\n",
      "          [ 0.0151, -0.0178, -0.0018,  0.0293, -0.0371],\n",
      "          [ 0.0179,  0.0396, -0.0287,  0.0398,  0.0247],\n",
      "          [-0.0006, -0.0194,  0.0441, -0.0135,  0.0007]],\n",
      "\n",
      "         [[ 0.0287, -0.0357, -0.0166, -0.0357,  0.0115],\n",
      "          [-0.0348,  0.0054, -0.0053,  0.0268, -0.0388],\n",
      "          [ 0.0368, -0.0196, -0.0086, -0.0437, -0.0399],\n",
      "          [ 0.0439,  0.0369,  0.0015,  0.0202, -0.0376],\n",
      "          [ 0.0356, -0.0442,  0.0120, -0.0426, -0.0179]],\n",
      "\n",
      "         [[-0.0060, -0.0424,  0.0258,  0.0328, -0.0268],\n",
      "          [ 0.0117, -0.0338, -0.0092,  0.0255, -0.0277],\n",
      "          [ 0.0294, -0.0212,  0.0179, -0.0134, -0.0003],\n",
      "          [ 0.0308, -0.0287, -0.0149,  0.0098, -0.0256],\n",
      "          [ 0.0136, -0.0250, -0.0192, -0.0345, -0.0139]]],\n",
      "\n",
      "\n",
      "        [[[-0.0006,  0.0354, -0.0174, -0.0344, -0.0208],\n",
      "          [-0.0322, -0.0429, -0.0061,  0.0007,  0.0239],\n",
      "          [-0.0307, -0.0317,  0.0290,  0.0053, -0.0297],\n",
      "          [-0.0177,  0.0080, -0.0024, -0.0080, -0.0294],\n",
      "          [ 0.0368,  0.0409, -0.0412,  0.0438, -0.0163]],\n",
      "\n",
      "         [[-0.0010,  0.0434, -0.0287, -0.0284,  0.0418],\n",
      "          [ 0.0111, -0.0394,  0.0359, -0.0344,  0.0345],\n",
      "          [ 0.0129, -0.0348,  0.0079, -0.0359, -0.0396],\n",
      "          [ 0.0163, -0.0012, -0.0201,  0.0442, -0.0054],\n",
      "          [-0.0288, -0.0256, -0.0117,  0.0149, -0.0219]],\n",
      "\n",
      "         [[-0.0214,  0.0216, -0.0403,  0.0427,  0.0164],\n",
      "          [ 0.0311,  0.0412,  0.0159, -0.0159, -0.0196],\n",
      "          [ 0.0388,  0.0288, -0.0077, -0.0149, -0.0246],\n",
      "          [ 0.0443, -0.0306, -0.0196,  0.0186,  0.0332],\n",
      "          [-0.0342, -0.0375,  0.0383, -0.0074,  0.0127]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0212, -0.0266, -0.0083, -0.0308,  0.0105],\n",
      "          [-0.0138,  0.0276, -0.0348, -0.0312, -0.0135],\n",
      "          [ 0.0221, -0.0349, -0.0050,  0.0100, -0.0263],\n",
      "          [-0.0327, -0.0415, -0.0175,  0.0039, -0.0264],\n",
      "          [ 0.0138,  0.0234, -0.0421,  0.0174, -0.0432]],\n",
      "\n",
      "         [[-0.0314, -0.0051,  0.0075, -0.0426, -0.0155],\n",
      "          [ 0.0382, -0.0093, -0.0057,  0.0283, -0.0122],\n",
      "          [ 0.0435,  0.0016,  0.0419, -0.0359, -0.0311],\n",
      "          [ 0.0435, -0.0083,  0.0283, -0.0327, -0.0038],\n",
      "          [-0.0359,  0.0262, -0.0017, -0.0163, -0.0168]],\n",
      "\n",
      "         [[-0.0183,  0.0138, -0.0026, -0.0237, -0.0238],\n",
      "          [ 0.0070,  0.0113, -0.0028, -0.0201, -0.0033],\n",
      "          [ 0.0418, -0.0436, -0.0154,  0.0430, -0.0197],\n",
      "          [ 0.0130, -0.0015,  0.0232, -0.0125, -0.0152],\n",
      "          [-0.0356, -0.0106, -0.0101,  0.0387,  0.0238]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0365, -0.0100, -0.0039,  0.0312,  0.0243],\n",
      "          [ 0.0380, -0.0169,  0.0138, -0.0038,  0.0175],\n",
      "          [-0.0090,  0.0318, -0.0330,  0.0268, -0.0265],\n",
      "          [-0.0137, -0.0163, -0.0308, -0.0255,  0.0079],\n",
      "          [ 0.0103, -0.0378,  0.0008,  0.0391, -0.0033]],\n",
      "\n",
      "         [[ 0.0030,  0.0021,  0.0363,  0.0296, -0.0247],\n",
      "          [-0.0067,  0.0164,  0.0093, -0.0122, -0.0091],\n",
      "          [ 0.0245, -0.0089,  0.0179, -0.0253,  0.0330],\n",
      "          [ 0.0042,  0.0060, -0.0187,  0.0278, -0.0023],\n",
      "          [-0.0284,  0.0116,  0.0049,  0.0075,  0.0329]],\n",
      "\n",
      "         [[ 0.0099,  0.0381, -0.0397,  0.0219, -0.0018],\n",
      "          [ 0.0254, -0.0227, -0.0309,  0.0293,  0.0405],\n",
      "          [-0.0016,  0.0188, -0.0363, -0.0279, -0.0374],\n",
      "          [ 0.0047, -0.0178, -0.0083,  0.0289,  0.0058],\n",
      "          [-0.0309,  0.0053, -0.0282, -0.0281,  0.0234]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0171, -0.0025,  0.0391, -0.0381,  0.0032],\n",
      "          [ 0.0210, -0.0008,  0.0016, -0.0233, -0.0226],\n",
      "          [ 0.0143, -0.0412,  0.0091, -0.0038, -0.0379],\n",
      "          [ 0.0403, -0.0361,  0.0361, -0.0060,  0.0409],\n",
      "          [ 0.0440,  0.0366,  0.0039,  0.0044,  0.0138]],\n",
      "\n",
      "         [[-0.0133,  0.0377,  0.0422,  0.0204, -0.0022],\n",
      "          [ 0.0435,  0.0047, -0.0206, -0.0079,  0.0185],\n",
      "          [ 0.0427, -0.0128,  0.0196,  0.0367, -0.0299],\n",
      "          [ 0.0422,  0.0438,  0.0179, -0.0162, -0.0006],\n",
      "          [-0.0014,  0.0113,  0.0203, -0.0260,  0.0069]],\n",
      "\n",
      "         [[-0.0014, -0.0011,  0.0126,  0.0200,  0.0380],\n",
      "          [ 0.0093,  0.0429, -0.0336, -0.0041,  0.0195],\n",
      "          [ 0.0134, -0.0383,  0.0252, -0.0407,  0.0442],\n",
      "          [-0.0098,  0.0275,  0.0087, -0.0250,  0.0060],\n",
      "          [ 0.0225,  0.0051,  0.0444,  0.0043,  0.0296]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0099, -0.0313, -0.0038, -0.0198, -0.0389],\n",
      "          [-0.0383,  0.0378, -0.0023, -0.0108, -0.0294],\n",
      "          [ 0.0156, -0.0087, -0.0366,  0.0133,  0.0385],\n",
      "          [-0.0374,  0.0438, -0.0170, -0.0307, -0.0249],\n",
      "          [ 0.0056,  0.0216, -0.0308, -0.0339, -0.0395]],\n",
      "\n",
      "         [[ 0.0438, -0.0412,  0.0080, -0.0098,  0.0068],\n",
      "          [-0.0151, -0.0361,  0.0085, -0.0431,  0.0245],\n",
      "          [-0.0210,  0.0323, -0.0093,  0.0083, -0.0171],\n",
      "          [-0.0393, -0.0134,  0.0032,  0.0186, -0.0274],\n",
      "          [ 0.0280,  0.0382,  0.0156, -0.0372,  0.0313]],\n",
      "\n",
      "         [[-0.0354, -0.0108,  0.0240,  0.0140, -0.0111],\n",
      "          [-0.0185,  0.0143, -0.0025, -0.0297,  0.0384],\n",
      "          [ 0.0065,  0.0184, -0.0058,  0.0237,  0.0162],\n",
      "          [ 0.0073, -0.0207, -0.0293, -0.0132,  0.0238],\n",
      "          [-0.0423, -0.0121, -0.0194, -0.0046, -0.0364]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0067,  0.0182,  0.0048, -0.0029, -0.0228],\n",
      "          [-0.0027, -0.0237,  0.0441,  0.0306,  0.0306],\n",
      "          [ 0.0174, -0.0405, -0.0409,  0.0041, -0.0447],\n",
      "          [ 0.0151,  0.0159,  0.0096, -0.0391, -0.0345],\n",
      "          [ 0.0320,  0.0071, -0.0261, -0.0215, -0.0179]],\n",
      "\n",
      "         [[ 0.0228,  0.0234,  0.0092,  0.0443, -0.0395],\n",
      "          [ 0.0300,  0.0150,  0.0131, -0.0139,  0.0148],\n",
      "          [-0.0257,  0.0025,  0.0129, -0.0236, -0.0265],\n",
      "          [ 0.0155,  0.0336, -0.0425, -0.0180,  0.0318],\n",
      "          [-0.0326, -0.0166,  0.0446,  0.0091, -0.0035]],\n",
      "\n",
      "         [[ 0.0074, -0.0317, -0.0225,  0.0305,  0.0188],\n",
      "          [ 0.0334,  0.0277,  0.0299,  0.0215, -0.0283],\n",
      "          [ 0.0106, -0.0426, -0.0052, -0.0008,  0.0271],\n",
      "          [-0.0310,  0.0017, -0.0409, -0.0123, -0.0060],\n",
      "          [ 0.0259, -0.0207, -0.0077,  0.0311,  0.0357]]],\n",
      "\n",
      "\n",
      "        [[[-0.0422, -0.0067,  0.0243,  0.0048,  0.0337],\n",
      "          [ 0.0139,  0.0300, -0.0200, -0.0029,  0.0374],\n",
      "          [ 0.0039, -0.0424,  0.0006, -0.0310, -0.0351],\n",
      "          [-0.0349, -0.0329, -0.0085,  0.0266, -0.0354],\n",
      "          [ 0.0065,  0.0081,  0.0328,  0.0192,  0.0187]],\n",
      "\n",
      "         [[ 0.0326, -0.0054, -0.0274, -0.0159, -0.0004],\n",
      "          [-0.0027,  0.0238,  0.0122,  0.0200, -0.0194],\n",
      "          [-0.0263, -0.0145,  0.0391, -0.0395, -0.0157],\n",
      "          [ 0.0423, -0.0412,  0.0166,  0.0191,  0.0243],\n",
      "          [ 0.0339, -0.0035,  0.0380,  0.0443,  0.0409]],\n",
      "\n",
      "         [[ 0.0302,  0.0342, -0.0254,  0.0070,  0.0083],\n",
      "          [-0.0123,  0.0350,  0.0319, -0.0046,  0.0214],\n",
      "          [-0.0331,  0.0046, -0.0236,  0.0317, -0.0002],\n",
      "          [ 0.0328, -0.0356, -0.0079,  0.0346, -0.0367],\n",
      "          [-0.0070, -0.0275,  0.0083,  0.0363, -0.0428]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0274, -0.0260, -0.0262, -0.0102,  0.0247],\n",
      "          [ 0.0175,  0.0223, -0.0206, -0.0165, -0.0353],\n",
      "          [ 0.0019, -0.0156, -0.0120, -0.0090,  0.0370],\n",
      "          [ 0.0357,  0.0436,  0.0128,  0.0297,  0.0318],\n",
      "          [-0.0161,  0.0180,  0.0302,  0.0416,  0.0008]],\n",
      "\n",
      "         [[ 0.0209,  0.0179, -0.0263, -0.0105,  0.0334],\n",
      "          [-0.0389,  0.0048, -0.0223,  0.0355, -0.0336],\n",
      "          [ 0.0304, -0.0066,  0.0219,  0.0102, -0.0004],\n",
      "          [ 0.0026,  0.0164,  0.0222, -0.0130,  0.0334],\n",
      "          [-0.0032,  0.0290, -0.0305, -0.0249,  0.0322]],\n",
      "\n",
      "         [[ 0.0159,  0.0001, -0.0224,  0.0010, -0.0134],\n",
      "          [ 0.0348,  0.0240, -0.0367, -0.0003,  0.0378],\n",
      "          [ 0.0068,  0.0168,  0.0375, -0.0012, -0.0038],\n",
      "          [-0.0208,  0.0447, -0.0414,  0.0059,  0.0417],\n",
      "          [-0.0206,  0.0027, -0.0262, -0.0108,  0.0444]]]],\n",
      "       requires_grad=True)\n",
      "Linear(in_features=800, out_features=500, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0078, -0.0068, -0.0138,  ..., -0.0210, -0.0239, -0.0104],\n",
      "        [-0.0130, -0.0206, -0.0051,  ...,  0.0277, -0.0024,  0.0198],\n",
      "        [ 0.0328, -0.0133,  0.0137,  ...,  0.0172, -0.0109, -0.0241],\n",
      "        ...,\n",
      "        [ 0.0063, -0.0176,  0.0335,  ..., -0.0092,  0.0308,  0.0201],\n",
      "        [ 0.0055, -0.0111, -0.0226,  ...,  0.0118,  0.0075, -0.0310],\n",
      "        [-0.0029,  0.0115, -0.0103,  ..., -0.0140, -0.0116, -0.0108]],\n",
      "       requires_grad=True)\n",
      "Linear(in_features=500, out_features=10, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0364,  0.0442,  0.0008,  ..., -0.0345,  0.0093, -0.0088],\n",
      "        [-0.0360,  0.0091,  0.0442,  ...,  0.0006, -0.0188, -0.0421],\n",
      "        [ 0.0365,  0.0107, -0.0368,  ...,  0.0111,  0.0183, -0.0401],\n",
      "        ...,\n",
      "        [-0.0284, -0.0006, -0.0301,  ..., -0.0265,  0.0005,  0.0196],\n",
      "        [ 0.0199, -0.0010, -0.0407,  ..., -0.0128, -0.0347,  0.0021],\n",
      "        [-0.0303, -0.0253, -0.0405,  ...,  0.0036,  0.0305,  0.0017]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model=Model()\n",
    "test = torch.nn.Parameter(data=torch.Tensor(data=[[1, 2, 3], [4, 5, 6]]), requires_grad=True)\n",
    "model.register_parameter(\"weight\", test)\n",
    "for name, module in model._modules.items():\n",
    "    \n",
    "    print(module)\n",
    "    print(module.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(module.weight.data.fill_(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply(self, fn):\n",
    "        for module in self.children():\n",
    "            module._apply(fn)\n",
    "\n",
    "        for param in self._parameters.values():\n",
    "            if param is not None:\n",
    "                # Tensors stored in modules are graph leaves, and we don't\n",
    "                # want to create copy nodes, so we have to unpack the data.\n",
    "                param.data = fn(param.data)\n",
    "                if param._grad is not None:\n",
    "                    param._grad.data = fn(param._grad.data)\n",
    "\n",
    "        for key, buf in self._buffers.items():\n",
    "            if buf is not None:\n",
    "                self._buffers[key] = fn(buf)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_apply` 함수는 `_parameters`와 `_buffer`에 `apply`함수 역활을 하며  \n",
    "`_parameters`의 경우에는 `_parameters`의 `_grad`가 `None`인 경우에는 `_grad`에도 `fn`을 적용한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cuda method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda(self, device=None):\n",
    "        r\"\"\"Moves all model parameters and buffers to the GPU.\n",
    "\n",
    "        This also makes associated parameters and buffers different objects. So\n",
    "        it should be called before constructing optimizer if the module will\n",
    "        live on GPU while being optimized.\n",
    "\n",
    "        Arguments:\n",
    "            device (int, optional): if specified, all parameters will be\n",
    "                copied to that device\n",
    "\n",
    "        Returns:\n",
    "            Module: self\n",
    "        \"\"\"\n",
    "        return self._apply(lambda t: t.cuda(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signature: torch.nn.modules.Module.cuda(self, device=None)  \n",
    "     \n",
    "Docstring:  \n",
    "Moves all model parameters and buffers to the GPU.  \n",
    "        \n",
    "This also makes associated parameters and buffers different objects. So  \n",
    "it should be called before constructing optimizer if the module will  \n",
    "live on GPU while being optimized.  \n",
    "    \n",
    "Arguments:  \n",
    "```\n",
    "    device (int, optional): if specified, all parameters will be\n",
    "        copied to that device\n",
    "```\n",
    "    \n",
    "Returns:  \n",
    "    Module: self  \n",
    "        \n",
    "File:      /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py  \n",
    "Type:      function  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "관련된 parameters나 buffer들을 다른 object로 변경함  \n",
    "     \n",
    "따라서 최적화를 GPU에서 수행하고 싶다면, optimizer를 만들기 전에 호출되어야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cpu method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def cpu(self):\n",
    "        r\"\"\"Moves all model parameters and buffers to the CPU.\n",
    "\n",
    "        Returns:\n",
    "            Module: self\n",
    "        \"\"\"\n",
    "        return self._apply(lambda t: t.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signature: torch.nn.modules.Module.cpu(self)  \n",
    "    \n",
    "Docstring:  \n",
    "Moves all model parameters and buffers to the CPU.  \n",
    "    \n",
    "Returns:  \n",
    "    Module: self  \n",
    "        \n",
    "File:      /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py  \n",
    "Type:      function  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 모든 parameter, buffer들을 cpu로 이동함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dtype casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type(self, dst_type):\n",
    "    r\"\"\"Casts all parameters and buffers to :attr:`dst_type`.\n",
    "\n",
    "    Arguments:\n",
    "        dst_type (type or string): the desired type\n",
    "\n",
    "    Returns:\n",
    "        Module: self\n",
    "    \"\"\"\n",
    "    return self._apply(lambda t: t.type(dst_type))\n",
    "\n",
    "\n",
    "def float(self):\n",
    "    r\"\"\"Casts all floating point parameters and buffers to float datatype.\n",
    "\n",
    "    Returns:\n",
    "        Module: self\n",
    "    \"\"\"\n",
    "    return self._apply(lambda t: t.float() if t.is_floating_point() else t)\n",
    "\n",
    "\n",
    "def double(self):\n",
    "    r\"\"\"Casts all floating point parameters and buffers to ``double`` datatype.\n",
    "\n",
    "    Returns:\n",
    "        Module: self\n",
    "    \"\"\"\n",
    "    return self._apply(lambda t: t.double() if t.is_floating_point() else t)\n",
    "\n",
    "\n",
    "def half(self):\n",
    "    r\"\"\"Casts all floating point parameters and buffers to ``half`` datatype.\n",
    "\n",
    "    Returns:\n",
    "        Module: self\n",
    "    \"\"\"\n",
    "    return self._apply(lambda t: t.half() if t.is_floating_point() else t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function <lambda> at 0x7f06086267b8>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
