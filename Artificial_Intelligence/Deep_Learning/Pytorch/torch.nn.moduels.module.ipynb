{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('__call__', <function Module.__call__ at 0x7f38d2c85400>)\n",
      "\n",
      "('__class__', <class 'type'>)\n",
      "\n",
      "('__delattr__', <function Module.__delattr__ at 0x7f38d2c85620>)\n",
      "\n",
      "('__dict__', mappingproxy({'__module__': 'torch.nn.modules.module', '__doc__': 'Base class for all neural network modules.\\n\\n    Your models should also subclass this class.\\n\\n    Modules can also contain other Modules, allowing to nest them in\\n    a tree structure. You can assign the submodules as regular attributes::\\n\\n        import torch.nn as nn\\n        import torch.nn.functional as F\\n\\n        class Model(nn.Module):\\n            def __init__(self):\\n                super(Model, self).__init__()\\n                self.conv1 = nn.Conv2d(1, 20, 5)\\n                self.conv2 = nn.Conv2d(20, 20, 5)\\n\\n            def forward(self, x):\\n               x = F.relu(self.conv1(x))\\n               return F.relu(self.conv2(x))\\n\\n    Submodules assigned in this way will be registered, and will have their\\n    parameters converted too when you call :meth:`to`, etc.\\n    ', 'dump_patches': False, '_version': 1, '__init__': <function Module.__init__ at 0x7f38d2c81950>, 'forward': <function Module.forward at 0x7f38d2c819d8>, 'register_buffer': <function Module.register_buffer at 0x7f38d2c81a60>, 'register_parameter': <function Module.register_parameter at 0x7f38d2c81ae8>, 'add_module': <function Module.add_module at 0x7f38d2c81b70>, '_apply': <function Module._apply at 0x7f38d2c81bf8>, 'apply': <function Module.apply at 0x7f38d2c81c80>, 'cuda': <function Module.cuda at 0x7f38d2c81d08>, 'cpu': <function Module.cpu at 0x7f38d2c81d90>, 'type': <function Module.type at 0x7f38d2c81e18>, 'float': <function Module.float at 0x7f38d2c81ea0>, 'double': <function Module.double at 0x7f38d2c81f28>, 'half': <function Module.half at 0x7f38d2c85048>, 'to': <function Module.to at 0x7f38d2c850d0>, 'register_backward_hook': <function Module.register_backward_hook at 0x7f38d2c85158>, 'register_forward_pre_hook': <function Module.register_forward_pre_hook at 0x7f38d2c851e0>, 'register_forward_hook': <function Module.register_forward_hook at 0x7f38d2c85268>, '_tracing_name': <function Module._tracing_name at 0x7f38d2c852f0>, '_slow_forward': <function Module._slow_forward at 0x7f38d2c85378>, '__call__': <function Module.__call__ at 0x7f38d2c85400>, '__setstate__': <function Module.__setstate__ at 0x7f38d2c85488>, '__getattr__': <function Module.__getattr__ at 0x7f38d2c85510>, '__setattr__': <function Module.__setattr__ at 0x7f38d2c85598>, '__delattr__': <function Module.__delattr__ at 0x7f38d2c85620>, '_register_state_dict_hook': <function Module._register_state_dict_hook at 0x7f38d2c856a8>, 'state_dict': <function Module.state_dict at 0x7f38d2c85730>, '_register_load_state_dict_pre_hook': <function Module._register_load_state_dict_pre_hook at 0x7f38d2c857b8>, '_load_from_state_dict': <function Module._load_from_state_dict at 0x7f38d2c85840>, 'load_state_dict': <function Module.load_state_dict at 0x7f38d2c858c8>, '_named_members': <function Module._named_members at 0x7f38d2c85950>, 'parameters': <function Module.parameters at 0x7f38d2c859d8>, 'named_parameters': <function Module.named_parameters at 0x7f38d2c85a60>, 'buffers': <function Module.buffers at 0x7f38d2c85ae8>, 'named_buffers': <function Module.named_buffers at 0x7f38d2c85b70>, 'children': <function Module.children at 0x7f38d2c85bf8>, 'named_children': <function Module.named_children at 0x7f38d2c85c80>, 'modules': <function Module.modules at 0x7f38d2c85d08>, 'named_modules': <function Module.named_modules at 0x7f38d2c85d90>, 'train': <function Module.train at 0x7f38d2c85e18>, 'eval': <function Module.eval at 0x7f38d2c85ea0>, 'zero_grad': <function Module.zero_grad at 0x7f38d2c85f28>, 'share_memory': <function Module.share_memory at 0x7f38d2c86048>, '_get_name': <function Module._get_name at 0x7f38d2c860d0>, 'extra_repr': <function Module.extra_repr at 0x7f38d2c86158>, '__repr__': <function Module.__repr__ at 0x7f38d2c861e0>, '__dir__': <function Module.__dir__ at 0x7f38d2c86268>, '__dict__': <attribute '__dict__' of 'Module' objects>, '__weakref__': <attribute '__weakref__' of 'Module' objects>}))\n",
      "\n",
      "('__dir__', <function Module.__dir__ at 0x7f38d2c86268>)\n",
      "\n",
      "('__doc__', 'Base class for all neural network modules.\\n\\n    Your models should also subclass this class.\\n\\n    Modules can also contain other Modules, allowing to nest them in\\n    a tree structure. You can assign the submodules as regular attributes::\\n\\n        import torch.nn as nn\\n        import torch.nn.functional as F\\n\\n        class Model(nn.Module):\\n            def __init__(self):\\n                super(Model, self).__init__()\\n                self.conv1 = nn.Conv2d(1, 20, 5)\\n                self.conv2 = nn.Conv2d(20, 20, 5)\\n\\n            def forward(self, x):\\n               x = F.relu(self.conv1(x))\\n               return F.relu(self.conv2(x))\\n\\n    Submodules assigned in this way will be registered, and will have their\\n    parameters converted too when you call :meth:`to`, etc.\\n    ')\n",
      "\n",
      "('__eq__', <slot wrapper '__eq__' of 'object' objects>)\n",
      "\n",
      "('__format__', <method '__format__' of 'object' objects>)\n",
      "\n",
      "('__ge__', <slot wrapper '__ge__' of 'object' objects>)\n",
      "\n",
      "('__getattr__', <function Module.__getattr__ at 0x7f38d2c85510>)\n",
      "\n",
      "('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>)\n",
      "\n",
      "('__gt__', <slot wrapper '__gt__' of 'object' objects>)\n",
      "\n",
      "('__hash__', <slot wrapper '__hash__' of 'object' objects>)\n",
      "\n",
      "('__init__', <function Module.__init__ at 0x7f38d2c81950>)\n",
      "\n",
      "('__init_subclass__', <built-in method __init_subclass__ of type object at 0x3658fb8>)\n",
      "\n",
      "('__le__', <slot wrapper '__le__' of 'object' objects>)\n",
      "\n",
      "('__lt__', <slot wrapper '__lt__' of 'object' objects>)\n",
      "\n",
      "('__module__', 'torch.nn.modules.module')\n",
      "\n",
      "('__ne__', <slot wrapper '__ne__' of 'object' objects>)\n",
      "\n",
      "('__new__', <built-in method __new__ of type object at 0x9d1260>)\n",
      "\n",
      "('__reduce__', <method '__reduce__' of 'object' objects>)\n",
      "\n",
      "('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>)\n",
      "\n",
      "('__repr__', <function Module.__repr__ at 0x7f38d2c861e0>)\n",
      "\n",
      "('__setattr__', <function Module.__setattr__ at 0x7f38d2c85598>)\n",
      "\n",
      "('__setstate__', <function Module.__setstate__ at 0x7f38d2c85488>)\n",
      "\n",
      "('__sizeof__', <method '__sizeof__' of 'object' objects>)\n",
      "\n",
      "('__str__', <slot wrapper '__str__' of 'object' objects>)\n",
      "\n",
      "('__subclasshook__', <built-in method __subclasshook__ of type object at 0x3658fb8>)\n",
      "\n",
      "('__weakref__', <attribute '__weakref__' of 'Module' objects>)\n",
      "\n",
      "('_apply', <function Module._apply at 0x7f38d2c81bf8>)\n",
      "\n",
      "('_get_name', <function Module._get_name at 0x7f38d2c860d0>)\n",
      "\n",
      "('_load_from_state_dict', <function Module._load_from_state_dict at 0x7f38d2c85840>)\n",
      "\n",
      "('_named_members', <function Module._named_members at 0x7f38d2c85950>)\n",
      "\n",
      "('_register_load_state_dict_pre_hook', <function Module._register_load_state_dict_pre_hook at 0x7f38d2c857b8>)\n",
      "\n",
      "('_register_state_dict_hook', <function Module._register_state_dict_hook at 0x7f38d2c856a8>)\n",
      "\n",
      "('_slow_forward', <function Module._slow_forward at 0x7f38d2c85378>)\n",
      "\n",
      "('_tracing_name', <function Module._tracing_name at 0x7f38d2c852f0>)\n",
      "\n",
      "('_version', 1)\n",
      "\n",
      "('add_module', <function Module.add_module at 0x7f38d2c81b70>)\n",
      "\n",
      "('apply', <function Module.apply at 0x7f38d2c81c80>)\n",
      "\n",
      "('buffers', <function Module.buffers at 0x7f38d2c85ae8>)\n",
      "\n",
      "('children', <function Module.children at 0x7f38d2c85bf8>)\n",
      "\n",
      "('cpu', <function Module.cpu at 0x7f38d2c81d90>)\n",
      "\n",
      "('cuda', <function Module.cuda at 0x7f38d2c81d08>)\n",
      "\n",
      "('double', <function Module.double at 0x7f38d2c81f28>)\n",
      "\n",
      "('dump_patches', False)\n",
      "\n",
      "('eval', <function Module.eval at 0x7f38d2c85ea0>)\n",
      "\n",
      "('extra_repr', <function Module.extra_repr at 0x7f38d2c86158>)\n",
      "\n",
      "('float', <function Module.float at 0x7f38d2c81ea0>)\n",
      "\n",
      "('forward', <function Module.forward at 0x7f38d2c819d8>)\n",
      "\n",
      "('half', <function Module.half at 0x7f38d2c85048>)\n",
      "\n",
      "('load_state_dict', <function Module.load_state_dict at 0x7f38d2c858c8>)\n",
      "\n",
      "('modules', <function Module.modules at 0x7f38d2c85d08>)\n",
      "\n",
      "('named_buffers', <function Module.named_buffers at 0x7f38d2c85b70>)\n",
      "\n",
      "('named_children', <function Module.named_children at 0x7f38d2c85c80>)\n",
      "\n",
      "('named_modules', <function Module.named_modules at 0x7f38d2c85d90>)\n",
      "\n",
      "('named_parameters', <function Module.named_parameters at 0x7f38d2c85a60>)\n",
      "\n",
      "('parameters', <function Module.parameters at 0x7f38d2c859d8>)\n",
      "\n",
      "('register_backward_hook', <function Module.register_backward_hook at 0x7f38d2c85158>)\n",
      "\n",
      "('register_buffer', <function Module.register_buffer at 0x7f38d2c81a60>)\n",
      "\n",
      "('register_forward_hook', <function Module.register_forward_hook at 0x7f38d2c85268>)\n",
      "\n",
      "('register_forward_pre_hook', <function Module.register_forward_pre_hook at 0x7f38d2c851e0>)\n",
      "\n",
      "('register_parameter', <function Module.register_parameter at 0x7f38d2c81ae8>)\n",
      "\n",
      "('share_memory', <function Module.share_memory at 0x7f38d2c86048>)\n",
      "\n",
      "('state_dict', <function Module.state_dict at 0x7f38d2c85730>)\n",
      "\n",
      "('to', <function Module.to at 0x7f38d2c850d0>)\n",
      "\n",
      "('train', <function Module.train at 0x7f38d2c85e18>)\n",
      "\n",
      "('type', <function Module.type at 0x7f38d2c81e18>)\n",
      "\n",
      "('zero_grad', <function Module.zero_grad at 0x7f38d2c85f28>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for element in inspect.getmembers(torch.nn.modules.Module):\n",
    "    print(\"{}\\n\".format(element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch `Module` class\n",
    "\n",
    "![torch nn modules module](https://user-images.githubusercontent.com/13328380/51440701-33105700-1d0d-11e9-9611-6bd5c517433c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "1. `:meth:`는 `method`를 의미함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init signature: torch.nn.modules.Module()\n",
    "Docstring:     \n",
    "## Base class for all neural network modules.\n",
    "\n",
    "Your models should also subclass this class.\n",
    "\n",
    "Modules can also contain other Modules, allowing to nest them in\n",
    "a tree structure. You can assign the submodules as regular attributes::\n",
    "```python\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Model, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "        def forward(self, x):\n",
    "           x = F.relu(self.conv1(x))\n",
    "           return F.relu(self.conv2(x))\n",
    "```\n",
    "\n",
    "Submodules assigned in this way will be registered, and will have their\n",
    "parameters converted too when you call :meth:`to`, etc.\n",
    "    \n",
    "File:           ~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py  \n",
    "Type:           type  \n",
    "Subclasses:     Linear, Bilinear, _ConvNd, Threshold, RReLU, Hardtanh, Sigmoid, Tanh, ELU, CELU, ...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field\n",
    "# what is mean?\n",
    "dump_patches = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field\n",
    "_version = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows better BC support for :meth:`load_state_dict`. In\n",
    "    :meth:`state_dict`, the version number will be saved as in the attribute\n",
    "    `_metadata` of the returned state dict, and thus pickled. `_metadata` is a\n",
    "    dictionary with keys that follow the naming convention of state dict. See\n",
    "    ``_load_from_state_dict`` on how to use this information in loading.\n",
    "\n",
    "    If new parameters/buffers are added/removed from a module, this number shall\n",
    "    be bumped, and the module's `_load_from_state_dict` method can compare the\n",
    "    version number and do appropriate changes if the state dict is from before\n",
    "    the change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_state_dict` method에 대한 BC 지원이 향상된다. `state_dict` method에서, 버전 번호는 return된 state dict의 `_metadata` 내부의 속성으로써 저장될 것이고, pickled된다.\n",
    "\n",
    "`_metadata`는 state dict의 naming convention을 따르는 key가 있는 dictionary다. 해당 정보를 어떻게 사용하는지 보고싶으면`_load_from_state_dict`을 봐라.\n",
    "\n",
    "만약 모듈에서 새로운 파라미터/버퍼가 추가되거나 제거된다면 해당 숫자는 충돌을 일으켜야한다.\n",
    "모듈의 `_load_from_state_dict` method는 버전 번호를 비교할 수 있고, 만약에 state dict이 이전 버전에서 온 경우, 이를 적절하게 변경할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "        self._backend = thnn_backend\n",
    "        self._parameters = OrderedDict()\n",
    "        self._buffers = OrderedDict()\n",
    "        self._backward_hooks = OrderedDict()\n",
    "        self._forward_hooks = OrderedDict()\n",
    "        self._forward_pre_hooks = OrderedDict()\n",
    "        self._state_dict_hooks = OrderedDict()\n",
    "        self._load_state_dict_pre_hooks = OrderedDict()\n",
    "        self._modules = OrderedDict()\n",
    "        self.training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    " def forward(self, *input):\n",
    "        r\"\"\"Defines the computation performed at every call.\n",
    "\n",
    "        Should be overridden by all subclasses.\n",
    "\n",
    "        .. note::\n",
    "            Although the recipe for forward pass needs to be defined within\n",
    "            this function, one should call the :class:`Module` instance afterwards\n",
    "            instead of this since the former takes care of running the\n",
    "            registered hooks while the latter silently ignores them.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signature: torch.nn.modules.Module.forward(self, *input)\n",
    "Docstring:\n",
    "Defines the computation performed at every call.\n",
    "\n",
    "Should be overridden by all subclasses.\n",
    "\n",
    ".. note::\n",
    "    Although the recipe for forward pass needs to be defined within\n",
    "    this function, one should call the :class:`Module` instance afterwards\n",
    "    instead of this since the former takes care of running the\n",
    "    registered hooks while the latter silently ignores them.  \n",
    "    \n",
    "File:      ~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py  \n",
    "Type:      function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 subclass들에 의해서 override되어야 한다.\n",
    "\n",
    "_**무슨 말일까?....**_\n",
    ">forwad pass를 위한 recipe는 `forward` method와 같이 정의되어야하지만  latter는 등록된 hook을 조용히 무시하는 반면에 former는 등록된 hook을 실행하기 때문에 Module Class instance는 나중에 호출할 필요가 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def register_buffer(self, name, tensor):\n",
    "        r\"\"\"Adds a persistent buffer to the module.\n",
    "\n",
    "        This is typically used to register a buffer that should not to be\n",
    "        considered a model parameter. For example, BatchNorm's ``running_mean``\n",
    "        is not a parameter, but is part of the persistent state.\n",
    "\n",
    "        Buffers can be accessed as attributes using given names.\n",
    "\n",
    "        Args:\n",
    "            name (string): name of the buffer. The buffer can be accessed\n",
    "                from this module using the given name\n",
    "            tensor (Tensor): buffer to be registered.\n",
    "\n",
    "        Example::\n",
    "\n",
    "            >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "\n",
    "        \"\"\"\n",
    "        if not isinstance(name, torch._six.string_classes):\n",
    "            raise TypeError(\"buffer name should be a string. \"\n",
    "                            \"Got {}\".format(torch.typename(name)))\n",
    "        elif '.' in name:\n",
    "            raise KeyError(\"buffer name can't contain \\\".\\\"\")\n",
    "        elif name == '':\n",
    "            raise KeyError(\"buffer name can't be empty string \\\"\\\"\")\n",
    "        elif hasattr(self, name) and name not in self._buffers:\n",
    "            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
    "        elif tensor is not None and not isinstance(tensor, torch.Tensor):\n",
    "            raise TypeError(\"cannot assign '{}' object to buffer '{}' \"\n",
    "                            \"(torch Tensor or None required)\"\n",
    "                            .format(torch.typename(tensor), name))\n",
    "        else:\n",
    "            self._buffers[name] = tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signature: torch.nn.modules.Module.register_buffer(self, name, tensor)\n",
    "Docstring:\n",
    "Adds a persistent buffer to the module.\n",
    "\n",
    "This is typically used to register a buffer that should not to be\n",
    "considered a model parameter. For example, BatchNorm's ``running_mean``\n",
    "is not a parameter, but is part of the persistent state.\n",
    "\n",
    "Buffers can be accessed as attributes using given names.\n",
    "\n",
    "Args:\n",
    "    name (string): name of the buffer. The buffer can be accessed\n",
    "        from this module using the given name\n",
    "    tensor (Tensor): buffer to be registered.\n",
    "\n",
    "Example::\n",
    "\n",
    "    >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "    \n",
    "File:      ~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py  \n",
    "Type:      function  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 함수는 특별하게 model parameter로 고려되면 안되는 파라미터(buffer)를 등록하는데 사용된다.\n",
    "> 예를들어서 BatchNorm의 `running_mean`은 prameter가 아니지만 지속성있는 상태(persistent state)의 일부다.\n",
    "\n",
    "Buffer들은 주어진 이름을 사용해서 attribute로써 접근될 수 있다.\n",
    "\n",
    "Args: name(string): buffer의 이름이다. Buffer는 지정할 이름과, Tensor를 이용해서 등록할 수 있다.\n",
    "Example ::\n",
    "\n",
    "```python\n",
    "self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = torch.nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = torch.nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = torch.nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict()\n",
      "OrderedDict([('running_mean', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))])\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "print(model._buffers)\n",
    "model.register_buffer('running_mean', torch.zeros(10))\n",
    "print(model._buffers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
