{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.0.1.post2\n",
      "Torchvision Version:  0.2.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary.torchsummary import summary\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        N = x.data.size(0)\n",
    "        C = x.data.size(1)\n",
    "        H = x.data.size(2)\n",
    "        W = x.data.size(3)\n",
    "        x = F.avg_pool2d(x, (H, W))\n",
    "        x = x.view(N, C)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darknet53(nn.Module):\n",
    "    def __init__(self, input_size = (608, 608), device='cpu'):\n",
    "        super(Darknet53, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.device = device\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.residual_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.residual_layer2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(), \n",
    "        )                \n",
    "        \n",
    "        self.residual_layer3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(), \n",
    "        )\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.residual_layer4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.residual_layer5 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.residual_layer6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.residual_layer7 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.residual_layer8 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.residual_layer9 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.residual_layer10 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.residual_layer11 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.residual_layer12 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.residual_layer13 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.residual_layer14 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.residual_layer15 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.residual_layer16 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.residual_layer17 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.residual_layer18 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.residual_layer19 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.residual_layer20 = nn.Sequential(\n",
    "            nn.Conv2d(1024, 512, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.residual_layer21 = nn.Sequential(\n",
    "            nn.Conv2d(1024, 512, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.residual_layer22 = nn.Sequential(\n",
    "            nn.Conv2d(1024, 512, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.residual_layer23 = nn.Sequential(\n",
    "            nn.Conv2d(1024, 512, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.Sequential(\n",
    "            GlobalAvgPool2d()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1024, 10)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.residual_block(out, self.residual_layer1)\n",
    "        out = self.layer3(out)\n",
    "        out = self.residual_block(out, self.residual_layer2)\n",
    "        out = self.residual_block(out, self.residual_layer3)\n",
    "        out = self.layer4(out)\n",
    "        out = self.residual_block(out, self.residual_layer4)\n",
    "        out = self.residual_block(out, self.residual_layer5)\n",
    "        out = self.residual_block(out, self.residual_layer6)\n",
    "        out = self.residual_block(out, self.residual_layer7)\n",
    "        out = self.residual_block(out, self.residual_layer8)\n",
    "        out = self.residual_block(out, self.residual_layer9)\n",
    "        out = self.residual_block(out, self.residual_layer10)\n",
    "        out = self.residual_block(out, self.residual_layer11)\n",
    "        out = self.layer5(out)\n",
    "        out = self.residual_block(out, self.residual_layer12)\n",
    "        out = self.residual_block(out, self.residual_layer13)\n",
    "        out = self.residual_block(out, self.residual_layer14)\n",
    "        out = self.residual_block(out, self.residual_layer15)\n",
    "        out = self.residual_block(out, self.residual_layer16)\n",
    "        out = self.residual_block(out, self.residual_layer17)\n",
    "        out = self.residual_block(out, self.residual_layer18)\n",
    "        out = self.residual_block(out, self.residual_layer19)\n",
    "        out = self.layer6(out)\n",
    "        out = self.residual_block(out, self.residual_layer20)\n",
    "        out = self.residual_block(out, self.residual_layer21)\n",
    "        out = self.residual_block(out, self.residual_layer22)\n",
    "        out = self.residual_block(out, self.residual_layer23)\n",
    "        out = self.avgpool(out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def residual_block(self, x, layer):\n",
    "        residual = x\n",
    "        out = layer(x)\n",
    "        b, c, h, w = out.shape\n",
    "        residual = residual.view(b, c, h, w)\n",
    "        \n",
    "        return residual + out\n",
    "        \n",
    "    def summary(self):\n",
    "        summary(self, input_size=(3, self.input_size[0], self.input_size[1]), device=self.device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 608, 608]             896\n",
      "       BatchNorm2d-2         [-1, 32, 608, 608]              64\n",
      "         LeakyReLU-3         [-1, 32, 608, 608]               0\n",
      "            Conv2d-4         [-1, 64, 304, 304]          18,496\n",
      "       BatchNorm2d-5         [-1, 64, 304, 304]             128\n",
      "         LeakyReLU-6         [-1, 64, 304, 304]               0\n",
      "            Conv2d-7         [-1, 32, 304, 304]           2,080\n",
      "       BatchNorm2d-8         [-1, 32, 304, 304]              64\n",
      "         LeakyReLU-9         [-1, 32, 304, 304]               0\n",
      "           Conv2d-10         [-1, 64, 304, 304]          18,496\n",
      "      BatchNorm2d-11         [-1, 64, 304, 304]             128\n",
      "        LeakyReLU-12         [-1, 64, 304, 304]               0\n",
      "           Conv2d-13        [-1, 128, 152, 152]          73,856\n",
      "      BatchNorm2d-14        [-1, 128, 152, 152]             256\n",
      "        LeakyReLU-15        [-1, 128, 152, 152]               0\n",
      "           Conv2d-16         [-1, 64, 152, 152]           8,256\n",
      "      BatchNorm2d-17         [-1, 64, 152, 152]             128\n",
      "        LeakyReLU-18         [-1, 64, 152, 152]               0\n",
      "           Conv2d-19        [-1, 128, 152, 152]          73,856\n",
      "      BatchNorm2d-20        [-1, 128, 152, 152]             256\n",
      "        LeakyReLU-21        [-1, 128, 152, 152]               0\n",
      "           Conv2d-22         [-1, 64, 152, 152]           8,256\n",
      "      BatchNorm2d-23         [-1, 64, 152, 152]             128\n",
      "        LeakyReLU-24         [-1, 64, 152, 152]               0\n",
      "           Conv2d-25        [-1, 128, 152, 152]          73,856\n",
      "      BatchNorm2d-26        [-1, 128, 152, 152]             256\n",
      "        LeakyReLU-27        [-1, 128, 152, 152]               0\n",
      "           Conv2d-28          [-1, 256, 76, 76]         295,168\n",
      "      BatchNorm2d-29          [-1, 256, 76, 76]             512\n",
      "        LeakyReLU-30          [-1, 256, 76, 76]               0\n",
      "           Conv2d-31          [-1, 128, 76, 76]          32,896\n",
      "      BatchNorm2d-32          [-1, 128, 76, 76]             256\n",
      "        LeakyReLU-33          [-1, 128, 76, 76]               0\n",
      "           Conv2d-34          [-1, 256, 76, 76]         295,168\n",
      "      BatchNorm2d-35          [-1, 256, 76, 76]             512\n",
      "        LeakyReLU-36          [-1, 256, 76, 76]               0\n",
      "           Conv2d-37          [-1, 128, 76, 76]          32,896\n",
      "      BatchNorm2d-38          [-1, 128, 76, 76]             256\n",
      "        LeakyReLU-39          [-1, 128, 76, 76]               0\n",
      "           Conv2d-40          [-1, 256, 76, 76]         295,168\n",
      "      BatchNorm2d-41          [-1, 256, 76, 76]             512\n",
      "        LeakyReLU-42          [-1, 256, 76, 76]               0\n",
      "           Conv2d-43          [-1, 128, 76, 76]          32,896\n",
      "      BatchNorm2d-44          [-1, 128, 76, 76]             256\n",
      "        LeakyReLU-45          [-1, 128, 76, 76]               0\n",
      "           Conv2d-46          [-1, 256, 76, 76]         295,168\n",
      "      BatchNorm2d-47          [-1, 256, 76, 76]             512\n",
      "        LeakyReLU-48          [-1, 256, 76, 76]               0\n",
      "           Conv2d-49          [-1, 128, 76, 76]          32,896\n",
      "      BatchNorm2d-50          [-1, 128, 76, 76]             256\n",
      "        LeakyReLU-51          [-1, 128, 76, 76]               0\n",
      "           Conv2d-52          [-1, 256, 76, 76]         295,168\n",
      "      BatchNorm2d-53          [-1, 256, 76, 76]             512\n",
      "        LeakyReLU-54          [-1, 256, 76, 76]               0\n",
      "           Conv2d-55          [-1, 128, 76, 76]          32,896\n",
      "      BatchNorm2d-56          [-1, 128, 76, 76]             256\n",
      "        LeakyReLU-57          [-1, 128, 76, 76]               0\n",
      "           Conv2d-58          [-1, 256, 76, 76]         295,168\n",
      "      BatchNorm2d-59          [-1, 256, 76, 76]             512\n",
      "        LeakyReLU-60          [-1, 256, 76, 76]               0\n",
      "           Conv2d-61          [-1, 128, 76, 76]          32,896\n",
      "      BatchNorm2d-62          [-1, 128, 76, 76]             256\n",
      "        LeakyReLU-63          [-1, 128, 76, 76]               0\n",
      "           Conv2d-64          [-1, 256, 76, 76]         295,168\n",
      "      BatchNorm2d-65          [-1, 256, 76, 76]             512\n",
      "        LeakyReLU-66          [-1, 256, 76, 76]               0\n",
      "           Conv2d-67          [-1, 128, 76, 76]          32,896\n",
      "      BatchNorm2d-68          [-1, 128, 76, 76]             256\n",
      "        LeakyReLU-69          [-1, 128, 76, 76]               0\n",
      "           Conv2d-70          [-1, 256, 76, 76]         295,168\n",
      "      BatchNorm2d-71          [-1, 256, 76, 76]             512\n",
      "        LeakyReLU-72          [-1, 256, 76, 76]               0\n",
      "           Conv2d-73          [-1, 128, 76, 76]          32,896\n",
      "      BatchNorm2d-74          [-1, 128, 76, 76]             256\n",
      "        LeakyReLU-75          [-1, 128, 76, 76]               0\n",
      "           Conv2d-76          [-1, 256, 76, 76]         295,168\n",
      "      BatchNorm2d-77          [-1, 256, 76, 76]             512\n",
      "        LeakyReLU-78          [-1, 256, 76, 76]               0\n",
      "           Conv2d-79          [-1, 512, 38, 38]       1,180,160\n",
      "      BatchNorm2d-80          [-1, 512, 38, 38]           1,024\n",
      "        LeakyReLU-81          [-1, 512, 38, 38]               0\n",
      "           Conv2d-82          [-1, 256, 38, 38]         131,328\n",
      "      BatchNorm2d-83          [-1, 256, 38, 38]             512\n",
      "        LeakyReLU-84          [-1, 256, 38, 38]               0\n",
      "           Conv2d-85          [-1, 512, 38, 38]       1,180,160\n",
      "      BatchNorm2d-86          [-1, 512, 38, 38]           1,024\n",
      "        LeakyReLU-87          [-1, 512, 38, 38]               0\n",
      "           Conv2d-88          [-1, 256, 38, 38]         131,328\n",
      "      BatchNorm2d-89          [-1, 256, 38, 38]             512\n",
      "        LeakyReLU-90          [-1, 256, 38, 38]               0\n",
      "           Conv2d-91          [-1, 512, 38, 38]       1,180,160\n",
      "      BatchNorm2d-92          [-1, 512, 38, 38]           1,024\n",
      "        LeakyReLU-93          [-1, 512, 38, 38]               0\n",
      "           Conv2d-94          [-1, 256, 38, 38]         131,328\n",
      "      BatchNorm2d-95          [-1, 256, 38, 38]             512\n",
      "        LeakyReLU-96          [-1, 256, 38, 38]               0\n",
      "           Conv2d-97          [-1, 512, 38, 38]       1,180,160\n",
      "      BatchNorm2d-98          [-1, 512, 38, 38]           1,024\n",
      "        LeakyReLU-99          [-1, 512, 38, 38]               0\n",
      "          Conv2d-100          [-1, 256, 38, 38]         131,328\n",
      "     BatchNorm2d-101          [-1, 256, 38, 38]             512\n",
      "       LeakyReLU-102          [-1, 256, 38, 38]               0\n",
      "          Conv2d-103          [-1, 512, 38, 38]       1,180,160\n",
      "     BatchNorm2d-104          [-1, 512, 38, 38]           1,024\n",
      "       LeakyReLU-105          [-1, 512, 38, 38]               0\n",
      "          Conv2d-106          [-1, 256, 38, 38]         131,328\n",
      "     BatchNorm2d-107          [-1, 256, 38, 38]             512\n",
      "       LeakyReLU-108          [-1, 256, 38, 38]               0\n",
      "          Conv2d-109          [-1, 512, 38, 38]       1,180,160\n",
      "     BatchNorm2d-110          [-1, 512, 38, 38]           1,024\n",
      "       LeakyReLU-111          [-1, 512, 38, 38]               0\n",
      "          Conv2d-112          [-1, 256, 38, 38]         131,328\n",
      "     BatchNorm2d-113          [-1, 256, 38, 38]             512\n",
      "       LeakyReLU-114          [-1, 256, 38, 38]               0\n",
      "          Conv2d-115          [-1, 512, 38, 38]       1,180,160\n",
      "     BatchNorm2d-116          [-1, 512, 38, 38]           1,024\n",
      "       LeakyReLU-117          [-1, 512, 38, 38]               0\n",
      "          Conv2d-118          [-1, 256, 38, 38]         131,328\n",
      "     BatchNorm2d-119          [-1, 256, 38, 38]             512\n",
      "       LeakyReLU-120          [-1, 256, 38, 38]               0\n",
      "          Conv2d-121          [-1, 512, 38, 38]       1,180,160\n",
      "     BatchNorm2d-122          [-1, 512, 38, 38]           1,024\n",
      "       LeakyReLU-123          [-1, 512, 38, 38]               0\n",
      "          Conv2d-124          [-1, 256, 38, 38]         131,328\n",
      "     BatchNorm2d-125          [-1, 256, 38, 38]             512\n",
      "       LeakyReLU-126          [-1, 256, 38, 38]               0\n",
      "          Conv2d-127          [-1, 512, 38, 38]       1,180,160\n",
      "     BatchNorm2d-128          [-1, 512, 38, 38]           1,024\n",
      "       LeakyReLU-129          [-1, 512, 38, 38]               0\n",
      "          Conv2d-130         [-1, 1024, 19, 19]       4,719,616\n",
      "     BatchNorm2d-131         [-1, 1024, 19, 19]           2,048\n",
      "       LeakyReLU-132         [-1, 1024, 19, 19]               0\n",
      "          Conv2d-133          [-1, 512, 19, 19]         524,800\n",
      "     BatchNorm2d-134          [-1, 512, 19, 19]           1,024\n",
      "       LeakyReLU-135          [-1, 512, 19, 19]               0\n",
      "          Conv2d-136         [-1, 1024, 19, 19]       4,719,616\n",
      "     BatchNorm2d-137         [-1, 1024, 19, 19]           2,048\n",
      "       LeakyReLU-138         [-1, 1024, 19, 19]               0\n",
      "          Conv2d-139          [-1, 512, 19, 19]         524,800\n",
      "     BatchNorm2d-140          [-1, 512, 19, 19]           1,024\n",
      "       LeakyReLU-141          [-1, 512, 19, 19]               0\n",
      "          Conv2d-142         [-1, 1024, 19, 19]       4,719,616\n",
      "     BatchNorm2d-143         [-1, 1024, 19, 19]           2,048\n",
      "       LeakyReLU-144         [-1, 1024, 19, 19]               0\n",
      "          Conv2d-145          [-1, 512, 19, 19]         524,800\n",
      "     BatchNorm2d-146          [-1, 512, 19, 19]           1,024\n",
      "       LeakyReLU-147          [-1, 512, 19, 19]               0\n",
      "          Conv2d-148         [-1, 1024, 19, 19]       4,719,616\n",
      "     BatchNorm2d-149         [-1, 1024, 19, 19]           2,048\n",
      "       LeakyReLU-150         [-1, 1024, 19, 19]               0\n",
      "          Conv2d-151          [-1, 512, 19, 19]         524,800\n",
      "     BatchNorm2d-152          [-1, 512, 19, 19]           1,024\n",
      "       LeakyReLU-153          [-1, 512, 19, 19]               0\n",
      "          Conv2d-154         [-1, 1024, 19, 19]       4,719,616\n",
      "     BatchNorm2d-155         [-1, 1024, 19, 19]           2,048\n",
      "       LeakyReLU-156         [-1, 1024, 19, 19]               0\n",
      " GlobalAvgPool2d-157                 [-1, 1024]               0\n",
      "          Linear-158                   [-1, 10]          10,250\n",
      "================================================================\n",
      "Total params: 40,613,034\n",
      "Trainable params: 40,613,034\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 4.23\n",
      "Forward/backward pass size (MB): 1599.13\n",
      "Params size (MB): 154.93\n",
      "Estimated Total Size (MB): 1758.28\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = Darknet53()\n",
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "net.to(device)\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs = F.interpolate(inputs, size=(608, 608), mode='bilinear', align_corners=True)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs = F.interpolate(inputs, size=(608, 608), mode='bilinear', align_corners=True)\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.t7')\n",
    "        best_acc = acc\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+200):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
