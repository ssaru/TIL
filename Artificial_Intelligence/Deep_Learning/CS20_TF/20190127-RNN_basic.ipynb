{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "from pygments import highlight\n",
    "from pygments.lexers import PythonLexer\n",
    "from pygments.formatters import Terminal256Formatter\n",
    "from pprint import pformat\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "print(tf.__version__)\n",
    "\n",
    "def pprint_color(obj):\n",
    "    print(highlight(pformat(obj), PythonLexer(), Terminal256Formatter()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 7, 5]\n"
     ]
    }
   ],
   "source": [
    "# 문장의 단어를 RNN에 하나 하나씩 넣는다고 하면?\n",
    "sentences = [['I', 'feel', 'hungry'],\n",
    "            ['tensorflow', 'is', 'verry', 'difficult'],\n",
    "            ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "            ['tensorflow', 'is', 'very', 'fast', 'change']]\n",
    "\n",
    "# RNN은 아래처럼 각 문장 별로 단어의 개수만큼 sequence를 처리해야한다.\n",
    "# --> variable sequence lenghth!\n",
    "print(list(map(lambda word : len(word), sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro : Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMerging : 하나의 리스트로 표현\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\n",
      "\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBefore\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\n",
      "[['I', 'feel', 'hungry'], ['tensorflow', 'is', 'verry', 'difficult'], ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'], ['tensorflow', 'is', 'very', 'fast', 'change']]\n",
      "\n",
      "\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAfter\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\n",
      "['I', 'feel', 'hungry', 'tensorflow', 'is', 'verry', 'difficult', 'tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning', 'tensorflow', 'is', 'very', 'fast', 'change']\n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhat is mean that set(word_list) : 중복제거\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\n",
      "\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBefore\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\n",
      "['I', 'feel', 'hungry', 'tensorflow', 'is', 'verry', 'difficult', 'tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning', 'tensorflow', 'is', 'very', 'fast', 'change']\n",
      "\n",
      "\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAfter\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\n",
      "{'for', 'hungry', 'tensorflow', 'difficult', 'framework', 'I', 'is', 'a', 'very', 'feel', 'change', 'verry', 'deep', 'fast', 'learning'}\n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmake list after set(word_list) : 중복 제거 후, 리스트로 재변환\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\n",
      "['for', 'hungry', 'tensorflow', 'difficult', 'framework', 'I', 'is', 'a', 'very', 'feel', 'change', 'verry', 'deep', 'fast', 'learning']\n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSorting : 알파벳순으로 정렬\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\n",
      "['I', 'a', 'change', 'deep', 'difficult', 'fast', 'feel', 'for', 'framework', 'hungry', 'is', 'learning', 'tensorflow', 'verry', 'very']\n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding : 왜 <pad> token을 붙이나요 보섭선생님?\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\n",
      "['<pad>', 'I', 'a', 'change', 'deep', 'difficult', 'fast', 'feel', 'for', 'framework', 'hungry', 'is', 'learning', 'tensorflow', 'verry', 'very']\n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring마다 idx붙여서 정수화?\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\n",
      "{'<pad>': 0,\n",
      " 'I': 1,\n",
      " 'a': 2,\n",
      " 'change': 3,\n",
      " 'deep': 4,\n",
      " 'difficult': 5,\n",
      " 'fast': 6,\n",
      " 'feel': 7,\n",
      " 'for': 8,\n",
      " 'framework': 9,\n",
      " 'hungry': 10,\n",
      " 'is': 11,\n",
      " 'learning': 12,\n",
      " 'tensorflow': 13,\n",
      " 'verry': 14,\n",
      " 'very': 15}\n"
     ]
    }
   ],
   "source": [
    "# word dic\n",
    "word_list = []\n",
    "pprint_color(\"Merging : 하나의 리스트로 표현\")\n",
    "pprint_color(\"Before\")\n",
    "print(sentences, end=\"\\n\\n\")\n",
    "\n",
    "for elm in sentences:\n",
    "    word_list += elm\n",
    "\n",
    "pprint_color(\"After\")\n",
    "print(word_list, end='\\n\\n\\n\\n')\n",
    "    \n",
    "    \n",
    "pprint_color(\"What is mean that set(word_list) : 중복제거\")\n",
    "pprint_color(\"Before\")\n",
    "print(word_list, end='\\n\\n')\n",
    "pprint_color(\"After\")\n",
    "print(set(word_list), end='\\n\\n\\n\\n')\n",
    "\n",
    "# set으로 만들어주는 이유는 중복 제거\n",
    "pprint_color(\"make list after set(word_list) : 중복 제거 후, 리스트로 재변환\")\n",
    "word_list = list(set(word_list))\n",
    "print(word_list, end='\\n\\n\\n\\n')\n",
    "\n",
    "# string 리스트에서 sort는 어떤 의미를 갖는가?\n",
    "# https://thispointer.com/python-how-to-sort-a-list-of-strings-list-sort-tutorial-examples/\n",
    "# sort은 낮은 순서에서 높은 순서로 정렬\n",
    "# 알파벳 순서\n",
    "word_list.sort()\n",
    "pprint_color(\"Sorting : 알파벳순으로 정렬\")\n",
    "print(word_list, end='\\n\\n\\n\\n')\n",
    "\n",
    "# token은 왜?\n",
    "pprint_color(\"padding : 왜 <pad> token을 붙이나요 보섭선생님?\")\n",
    "word_list = ['<pad>'] + word_list # '<pad>'라는 의미없는 token 추가\n",
    "print(word_list, end='\\n\\n\\n\\n')\n",
    "\n",
    "# string마다 idx로 정수화\n",
    "pprint_color(\"string마다 idx붙여서 정수화?\")\n",
    "word_dic = {word :idx for idx, word in enumerate(word_list)}\n",
    "pprint(word_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_len의 길이에 못미치는 문장은 <pad>로 max_len만큼 padding\n",
    "def pad_seq(sequences, max_len, dic):\n",
    "    seq_len, seq_indices = [], []\n",
    "    for seq in sequences:\n",
    "        # 길이를 list에 appendix\n",
    "        # 즉, 해당 원소의 유의미한 elements 개수를 확인\n",
    "        seq_len.append(len(seq))\n",
    "        \n",
    "        # 문장의 단어를 만들어놓은 idx로 정수화\n",
    "        seq_idx = [dic.get(char) for char in seq]\n",
    "        \n",
    "        # dic.get('<pad>') => '<pad>'의 idx값을 뽑아옴\n",
    "        # len(seq_idx) => 문장의 길이를 구함 \n",
    "        # 즉, 고정된 최대 길이에서 얼만큼의 length가 여유가 있는지 확인\n",
    "        # 그 이후, 남는 길이만큼 기존 seq_idx 뒤에 `<pad>` idx값을 padding해줌.\n",
    "        # seq_indices는 text를 정수화하고 정수화 된 배열을 모아두는 곳.\n",
    "        # 일종의 input data 느낌\n",
    "        seq_idx += (max_len - len(seq_idx)) * [dic.get('<pad>')] # 0 is idx of meaningless token \"<pad>\"\n",
    "        seq_indices.append(seq_idx)    \n",
    "    return seq_len, seq_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 7, 5]\n",
      "[[1, 7, 10, 0, 0, 0, 0, 0],\n",
      " [13, 11, 14, 5, 0, 0, 0, 0],\n",
      " [13, 11, 2, 9, 8, 4, 12, 0],\n",
      " [13, 11, 15, 6, 3, 0, 0, 0]]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlist data type cast to numpy array\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\n",
      "[[ 1  7 10  0  0  0  0  0]\n",
      " [13 11 14  5  0  0  0  0]\n",
      " [13 11  2  9  8  4 12  0]\n",
      " [13 11 15  6  3  0  0  0]]\n",
      "(4, 8)\n"
     ]
    }
   ],
   "source": [
    "max_length = 8\n",
    "sen_len, sen_indices = pad_seq(sequences = sentences, max_len = max_length, dic = word_dic)\n",
    "pprint(sen_len)\n",
    "pprint(sen_indices)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "sen_indices = np.asarray(sen_indices)\n",
    "pprint_color(\"list data type cast to numpy array\")\n",
    "print(sen_indices)\n",
    "print(sen_indices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro : Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 했던 작업을 tf로 하는 내용\n",
    "seq_len = tf.placeholder(dtype = tf.int32, shape = [None])\n",
    "seq_indices = tf.placeholder(dtype = tf.int32, shape = [None, max_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 padding되어있는 빈 tensor를 생성\n",
    "one_hot = np.eye(len(word_dic)).astype(np.float32)\n",
    "# 초기값을 paddingg값으로 채워진 빈 tensor로 'one_hot'이라는 변수를 생성\n",
    "# placeholder가 아니라 variable로 하는 이유는? => tf.nn.embedding_lookup의 params Arg가 tf.get_variable()\n",
    "one_hot = tf.get_variable(name='one_hot', initializer = one_hot,\n",
    "                         trainable = False)\n",
    "\n",
    "#  params may be a PartitionedVariable as returned by using tf.get_variable() with a partitioner.\n",
    "seq_batch = tf.nn.embedding_lookup(params = one_hot, ids = seq_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8)\n",
      "(4, 8, 16)\n",
      "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tmp = sess.run(seq_batch, feed_dict = {seq_indices : sen_indices})\n",
    "    \n",
    "# 위에서는 shape이 (4,8)이었으나\n",
    "# one-hot encoding되기 때문에 idx가 0~15인점을 감안해서 shape은 (4, 8, 16)\n",
    "print(np.shape(sen_indices))\n",
    "print(np.shape(tmp))\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
