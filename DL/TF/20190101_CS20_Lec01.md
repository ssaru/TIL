# Lec01- CS20

TF 2.0흐름에 맞춰서 Tensorflow를 재학습해보자

​    

## Data Flow Graphs

TF는 정의와 실행이 분리된다.

- 그래프를 정의한다.
- 정의된 그래프를 `session`을 통해서 실행한다.



위와 같은 방식은 `eager` 모드를 통해서 변경될 예정이다.



![simple_node_tffmi](https://user-images.githubusercontent.com/13328380/50572264-64c58a80-0dff-11e9-85ad-9a692279e763.jpg)



​    

### What's a tensor

n차원 배열이다.

- 0-d tensor : scalar(number)
- 1-d tensor : vector
- 2-d tensor : matrix

​    

### Data Flow Grphs



```python
import tensorflow as tf
a = tf.add(3,5)
```



의 문법은 아래와 같이 표현될 수 있으며, 아래와 같은 의미를 갖는다.

- 노드(Nodes) : 연산자(Operator), 변수(Variables), 상수(Constants)
- 엣지(Edges) : Tensor



> TF는 이름을 명시하지 않으면 자동적으로 x,y로 생성한다.



![simple_node1](https://user-images.githubusercontent.com/13328380/50572295-36947a80-0e00-11e9-8cff-54748f4c61b0.jpg)



결론적으로 TensorFlow란

`TensorFlow = tensor + flow = data + flow`가 된다.



위에서 이야기했듯이, TF는 `정의`와 `실행`이 분리된다고 언급했는데, 이는 `a`를 `print`로 확인해보면 알 수 있다.

```python
import tensorflow as tf
a = tf.add(3,5)
print(a)

>> Tensor("Add:0", shape=(), dtype=int32)
```



여기서 `a`의 값을 얻고 싶다면, `Session`을 사용해야한다.

​    

### Session

`a`값을 알고싶다면, 다음과 같이 Session을 이용한다.

```python
import tensorflow as tf
a = tf.add(3,5)
sess = tf.Session()
print(sess.run(a))
sess.close()

>> 8
```



이를 다음과 같이 `with`로 감쌀 수 있다.

```python
import tensorflow as tf
a = tf.add(3,5)
with tf.Session() as sess:
    print(sess.run(a))
```



`Session` 객체는 Operation 객체의 실행, Tensor 객체의 평가 환경에 대해서 캡슐화되어있으며, 현재 변수의 값을 저장하기 위해서 메모리를 할당(allocate)한다.

​    

### Subgraph

#### #1

```python
x = 2
y = 3
add_op = tf.add(x,y)
mul_op = tf.multiply(x,y)
useless = tf.multiply(x,add_op)
pow_op = tf.pow(add_op, mul_op)
with tf.Session() as sess:
    z = sess.run(pow_op)
```



![simple_node6](https://user-images.githubusercontent.com/13328380/50572439-39916a00-0e04-11e9-9cf4-c827c9dbd646.jpg)



해당 그래프에서는 `pow_op`만 사용하므로, `useless`에 해당하는 부분은 연산하지 않음.(연산 절약)

​    

#### #2

```python
x = 2
y = 3
add_op = tf.add(x, y)
mul_op = tf.multiply(x, y)
useless = tf.multiply(x, add_op)
pow_op = tf.pow(add_op, mul_op)
with tf.Session() as sess:
    z, not_useless = sess.run([pow_op, useless])
```



위와 같이 `Session`을 사용할 수 있으며, `Session.run`은 아래와 같은 파라미터를 받는다.

여기서 `fetches`란 얻고자하는 변수(variable)인 `tensor`의 리스트를 이야기한다.



```python
tf.Session.run(fetches, feed_dict=None, options=None, run_metadata=None)
```

​    

#### #3

![simple_node7](https://user-images.githubusercontent.com/13328380/50573197-343b1c00-0e12-11e9-82c3-267d9c0f3508.jpg)



위와 같이 특정 그래프(graphs) 뭉치를 다양한 device(CPUs, GPUs, TPUs)로 나누어서 병렬처리할 수 있다.



```python
# Creates a graph
with tf.device('/gpu:2'):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], name='a')
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], name='b')
    c = tf.multiply(a,b)    

# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProtoo(log_device_placement=True))

# Runs the op
print(sess.run(c))
```

​    

### Multiple Graph

여러 그래프를 만들 수는 있지만 실제로는 그렇게 필요할 일이 없다. 

하지만 진짜 그것을 사용해야할 일이 있다면, 기본 `Session`에서는 default graph를 실행하기 때문에 에러가 발생한다.

- Multiple graphs는 multiple session을 필요로한다.
- multiple session은 기본적으로 사용가능한 모든 자원을 사용하려 할 것이다.(서로 경쟁)
- 분산환경에서 data를 numpy나 이런형태로 다른 graph에 던질 수 없다.



`이런 경우 하나의 graph에서 연결이 끊어진 subgraph를 사용하는게 더 좋다.`



#### tf.Graph()

아래와 같은 경우는 Multiple graphs를 사용하는 경우로 default graph와 섞이기 때ㅐ문에 에러를 발생시킨다.

```python
g = tf.Graph()
with g.as_default():
    x = tf.add(3,5)
sess = tf.Session(graph=g)
with tf.Session() as sess:
    sess.run(x)
```



```python
g = tf.Graph()

# add ops to the default graph
a = tf.constant(3)

# add ops to the user created graph -> Prone to errors
with g.as_default():
    b = tf.constant(5)
```



따라서 다수의 그래프를 사용할 때는 하나의 그래프로 묶이지 않는 점이 완벽하지는 않지만 아래와 같은 방법이 더 좋다.

```python
g1 = tf.get_default_graph()
g2 = tf.Graph()

# add ops to the default graph
with g1.as_default():
    a = tf.Constant(3)
    
# add ops to the user createed graph
with g2.as_default():
    b = tf.Constant(5)
```



#### Graphs를 사용하는 방법

1. 계산을 저장하고, 가져올 값으로 연결되는 하위 그래프만 실행
2. auto-differentiation을 위해서 계산을 작은 differential 조각으로 나눈다.
3. GPUs, GPUs, TPUs나 다른 device를 이용하여 분산 컴퓨팅을 촉진
4. 많은 ML model이 directed graphs로 학습되고, 시각화된다.



