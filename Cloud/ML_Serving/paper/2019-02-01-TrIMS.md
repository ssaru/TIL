# TrIMS: Transparent and Isolated Model Sharing for Low Latency Deep LearningInference in Function as a Service Environments

​    

## Evaluation

- TrIMS을 31개의 pre-trained된 small model과 8개의 large 모델을 이용해서 3가지 시스템에서 평가했다. 

  > 여기서 System3개는 NVLink bus를 사용했다.



![table2](https://user-images.githubusercontent.com/13328380/52103565-a4170f00-2629-11e9-8dfe-db15744c3ec9.PNG)



- 여기서는 FaaS환경에서 TrIMS 성능을 이상적인 환경(모델 로딩 및 데이터 이동에 시간이 걸리지 않음)과 비교하고, end-to-end cold-start를 baseline으로 한다. baseline의 선정 이유는 FaaS 환경이 기본적으로 이를 채용하고 있기 때문이다.

​    

### Latency Improvement

- Figure 8은 TrIMS를 사용하지 않은 MXNet에서 모델셋을 돌렸을 때, 달성된 속도에 대해서 비교한다. 우리는 아래와 같은 2가지 경우를 확인할 수 있다.

  - best case (GPU cache hit)
  - worst case (GPU, CPU cache missed)

- best case 분석 결과, 서버는 CUDA IPC handle을 생성하고 framework client는 framework의 컨테이너안에 GPU device pointer를 넣어야한다. 이러한 경우 약간의 overhead가 있으나, 이상적인 상황에서 20%정도의 차이밖에 안난다

- worst case 분석 결과 MRM은 디스크에서 데이터를 로드하고, CPU에서 모델을 유지하고, GPU로 데이터를 복사하고, GPU 메모리 핸들을 클라이언트로 보내야 한다.

  > 속도가 느려지지만 이 경우에는 파이프라인 간에 모델을 공유하지 않는 것으로 가정하므로 클라우드 설정에서는 드문 경우다.



![fig8](https://user-images.githubusercontent.com/13328380/52104346-864ba900-262d-11e9-9384-a17c99de4a91.PNG)

​    

### Speedup Breakdown

- TrIMS를 통해서 inference시에 생기는 새로운 bottleneck이 어디에서 생기는지 이해하기 위해서 가장 낮은 속도를 달성한 System3를 살펴보고, 아래와 같은 내용들을 측정했다.
  - 추론 계산을 수행하는 시간
  - 모델을 초기화하기 위한 시간(TRIMS를 사용하지 않을 때 GPU에 데이터를 복사하는 것 포함)
  - 디스크에서 모델을 로드하는 것
  - 오버헤드를 공유

- Figure 9에서 볼 수 있지만, TrIMS 사용 없이 평균적으로 모델 loading & initializing하는 시간은 86%를 소모하며, 7%는 연산수행에 쓰인다. 
- TrIMS를 사용하는 경우에는, disk에서 model을 로딩하고 GPU로 메모리를 복사하고, 삭제하는 것에 대해서 비효율적인 것을 줄였는데, 이를 통해서 우리는 4.8배의 평균 속도 증가를 얻을 수 있었다.



![fig9](https://user-images.githubusercontent.com/13328380/52104350-92376b00-262d-11e9-8af1-dd6d7c779686.PNG)

​    

### Large Model Evaluation

- 표 4는 우리가 평가를 위해 선택한 모델 8개에 대해서 메모리 공간 및 입력 크기를 나타낸다. 



![table4](https://user-images.githubusercontent.com/13328380/52104367-aa0eef00-262d-11e9-8ef7-40be6a608306.PNG)



- 그림 10은 모델 로딩 오버헤드를 제거함으로써 large 모델에 대한 inference 성능이 컴퓨팅 바인딩*(?)*이 되어 더 빠른 GPU의 이점을 제공한다는 것을 보여준다.
- inference 계산이 시스템 1의 85%와 시스템 2의 50%를 차지하기 때문에 더 컴퓨팅 집약적인 VGG16 네트워크(예: 모델 7)의 경우 시스템 2보다 시스템 1의 속도가 느린 이유가 된다. 
- 이는 저가 GPU에서 사용하게 될 경우 더 뚜렷한 병목현상이 될 것으로 예상하며, 특별하게 전문화된 low latency inference 가속이 이 문제를 을의 문제는 줄어들게 될 것이다. 



![fig10](https://user-images.githubusercontent.com/13328380/52104413-ddea1480-262d-11e9-9c33-bd0a2c874ddd.PNG)

​     