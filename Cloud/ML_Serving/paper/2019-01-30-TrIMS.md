# TrIMS: Transparent and Isolated Model Sharing for Low Latency Deep LearningInference in Function as a Service Environments

​    

### Implementation

- Apache MXNet 기반으로 TrIMS를 구현했고, 이를 기반으로 실험결과를 리포팅한다.
- 논문의 제안이 효율성과 일반화를 만족하는지 검증하기 위해서 아래와 같은 몇가지 원칙을 따른다.

​    

1. **Backward Compatible**

   기존의 Python이나 Scala로 작성된 MXNet 코드를 변경하지 않고 실행할 수 있어야한다.

2. **Simple and Minimal**

   구현체는 가능한 framework code를 수정하지 않으며 간단해야한다.

   > TrIMS는 MXNet 코드의 0.5%정도 밖에 안되는 1500 line의 코드를 수정했다. 800 line은 서버를 위해서, 700 line은 클라이언트를 위해서 수정했다.

3. **Configurable**

   구현에는 메모리 공유의 eviction 전략, 사용할 수있는 메모리의 양, "TrIMS"사용 가능 여부, 사용할 수있는 캐시 레벨 등 모든 것을 설정할 수 있어야한다.

4. **Fast, Concurrent and Scalable**

   gRPC를 이용해서 통신을 하고, 서빙을 빠르고 MRM database를 위해 효율적인 데이터 구조를 사용한다. MRM database는 서빙을 빠르고 concurrent하게 만드는게 목적이다.

​    

#### TrIMS Apache MXNet Framework

- TrIMS가 Apache MXNet framework를 지원하도록 client를 수정했다. 해당 내용은 MXNet C predict API's implementation에 있는 `MXPredCreate`, `MXPredFree`를 수정한 것이다.

- TrIMS가 enable될 때, `trims::open`과 `trims::close`가 predictor creation과 deletion의 일부로써 호출된다.  



  ![listing1](https://user-images.githubusercontent.com/13328380/52024826-81132f00-2545-11e9-9ed0-366362910096.PNG)

​    

- MXNet은 training에 초점이 맞춰져있지 inference에는 초점이 맞춰져있지 않다.
- inference latency를 줄이기 위해서 몇가지 개선을 진행했다.
  - 초기부터 CUDA resource를 초기화하는 것을 피했다.
  - backward propagation을 위한 cuDNN algorithm selection을 제거했다.
  - random resource generation을 단순화했다.
- 위의 최적화 작업을 거치고나서, MXNet의 평균 inference시간은 6배 이상 빨라졌다.

​    

#### 5.2 GPU Memory Sharing

- GPU memory sharing은 CUDA의 `cudaIPC*` runtime function을 사용한다.

- Volta 이전 GPU의 경우, CUDA IPC 메커니즘은 CUDA MPS를 활용한다.

  - CUDA MPS란 메모리 할당을 수행하는 user process를 이야기한다.

- 위의 의미는 모든 CUDA 작업이 동일한 CUDA MPS 컨텍스트 내에서 직렬화되고 실행된다는 것을 의미한다.

  - 이는 다른 프로세스가 같은 GPU 가상 주소 공간(virtual address space; VAS)를 사용할 수 있게한다.

- 볼타 GPU를 위해 NVIDIA는 context가 page-table mapping으로 공유될 수 있게하는 새로운 기능을 추가했다.

- 이는 사용자 프로세스가 다른 context에서 실행되더라도 memory sharing이 가능해진다는 이야기가 된다.

- CUDA 9.2의 경우, CUDA MPS는 여전히 shared allocation을 유지하고 이들 간에 통신하기 위해 호출되지만, 몇개의 기능을 제외하고 대부분의 CUDA 작업은 IPC 통신 없이 수행된다.

- sharing은 CUDA MPS를 사용하기 위해서 serialize될 것인데, CUDA IPC 함수는 측정 가능한 오버 헤드가 있다는 단점이 있다.

- 이러한 단점은 bottleneck이 될 수 있고, 실제로 layer를 세분화해서 model을 sharing할 경우 ResNet269-v2와 같이 큰 모델은 굉장히 큰 overhead를 보였다.

- 이를 해결하기 위해서 그룹의 layer sharing(per-group of layer sharing)이나 모델 공유를 세분화(model sharing granularity)를 통해서 해결했다.

- CUDA IPC overhead는 측정할 수 있고, TrIMS의 이득을 정량화하기 위해서 아래와 같은 식을 사용한다.



  $\rho = b \div q - n \times (o+s)$

  **$\rho$** : positive value면 magnitude는 TrIMS를 사용해서 얻는 속도상승과 상관관계가 있다는 의미다.

  $b$ : disk를 점유하고 있는 모델의 bytes 수

  $q$ : dis I/O bandwidth

  $n$ : model level에서 sharing granularity되었을 때, 공유된 object의 수 (만약 해당 값이 1이고, granularity가 layer라면 해당 value는 layer의 개수가 된다.)

  $o$ : CUDA IPC를 통해 CUDA memory를 공유하는 overhead

  $s$ : CUDA IPC handle로부터 공유된 CUDA device포인터에서 얻은 overhead
