# TrIMS: Transparent and Isolated Model Sharing for Low Latency Deep LearningInference in Function as a Service Environments

​    

## TrIMS Design

*TrIMS*는 2개의 컴포넌트로 구성된다.

- **Model Resource Manager(MRM)**

  system memory에 상주하고 있는 resource들을 관리하고, framework client로부터 model loading하는 것을 추상화한다

- **framework client**

  각 프레임워크 클라이언트는 아래 그림과 같이 IPC(*inter-process communication*)를 통해 MRM과 통신한다.



![fig4](https://user-images.githubusercontent.com/13328380/51810131-68a6d880-22e9-11e9-9118-4d0a6a153ea5.PNG)

​    

### *TrIMS* Model Resource Manager (MRM)

- MRM은 framework와 모델 종류를 구분하며, 이를 model의 name과 version별로 해서 database를 관리한다. 

- MRM placement manager는 모델을 GPU memory, CPU memory, local storage, cloud storage로 mapping한다. *이러한 구조는 CPU의 "cache"구조와 비슷하기 때문에 이를 cache라고 부르자.* 

- 시스템이 cold boot된 후에 모델은 cloud storage에서 download 받아야하기 때문에, "initial model" 요청은 GPU, CPU, 그리고 local storage 에서 찾지 못할 것이다. (*cache*영역에서 찾기 못함) *이는 cloud storage에서 "cache"로 "load"되는 것으로 표현할 수 있다.* 

  "cache"가 가득 찼을 때, 하나 혹은 그 이상의 모델들을 cache에서 제거하게(evicted) 된다.

- TrIMS에서 MRM과 clinet간 통신은 gRPC 기반의 *inter-process communication*을 사용한다. 

- TrIMS는 process간 GPU memory를 공유하기 위해서 CUDA runtime의 *cudaIpc*을 사용한다.
- MRM은 `trims::open`, `trims::close`라는 2개의 API function을 사용해서 모델을 개별적으로 open하고 close하는 방법을 추상화한다.



![fig5](https://user-images.githubusercontent.com/13328380/51810481-63e32400-22eb-11e9-9767-db858ec4b9d2.PNG)

​    

#### Loading Models

GPU를 사용한다면 모델 로딩시, MRM은 shape inference를 통해서 memory 사용량을 추정한다. shape inference 후에 MRM은 아래와 같은 state diagram을 따른다.



![fig7](https://user-images.githubusercontent.com/13328380/51810544-deac3f00-22eb-11e9-8f05-83b6e7ab086b.PNG)



​     

#### GPU cache hit

MRM은 모델의 기준 개수를 증가시키고 MRM이 소유한 장치 메모리에서 공유 메모리 핸들을 생성한다. 그 이후, 핸들은 framework clinet로 리턴된다. 만약 모델의 중간 결과가 사용 가능한 메모리보다 클 경우 모델 제거를 수행한다.

​    

#### GPU cache miss / CPU cache hit

서버는 GPU의 메모리 사용량을 조회하고, 만약에 모델을 GPU 메모리에 복사할 수 있다면, 이를 수행한다. 하지만 그렇지 않다면 일부 메모리를 다시 확보해야한다.

​    

#### CPU and GPU cache miss

데이터가 로컬 저장소에 없으면 MRM은 클라우드에서 모델을 다운로드한다. 데이터가 디스크에 있는 경우, MRM은 프레임워크의 serializer를 사용하여 디스크에서 데이터를 로딩한다. 고정된 메모리는 CPU에 할당되고 모델 가중치는 CPU에 복사된다. 그런 다음 MRM은 데이터가 CPU 메모리에 영구적일 때와 동일한 논리를 따른다.



![fig6](https://user-images.githubusercontent.com/13328380/51810681-b7a23d00-22ec-11e9-99c1-3f3594c66bae.PNG)

