# TrIMS: Transparent and Isolated Model Sharing for Low Latency Deep LearningInference in Function as a Service Environments

​    

## Workload Modeling

- 유저가 과도하게 쏠린 멀티테넌트 시스템에서 TrIMS의 작동을 이해하기 위해서 workload modeling을 수행했다.
- Table 3에서 보이는 것과 같이 37개의 작은 모델들은 Pareto distribution에 따라서 선택된다.
- 해당 모델들의 메모리 총합은 사용된 GPU 메모리의 2배이기 때문에, 각 모델들은 동시에 GPU에 동시에 올라갈 수 없다.
  - 따라서 TrIMS MRM은 model reclamation과 eviction 절차를 수행해야한다.
  - 제한된 메모리 공간으로 인해서, 본 실험에서는 *LRU eviction*전략을 이용했지만 관찰 결과 다른 전략도 유효함을 확인했다.



![table3](https://user-images.githubusercontent.com/13328380/52271569-5bdc5180-2987-11e9-9440-7be13d173842.png)



- Figure 11은 geometric mean speedup에 대한 iso-efficiency curve를 보여준다.
  - 가로축은 model의 양이다.
  - 세로축은 동시 리퀘스트의 양이다.
- 유저가 과도하게 쏠린(oversubscribed setting) 환경임에도 불구하고, TrIMS는 10개의 client를 동시에 수행이 가능하며, 전체적인 batch execution time이 8배 빠르게 나타난다. 각 request의 latency penalty는 20%정도 밖에 나타나지 않았다.
- latency penalty는 더 큰 메모리 공간을 사용하기 위해서 model을 evicting하는 비용으로 인해 나타난 결과다.
- 전체 시스템 안에서 over-subscription sweet spot을 관찰할 수 있었는데, sweet spot에서는 모델의 양과 동시 request의 수가 증가되는 동안 batch execution time은 1배ㄹ 유지할 수 있는 구간이다.
  - 40%의 모델이 활발히 요청되고 있을 때 sweet spot을 확인할 수 있다.
  - over-subscription sweet spot이 각 시스템마다 다른 이유는 시스템의 hardware spec이 서로 다르기 때문이다.
  - 본질적으로 연산을 위한 disk I/O를 제거했기 때문에, bottleneck은 inference에 있다. 따라서 sweet spot은 컴퓨팅 리소스에 의해서 결정된다.



![figure11](https://user-images.githubusercontent.com/13328380/52273055-fd65a200-298b-11e9-8852-43537a8041e1.png)

​    

# Related Works

- 다음과 같은 테크닉들은 user isolation과 low latency inference 문제를 해결하고자 했다.
  - Tensorflow-Serving은 soft model isolation을 제공한다.
  - TFX는 쓰레드 thread-level user isolation을 제공하고 model-loading overhead를 숨기기 위해서 thread pool을 사용한다.
  - Clipper는 동시 request를 batch를 이용해서 latency 효율화를 추구했다.
- 최근 CUDA IPC를 활용하여 단일 프로세스/애플리케이션의 다양한 노드 간 MPI 수집을 개선함으로써 GPU에 대한 HPC 애플리케이션의 성능을 향상시킨 연구들이 있다.
- 딥러닝 모델의 inference time자체를 줄이기 위해서 경량화 네트워크가 있다.
- 다양한 CPU/GPU 가상화기술과 GPU 멀티 테넌시 기술에 대한 연구는 CPU/GPU 병렬 처리 개선을 가지고 왔다.
- TrIMS는 위에서 언급된 기술들과는 독립적이며, plugin으로 container에 통합될 수 있다.

​    

# Conclusion

- "code start" 지연 시간의 주요  원인(모델 로딩 오버헤드)를 완화하고 모듈식 딥러닝 구성 요소를 사용하여 복잡한 latency에 예민한 파이프라인을 구축할 수 있도록 TrIMS를 제안했다.



